Python 3.8.17
2023-06-20 10:50:24.049548: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:24.931075: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:35.941275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.941344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.948447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.966063: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.973458: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.988563: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.991215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.993638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.995714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.996945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.997444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:36.029850: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.029850: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.030859: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.042893: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.044967: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:36.047519: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066513: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066519: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066517: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066526: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066528: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066532: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.129595: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Using cpu device
Wrapping the env in a VecTransposeImage.
<<<<< Start learning >>>>>
Logging to ./tensorboard/PPO-00003_1
------------------------------
| time/              |       |
|    fps             | 1044  |
|    iterations      | 1     |
|    time_elapsed    | 23    |
|    total_timesteps | 24576 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 2           |
|    time_elapsed         | 319         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014710789 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | -5.94e-05   |
|    learning_rate        | 3e-06       |
|    loss                 | 0.195       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 3           |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -inf - Last mean reward per episode: 337.00
Saving new best model to tmp/best_model
Num timesteps: 72000
Best mean reward: 337.00 - Last mean reward per episode: 337.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.85e+03    |
|    ep_rew_mean          | 337.0       |
| time/                   |             |
|    fps                  | 119         |
|    iterations           | 3           |
|    time_elapsed         | 617         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.009660824 |
|    clip_fraction        | 0.0963      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.125       |
|    learning_rate        | 3e-06       |
|    loss                 | 16.4        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 31.4        |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 337.00 - Last mean reward per episode: 610.75
Saving new best model to tmp/best_model
Num timesteps: 96000
Best mean reward: 610.75 - Last mean reward per episode: 610.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.77e+03    |
|    ep_rew_mean          | 610.75      |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 4           |
|    time_elapsed         | 920         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.008571111 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 0.506       |
|    learning_rate        | 3e-06       |
|    loss                 | 54.2        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 65.6        |
-----------------------------------------
Num timesteps: 108000
Best mean reward: 610.75 - Last mean reward per episode: 610.75
Num timesteps: 120000
Best mean reward: 610.75 - Last mean reward per episode: 663.80
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 5.67e+03     |
|    ep_rew_mean          | 663.8        |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 5            |
|    time_elapsed         | 1217         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0072161946 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.666        |
|    learning_rate        | 3e-06        |
|    loss                 | 20.4         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 41.1         |
------------------------------------------
Num timesteps: 132000
Best mean reward: 663.80 - Last mean reward per episode: 989.89
Saving new best model to tmp/best_model
Num timesteps: 144000
Best mean reward: 989.89 - Last mean reward per episode: 1063.50
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.15e+03    |
|    ep_rew_mean          | 1063.5      |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 6           |
|    time_elapsed         | 1518        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.006069818 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.748       |
|    learning_rate        | 3e-06       |
|    loss                 | 1.96        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 13.3        |
-----------------------------------------
Num timesteps: 156000
Best mean reward: 1063.50 - Last mean reward per episode: 1063.50
Num timesteps: 168000
Best mean reward: 1063.50 - Last mean reward per episode: 1063.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.15e+03    |
|    ep_rew_mean          | 1063.5      |
| time/                   |             |
|    fps                  | 94          |
|    iterations           | 7           |
|    time_elapsed         | 1825        |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.006016215 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.817       |
|    learning_rate        | 3e-06       |
|    loss                 | 16          |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 71.1        |
-----------------------------------------
Num timesteps: 180000
Best mean reward: 1063.50 - Last mean reward per episode: 1063.50
Num timesteps: 192000
Best mean reward: 1063.50 - Last mean reward per episode: 1231.85
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 9.75e+03     |
|    ep_rew_mean          | 1231.8462    |
| time/                   |              |
|    fps                  | 92           |
|    iterations           | 8            |
|    time_elapsed         | 2125         |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0057142586 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.784        |
|    learning_rate        | 3e-06        |
|    loss                 | 9.11         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000994    |
|    value_loss           | 5.71         |
------------------------------------------
Num timesteps: 204000
Best mean reward: 1231.85 - Last mean reward per episode: 1265.33
Saving new best model to tmp/best_model
Num timesteps: 216000
Best mean reward: 1265.33 - Last mean reward per episode: 1265.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.85e+03    |
|    ep_rew_mean          | 1265.3334   |
| time/                   |             |
|    fps                  | 91          |
|    iterations           | 9           |
|    time_elapsed         | 2424        |
|    total_timesteps      | 221184      |
| train/                  |             |
|    approx_kl            | 0.007565147 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.875       |
|    learning_rate        | 3e-06       |
|    loss                 | 14.5        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 45.4        |
-----------------------------------------
Num timesteps: 228000
Best mean reward: 1265.33 - Last mean reward per episode: 1271.44
Saving new best model to tmp/best_model
Num timesteps: 240000
Best mean reward: 1271.44 - Last mean reward per episode: 1282.41
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.41e+03    |
|    ep_rew_mean          | 1282.4117   |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 10          |
|    time_elapsed         | 2723        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.007445943 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 9.93        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.000155   |
|    value_loss           | 33.7        |
-----------------------------------------
Num timesteps: 252000
Best mean reward: 1282.41 - Last mean reward per episode: 1313.61
Saving new best model to tmp/best_model
Num timesteps: 264000
Best mean reward: 1313.61 - Last mean reward per episode: 1435.30
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.57e+03    |
|    ep_rew_mean          | 1417.9166   |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 11          |
|    time_elapsed         | 3021        |
|    total_timesteps      | 270336      |
| train/                  |             |
|    approx_kl            | 0.008166317 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 3.45        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.000958   |
|    value_loss           | 16.6        |
-----------------------------------------
Num timesteps: 276000
Best mean reward: 1435.30 - Last mean reward per episode: 1417.92
Num timesteps: 288000
Best mean reward: 1435.30 - Last mean reward per episode: 1450.44
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 9.06e+03     |
|    ep_rew_mean          | 1473.7778    |
| time/                   |              |
|    fps                  | 88           |
|    iterations           | 12           |
|    time_elapsed         | 3328         |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0061202296 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 0.938        |
|    learning_rate        | 3e-06        |
|    loss                 | 17.1         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000856    |
|    value_loss           | 95.2         |
------------------------------------------
Num timesteps: 300000
Best mean reward: 1450.44 - Last mean reward per episode: 1476.96
Saving new best model to tmp/best_model
Num timesteps: 312000
Best mean reward: 1476.96 - Last mean reward per episode: 1482.93
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.83e+03    |
|    ep_rew_mean          | 1503.1936   |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 13          |
|    time_elapsed         | 3632        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.007489993 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.891       |
|    learning_rate        | 3e-06       |
|    loss                 | 69.5        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 115         |
-----------------------------------------
Num timesteps: 324000
Best mean reward: 1482.93 - Last mean reward per episode: 1503.19
Saving new best model to tmp/best_model
Num timesteps: 336000
Best mean reward: 1503.19 - Last mean reward per episode: 1569.06
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.33e+03    |
|    ep_rew_mean          | 1570.9166   |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 14          |
|    time_elapsed         | 3936        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.007523173 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.87       |
|    explained_variance   | 0.911       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000614   |
|    value_loss           | 166         |
-----------------------------------------
Num timesteps: 348000
Best mean reward: 1569.06 - Last mean reward per episode: 1586.32
Saving new best model to tmp/best_model
Num timesteps: 360000
Best mean reward: 1586.32 - Last mean reward per episode: 1630.55
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.55e+03     |
|    ep_rew_mean          | 1644.9783    |
| time/                   |              |
|    fps                  | 86           |
|    iterations           | 15           |
|    time_elapsed         | 4240         |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0075204372 |
|    clip_fraction        | 0.0892       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 0.923        |
|    learning_rate        | 3e-06        |
|    loss                 | 80.4         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 164          |
------------------------------------------
Num timesteps: 372000
Best mean reward: 1630.55 - Last mean reward per episode: 1664.60
Saving new best model to tmp/best_model
Num timesteps: 384000
Best mean reward: 1664.60 - Last mean reward per episode: 1685.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.3e+03     |
|    ep_rew_mean          | 1669.7457   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 16          |
|    time_elapsed         | 4535        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.006384143 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.82       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 143         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00098    |
|    value_loss           | 234         |
-----------------------------------------
Num timesteps: 396000
Best mean reward: 1685.59 - Last mean reward per episode: 1676.79
Num timesteps: 408000
Best mean reward: 1685.59 - Last mean reward per episode: 1685.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.62e+03    |
|    ep_rew_mean          | 1713.9143   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 17          |
|    time_elapsed         | 4829        |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.007551912 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.69       |
|    explained_variance   | 0.899       |
|    learning_rate        | 3e-06       |
|    loss                 | 168         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 385         |
-----------------------------------------
Num timesteps: 420000
Best mean reward: 1685.59 - Last mean reward per episode: 1745.69
Saving new best model to tmp/best_model
Num timesteps: 432000
Best mean reward: 1745.69 - Last mean reward per episode: 1751.91
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.99e+03    |
|    ep_rew_mean          | 1786.1548   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 18          |
|    time_elapsed         | 5126        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.007898296 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.878       |
|    learning_rate        | 3e-06       |
|    loss                 | 205         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.000917   |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 444000
Best mean reward: 1751.91 - Last mean reward per episode: 1780.74
Saving new best model to tmp/best_model
Num timesteps: 456000
Best mean reward: 1780.74 - Last mean reward per episode: 1780.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.66e+03    |
|    ep_rew_mean          | 1802.5773   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 19          |
|    time_elapsed         | 5422        |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.007152064 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.907       |
|    learning_rate        | 3e-06       |
|    loss                 | 315         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000511   |
|    value_loss           | 409         |
-----------------------------------------
Num timesteps: 468000
Best mean reward: 1780.74 - Last mean reward per episode: 1802.58
Saving new best model to tmp/best_model
Num timesteps: 480000
Best mean reward: 1802.58 - Last mean reward per episode: 1831.46
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.26e+03     |
|    ep_rew_mean          | 1883.87      |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 20           |
|    time_elapsed         | 5723         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0073837326 |
|    clip_fraction        | 0.0774       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.936        |
|    learning_rate        | 3e-06        |
|    loss                 | 98.7         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.000312     |
|    value_loss           | 283          |
------------------------------------------
Num timesteps: 492000
Best mean reward: 1831.46 - Last mean reward per episode: 1883.87
Saving new best model to tmp/best_model
Num timesteps: 504000
Best mean reward: 1883.87 - Last mean reward per episode: 1897.77
Saving new best model to tmp/best_model
Num timesteps: 516000
Best mean reward: 1897.77 - Last mean reward per episode: 1893.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.01e+03     |
|    ep_rew_mean          | 1893.8       |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 21           |
|    time_elapsed         | 6025         |
|    total_timesteps      | 516096       |
| train/                  |              |
|    approx_kl            | 0.0067628263 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.62        |
|    explained_variance   | 0.922        |
|    learning_rate        | 3e-06        |
|    loss                 | 112          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00037     |
|    value_loss           | 362          |
------------------------------------------
Num timesteps: 528000
Best mean reward: 1897.77 - Last mean reward per episode: 1896.88
Num timesteps: 540000
Best mean reward: 1897.77 - Last mean reward per episode: 1888.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.3e+03      |
|    ep_rew_mean          | 1878.28      |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 22           |
|    time_elapsed         | 6320         |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0072628316 |
|    clip_fraction        | 0.0735       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.63        |
|    explained_variance   | 0.925        |
|    learning_rate        | 3e-06        |
|    loss                 | 86.3         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0004      |
|    value_loss           | 346          |
------------------------------------------
Num timesteps: 552000
Best mean reward: 1897.77 - Last mean reward per episode: 1895.70
Num timesteps: 564000
Best mean reward: 1897.77 - Last mean reward per episode: 1935.01
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 1927.81     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 23          |
|    time_elapsed         | 6615        |
|    total_timesteps      | 565248      |
| train/                  |             |
|    approx_kl            | 0.006257016 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.66       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 120         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.000212   |
|    value_loss           | 348         |
-----------------------------------------
Num timesteps: 576000
Best mean reward: 1935.01 - Last mean reward per episode: 1922.51
Num timesteps: 588000
Best mean reward: 1935.01 - Last mean reward per episode: 1938.45
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02e+03    |
|    ep_rew_mean          | 1937.94     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 24          |
|    time_elapsed         | 6919        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.006880377 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 58          |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 382         |
-----------------------------------------
Num timesteps: 600000
Best mean reward: 1938.45 - Last mean reward per episode: 1948.65
Saving new best model to tmp/best_model
Num timesteps: 612000
Best mean reward: 1948.65 - Last mean reward per episode: 1976.83
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.95e+03     |
|    ep_rew_mean          | 1976.83      |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 25           |
|    time_elapsed         | 7211         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0071697645 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.5         |
|    explained_variance   | 0.907        |
|    learning_rate        | 3e-06        |
|    loss                 | 270          |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 478          |
------------------------------------------
Num timesteps: 624000
Best mean reward: 1976.83 - Last mean reward per episode: 1979.80
Saving new best model to tmp/best_model
Num timesteps: 636000
Best mean reward: 1979.80 - Last mean reward per episode: 1998.57
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 1990.76     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 26          |
|    time_elapsed         | 7506        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.007851045 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.38       |
|    explained_variance   | 0.894       |
|    learning_rate        | 3e-06       |
|    loss                 | 189         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.000531   |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 648000
Best mean reward: 1998.57 - Last mean reward per episode: 1974.43
Num timesteps: 660000
Best mean reward: 1998.57 - Last mean reward per episode: 2016.88
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 2028.22     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 27          |
|    time_elapsed         | 7802        |
|    total_timesteps      | 663552      |
| train/                  |             |
|    approx_kl            | 0.007755907 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.907       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.000111   |
|    value_loss           | 447         |
-----------------------------------------
Num timesteps: 672000
Best mean reward: 2016.88 - Last mean reward per episode: 2024.95
Saving new best model to tmp/best_model
Num timesteps: 684000
Best mean reward: 2024.95 - Last mean reward per episode: 2032.02
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.81e+03     |
|    ep_rew_mean          | 2036.75      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 28           |
|    time_elapsed         | 8098         |
|    total_timesteps      | 688128       |
| train/                  |              |
|    approx_kl            | 0.0072596795 |
|    clip_fraction        | 0.0884       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.927        |
|    learning_rate        | 3e-06        |
|    loss                 | 111          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000542    |
|    value_loss           | 388          |
------------------------------------------
Num timesteps: 696000
Best mean reward: 2032.02 - Last mean reward per episode: 2040.27
Saving new best model to tmp/best_model
Num timesteps: 708000
Best mean reward: 2040.27 - Last mean reward per episode: 2054.33
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 2100.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 29          |
|    time_elapsed         | 8396        |
|    total_timesteps      | 712704      |
| train/                  |             |
|    approx_kl            | 0.008104875 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 91.4        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.000452   |
|    value_loss           | 435         |
-----------------------------------------
Num timesteps: 720000
Best mean reward: 2054.33 - Last mean reward per episode: 2104.39
Saving new best model to tmp/best_model
Num timesteps: 732000
Best mean reward: 2104.39 - Last mean reward per episode: 2067.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.75e+03    |
|    ep_rew_mean          | 2063.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 30          |
|    time_elapsed         | 8696        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.007309798 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 298         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000925   |
|    value_loss           | 475         |
-----------------------------------------
Num timesteps: 744000
Best mean reward: 2104.39 - Last mean reward per episode: 2048.30
Num timesteps: 756000
Best mean reward: 2104.39 - Last mean reward per episode: 2072.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | 2077.47      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 31           |
|    time_elapsed         | 8994         |
|    total_timesteps      | 761856       |
| train/                  |              |
|    approx_kl            | 0.0077880025 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.6         |
|    explained_variance   | 0.927        |
|    learning_rate        | 3e-06        |
|    loss                 | 127          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 393          |
------------------------------------------
Num timesteps: 768000
Best mean reward: 2104.39 - Last mean reward per episode: 2077.60
Num timesteps: 780000
Best mean reward: 2104.39 - Last mean reward per episode: 2046.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 2030.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 32          |
|    time_elapsed         | 9291        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.008641992 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 104         |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 360         |
-----------------------------------------
Num timesteps: 792000
Best mean reward: 2104.39 - Last mean reward per episode: 2006.47
Num timesteps: 804000
Best mean reward: 2104.39 - Last mean reward per episode: 2006.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.7e+03     |
|    ep_rew_mean          | 1996.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 33          |
|    time_elapsed         | 9587        |
|    total_timesteps      | 811008      |
| train/                  |             |
|    approx_kl            | 0.007984597 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.921       |
|    learning_rate        | 3e-06       |
|    loss                 | 161         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.000583   |
|    value_loss           | 431         |
-----------------------------------------
Num timesteps: 816000
Best mean reward: 2104.39 - Last mean reward per episode: 1991.10
Num timesteps: 828000
Best mean reward: 2104.39 - Last mean reward per episode: 2039.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.76e+03    |
|    ep_rew_mean          | 2018.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 34          |
|    time_elapsed         | 9885        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.008569358 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.52       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 251         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.000628   |
|    value_loss           | 462         |
-----------------------------------------
Num timesteps: 840000
Best mean reward: 2104.39 - Last mean reward per episode: 2017.40
Num timesteps: 852000
Best mean reward: 2104.39 - Last mean reward per episode: 1989.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.75e+03    |
|    ep_rew_mean          | 1998.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 35          |
|    time_elapsed         | 10181       |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.008056257 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.47       |
|    explained_variance   | 0.921       |
|    learning_rate        | 3e-06       |
|    loss                 | 312         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.000972   |
|    value_loss           | 445         |
-----------------------------------------
Num timesteps: 864000
Best mean reward: 2104.39 - Last mean reward per episode: 2000.15
Num timesteps: 876000
Best mean reward: 2104.39 - Last mean reward per episode: 2009.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.73e+03    |
|    ep_rew_mean          | 1990.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 36          |
|    time_elapsed         | 10478       |
|    total_timesteps      | 884736      |
| train/                  |             |
|    approx_kl            | 0.008040972 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 559         |
-----------------------------------------
Num timesteps: 888000
Best mean reward: 2104.39 - Last mean reward per episode: 1978.15
Num timesteps: 900000
Best mean reward: 2104.39 - Last mean reward per episode: 1990.68
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.66e+03   |
|    ep_rew_mean          | 1975.29    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 37         |
|    time_elapsed         | 10777      |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.00820512 |
|    clip_fraction        | 0.0992     |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.32      |
|    explained_variance   | 0.913      |
|    learning_rate        | 3e-06      |
|    loss                 | 215        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.00051   |
|    value_loss           | 532        |
----------------------------------------
Num timesteps: 912000
Best mean reward: 2104.39 - Last mean reward per episode: 1976.79
Num timesteps: 924000
Best mean reward: 2104.39 - Last mean reward per episode: 1956.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 1965.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 38          |
|    time_elapsed         | 11073       |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.008466725 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.909       |
|    learning_rate        | 3e-06       |
|    loss                 | 224         |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.000359   |
|    value_loss           | 532         |
-----------------------------------------
Num timesteps: 936000
Best mean reward: 2104.39 - Last mean reward per episode: 1965.71
Num timesteps: 948000
Best mean reward: 2104.39 - Last mean reward per episode: 1982.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.56e+03     |
|    ep_rew_mean          | 1978.14      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 39           |
|    time_elapsed         | 11364        |
|    total_timesteps      | 958464       |
| train/                  |              |
|    approx_kl            | 0.0074082282 |
|    clip_fraction        | 0.0924       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.946        |
|    learning_rate        | 3e-06        |
|    loss                 | 220          |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000219    |
|    value_loss           | 394          |
------------------------------------------
Num timesteps: 960000
Best mean reward: 2104.39 - Last mean reward per episode: 1978.53
Num timesteps: 972000
Best mean reward: 2104.39 - Last mean reward per episode: 1890.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.56e+03     |
|    ep_rew_mean          | 1905.79      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 40           |
|    time_elapsed         | 11658        |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0076002404 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.944        |
|    learning_rate        | 3e-06        |
|    loss                 | 72.5         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00058     |
|    value_loss           | 411          |
------------------------------------------
Num timesteps: 984000
Best mean reward: 2104.39 - Last mean reward per episode: 1902.94
Num timesteps: 996000
Best mean reward: 2104.39 - Last mean reward per episode: 1898.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 1904.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 41         |
|    time_elapsed         | 11951      |
|    total_timesteps      | 1007616    |
| train/                  |            |
|    approx_kl            | 0.00850828 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.41      |
|    explained_variance   | 0.934      |
|    learning_rate        | 3e-06      |
|    loss                 | 388        |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.00161   |
|    value_loss           | 494        |
----------------------------------------
Num timesteps: 1008000
Best mean reward: 2104.39 - Last mean reward per episode: 1906.92
Num timesteps: 1020000
Best mean reward: 2104.39 - Last mean reward per episode: 1874.77
Num timesteps: 1032000
Best mean reward: 2104.39 - Last mean reward per episode: 1858.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.47e+03    |
|    ep_rew_mean          | 1858.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 42          |
|    time_elapsed         | 12247       |
|    total_timesteps      | 1032192     |
| train/                  |             |
|    approx_kl            | 0.008319271 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.29       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000982   |
|    value_loss           | 557         |
-----------------------------------------
Num timesteps: 1044000
Best mean reward: 2104.39 - Last mean reward per episode: 1873.38
Num timesteps: 1056000
Best mean reward: 2104.39 - Last mean reward per episode: 1866.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 1866.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 43          |
|    time_elapsed         | 12539       |
|    total_timesteps      | 1056768     |
| train/                  |             |
|    approx_kl            | 0.008626626 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.14       |
|    explained_variance   | 0.912       |
|    learning_rate        | 3e-06       |
|    loss                 | 424         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 648         |
-----------------------------------------
Num timesteps: 1068000
Best mean reward: 2104.39 - Last mean reward per episode: 1889.04
Num timesteps: 1080000
Best mean reward: 2104.39 - Last mean reward per episode: 1870.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.46e+03     |
|    ep_rew_mean          | 1864.79      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 44           |
|    time_elapsed         | 12836        |
|    total_timesteps      | 1081344      |
| train/                  |              |
|    approx_kl            | 0.0087124435 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.14        |
|    explained_variance   | 0.914        |
|    learning_rate        | 3e-06        |
|    loss                 | 407          |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 634          |
------------------------------------------
Num timesteps: 1092000
Best mean reward: 2104.39 - Last mean reward per episode: 1894.92
Num timesteps: 1104000
Best mean reward: 2104.39 - Last mean reward per episode: 1893.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.39e+03    |
|    ep_rew_mean          | 1921.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 45          |
|    time_elapsed         | 13130       |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.009648025 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.895       |
|    learning_rate        | 3e-06       |
|    loss                 | 276         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 640         |
-----------------------------------------
Num timesteps: 1116000
Best mean reward: 2104.39 - Last mean reward per episode: 1927.14
Num timesteps: 1128000
Best mean reward: 2104.39 - Last mean reward per episode: 1952.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 1973.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 46          |
|    time_elapsed         | 13421       |
|    total_timesteps      | 1130496     |
| train/                  |             |
|    approx_kl            | 0.009506631 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.906       |
|    learning_rate        | 3e-06       |
|    loss                 | 318         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 1140000
Best mean reward: 2104.39 - Last mean reward per episode: 1921.87
Num timesteps: 1152000
Best mean reward: 2104.39 - Last mean reward per episode: 1952.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 1948.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 47          |
|    time_elapsed         | 13714       |
|    total_timesteps      | 1155072     |
| train/                  |             |
|    approx_kl            | 0.009655134 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 590         |
-----------------------------------------
Num timesteps: 1164000
Best mean reward: 2104.39 - Last mean reward per episode: 1936.42
Num timesteps: 1176000
Best mean reward: 2104.39 - Last mean reward per episode: 1942.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 1948.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 48          |
|    time_elapsed         | 14009       |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.009487769 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.01       |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 214         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 628         |
-----------------------------------------
Num timesteps: 1188000
Best mean reward: 2104.39 - Last mean reward per episode: 1965.34
Num timesteps: 1200000
Best mean reward: 2104.39 - Last mean reward per episode: 1983.92
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.33e+03     |
|    ep_rew_mean          | 1990.46      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 49           |
|    time_elapsed         | 14300        |
|    total_timesteps      | 1204224      |
| train/                  |              |
|    approx_kl            | 0.0089955395 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.02        |
|    explained_variance   | 0.909        |
|    learning_rate        | 3e-06        |
|    loss                 | 234          |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 624          |
------------------------------------------
Num timesteps: 1212000
Best mean reward: 2104.39 - Last mean reward per episode: 1986.88
Num timesteps: 1224000
Best mean reward: 2104.39 - Last mean reward per episode: 1941.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 1955.75     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 50          |
|    time_elapsed         | 14591       |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.010048386 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 303         |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 1236000
Best mean reward: 2104.39 - Last mean reward per episode: 1962.69
Num timesteps: 1248000
Best mean reward: 2104.39 - Last mean reward per episode: 1942.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.24e+03   |
|    ep_rew_mean          | 1890.42    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 51         |
|    time_elapsed         | 14886      |
|    total_timesteps      | 1253376    |
| train/                  |            |
|    approx_kl            | 0.00980165 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.94      |
|    explained_variance   | 0.915      |
|    learning_rate        | 3e-06      |
|    loss                 | 559        |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00163   |
|    value_loss           | 608        |
----------------------------------------
Num timesteps: 1260000
Best mean reward: 2104.39 - Last mean reward per episode: 1912.09
Num timesteps: 1272000
Best mean reward: 2104.39 - Last mean reward per episode: 1912.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 1893.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 52          |
|    time_elapsed         | 15179       |
|    total_timesteps      | 1277952     |
| train/                  |             |
|    approx_kl            | 0.010225041 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.9        |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 422         |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 661         |
-----------------------------------------
Num timesteps: 1284000
Best mean reward: 2104.39 - Last mean reward per episode: 1892.32
Num timesteps: 1296000
Best mean reward: 2104.39 - Last mean reward per episode: 1917.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 1897.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 53          |
|    time_elapsed         | 15474       |
|    total_timesteps      | 1302528     |
| train/                  |             |
|    approx_kl            | 0.009864786 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.89       |
|    explained_variance   | 0.913       |
|    learning_rate        | 3e-06       |
|    loss                 | 194         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 718         |
-----------------------------------------
Num timesteps: 1308000
Best mean reward: 2104.39 - Last mean reward per episode: 1900.97
Num timesteps: 1320000
Best mean reward: 2104.39 - Last mean reward per episode: 1888.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.21e+03    |
|    ep_rew_mean          | 1899.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 54          |
|    time_elapsed         | 15770       |
|    total_timesteps      | 1327104     |
| train/                  |             |
|    approx_kl            | 0.009636044 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 313         |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 623         |
-----------------------------------------
Num timesteps: 1332000
Best mean reward: 2104.39 - Last mean reward per episode: 1900.02
Num timesteps: 1344000
Best mean reward: 2104.39 - Last mean reward per episode: 1928.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.21e+03    |
|    ep_rew_mean          | 1929.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 55          |
|    time_elapsed         | 16064       |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.009385503 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.913       |
|    learning_rate        | 3e-06       |
|    loss                 | 311         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.000778   |
|    value_loss           | 753         |
-----------------------------------------
Num timesteps: 1356000
Best mean reward: 2104.39 - Last mean reward per episode: 1943.71
Num timesteps: 1368000
Best mean reward: 2104.39 - Last mean reward per episode: 1968.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.23e+03     |
|    ep_rew_mean          | 1999.73      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 56           |
|    time_elapsed         | 16357        |
|    total_timesteps      | 1376256      |
| train/                  |              |
|    approx_kl            | 0.0097677875 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.74        |
|    explained_variance   | 0.911        |
|    learning_rate        | 3e-06        |
|    loss                 | 701          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 765          |
------------------------------------------
Num timesteps: 1380000
Best mean reward: 2104.39 - Last mean reward per episode: 1990.63
Num timesteps: 1392000
Best mean reward: 2104.39 - Last mean reward per episode: 2031.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.25e+03     |
|    ep_rew_mean          | 2046.05      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 57           |
|    time_elapsed         | 16653        |
|    total_timesteps      | 1400832      |
| train/                  |              |
|    approx_kl            | 0.0091545405 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.73        |
|    explained_variance   | 0.917        |
|    learning_rate        | 3e-06        |
|    loss                 | 204          |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 738          |
------------------------------------------
Num timesteps: 1404000
Best mean reward: 2104.39 - Last mean reward per episode: 2060.72
Num timesteps: 1416000
Best mean reward: 2104.39 - Last mean reward per episode: 2061.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 2088.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 58          |
|    time_elapsed         | 16949       |
|    total_timesteps      | 1425408     |
| train/                  |             |
|    approx_kl            | 0.009358622 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 151         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 687         |
-----------------------------------------
Num timesteps: 1428000
Best mean reward: 2104.39 - Last mean reward per episode: 2077.52
Num timesteps: 1440000
Best mean reward: 2104.39 - Last mean reward per episode: 2079.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2073.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 59          |
|    time_elapsed         | 17244       |
|    total_timesteps      | 1449984     |
| train/                  |             |
|    approx_kl            | 0.010389641 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 211         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 703         |
-----------------------------------------
Num timesteps: 1452000
Best mean reward: 2104.39 - Last mean reward per episode: 2083.12
Num timesteps: 1464000
Best mean reward: 2104.39 - Last mean reward per episode: 2080.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2073.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 60          |
|    time_elapsed         | 17537       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.011292889 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.925       |
|    learning_rate        | 3e-06       |
|    loss                 | 188         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 767         |
-----------------------------------------
Num timesteps: 1476000
Best mean reward: 2104.39 - Last mean reward per episode: 2073.38
Num timesteps: 1488000
Best mean reward: 2104.39 - Last mean reward per episode: 2096.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.32e+03    |
|    ep_rew_mean          | 2123.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 61          |
|    time_elapsed         | 17830       |
|    total_timesteps      | 1499136     |
| train/                  |             |
|    approx_kl            | 0.009676444 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 455         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 676         |
-----------------------------------------
Num timesteps: 1500000
Best mean reward: 2104.39 - Last mean reward per episode: 2123.02
Saving new best model to tmp/best_model
Num timesteps: 1512000
Best mean reward: 2123.02 - Last mean reward per episode: 2137.96
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.38e+03    |
|    ep_rew_mean          | 2116.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 62          |
|    time_elapsed         | 18125       |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.010773924 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 232         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 659         |
-----------------------------------------
Num timesteps: 1524000
Best mean reward: 2137.96 - Last mean reward per episode: 2116.67
Num timesteps: 1536000
Best mean reward: 2137.96 - Last mean reward per episode: 2105.02
Num timesteps: 1548000
Best mean reward: 2137.96 - Last mean reward per episode: 2116.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 2116.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 63          |
|    time_elapsed         | 18418       |
|    total_timesteps      | 1548288     |
| train/                  |             |
|    approx_kl            | 0.009775658 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.93        |
|    learning_rate        | 3e-06       |
|    loss                 | 440         |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 717         |
-----------------------------------------
Num timesteps: 1560000
Best mean reward: 2137.96 - Last mean reward per episode: 2071.37
Num timesteps: 1572000
Best mean reward: 2137.96 - Last mean reward per episode: 2059.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.33e+03     |
|    ep_rew_mean          | 2059.77      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 64           |
|    time_elapsed         | 18712        |
|    total_timesteps      | 1572864      |
| train/                  |              |
|    approx_kl            | 0.0099542625 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | 0.911        |
|    learning_rate        | 3e-06        |
|    loss                 | 440          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 896          |
------------------------------------------
Num timesteps: 1584000
Best mean reward: 2137.96 - Last mean reward per episode: 2071.69
Num timesteps: 1596000
Best mean reward: 2137.96 - Last mean reward per episode: 2077.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2077.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 65          |
|    time_elapsed         | 19003       |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.011037189 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 461         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 846         |
-----------------------------------------
Num timesteps: 1608000
Best mean reward: 2137.96 - Last mean reward per episode: 2042.67
Num timesteps: 1620000
Best mean reward: 2137.96 - Last mean reward per episode: 1983.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 1988.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 66          |
|    time_elapsed         | 19294       |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.010591355 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.91        |
|    learning_rate        | 3e-06       |
|    loss                 | 498         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 855         |
-----------------------------------------
Num timesteps: 1632000
Best mean reward: 2137.96 - Last mean reward per episode: 1961.67
Num timesteps: 1644000
Best mean reward: 2137.96 - Last mean reward per episode: 1908.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.16e+03    |
|    ep_rew_mean          | 1909.1      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 67          |
|    time_elapsed         | 19592       |
|    total_timesteps      | 1646592     |
| train/                  |             |
|    approx_kl            | 0.011177283 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.912       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 827         |
-----------------------------------------
Num timesteps: 1656000
Best mean reward: 2137.96 - Last mean reward per episode: 1933.86
Num timesteps: 1668000
Best mean reward: 2137.96 - Last mean reward per episode: 1944.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.18e+03    |
|    ep_rew_mean          | 1947.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 68          |
|    time_elapsed         | 19881       |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.010736622 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-06       |
|    loss                 | 450         |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 798         |
-----------------------------------------
Num timesteps: 1680000
Best mean reward: 2137.96 - Last mean reward per episode: 1956.88
Num timesteps: 1692000
Best mean reward: 2137.96 - Last mean reward per episode: 1975.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.19e+03    |
|    ep_rew_mean          | 1975.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 69          |
|    time_elapsed         | 20171       |
|    total_timesteps      | 1695744     |
| train/                  |             |
|    approx_kl            | 0.010727134 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.924       |
|    learning_rate        | 3e-06       |
|    loss                 | 615         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00193    |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 1704000
Best mean reward: 2137.96 - Last mean reward per episode: 1962.67
Num timesteps: 1716000
Best mean reward: 2137.96 - Last mean reward per episode: 1971.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.17e+03    |
|    ep_rew_mean          | 1971.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 70          |
|    time_elapsed         | 20467       |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.010479946 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 570         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 841         |
-----------------------------------------
Num timesteps: 1728000
Best mean reward: 2137.96 - Last mean reward per episode: 1960.04
Num timesteps: 1740000
Best mean reward: 2137.96 - Last mean reward per episode: 2057.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 2044.76     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 71          |
|    time_elapsed         | 20753       |
|    total_timesteps      | 1744896     |
| train/                  |             |
|    approx_kl            | 0.010274403 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.923       |
|    learning_rate        | 3e-06       |
|    loss                 | 291         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 820         |
-----------------------------------------
Num timesteps: 1752000
Best mean reward: 2137.96 - Last mean reward per episode: 2063.51
Num timesteps: 1764000
Best mean reward: 2137.96 - Last mean reward per episode: 2129.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2136.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 72          |
|    time_elapsed         | 21043       |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.012156661 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 233         |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 779         |
-----------------------------------------
Num timesteps: 1776000
Best mean reward: 2137.96 - Last mean reward per episode: 2127.64
Num timesteps: 1788000
Best mean reward: 2137.96 - Last mean reward per episode: 2124.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 2156.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 73          |
|    time_elapsed         | 21332       |
|    total_timesteps      | 1794048     |
| train/                  |             |
|    approx_kl            | 0.012877218 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 248         |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 738         |
-----------------------------------------
Num timesteps: 1800000
Best mean reward: 2137.96 - Last mean reward per episode: 2170.79
Saving new best model to tmp/best_model
Num timesteps: 1812000
Best mean reward: 2170.79 - Last mean reward per episode: 2169.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2179.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 74          |
|    time_elapsed         | 21619       |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.011774891 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 164         |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 775         |
-----------------------------------------
Num timesteps: 1824000
Best mean reward: 2170.79 - Last mean reward per episode: 2188.38
Saving new best model to tmp/best_model
Num timesteps: 1836000
Best mean reward: 2188.38 - Last mean reward per episode: 2178.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2187.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 75          |
|    time_elapsed         | 21915       |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.010482428 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 705         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 911         |
-----------------------------------------
Num timesteps: 1848000
Best mean reward: 2188.38 - Last mean reward per episode: 2218.00
Saving new best model to tmp/best_model
Num timesteps: 1860000
Best mean reward: 2218.00 - Last mean reward per episode: 2229.37
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 2186.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 76          |
|    time_elapsed         | 22214       |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.011233815 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 212         |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.000645   |
|    value_loss           | 810         |
-----------------------------------------
Num timesteps: 1872000
Best mean reward: 2229.37 - Last mean reward per episode: 2192.06
Num timesteps: 1884000
Best mean reward: 2229.37 - Last mean reward per episode: 2200.49
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.27e+03   |
|    ep_rew_mean          | 2188.27    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 77         |
|    time_elapsed         | 22509      |
|    total_timesteps      | 1892352    |
| train/                  |            |
|    approx_kl            | 0.01177654 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.926      |
|    learning_rate        | 3e-06      |
|    loss                 | 569        |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00153   |
|    value_loss           | 847        |
----------------------------------------
Num timesteps: 1896000
Best mean reward: 2229.37 - Last mean reward per episode: 2205.92
Num timesteps: 1908000
Best mean reward: 2229.37 - Last mean reward per episode: 2207.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 2189.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 78          |
|    time_elapsed         | 22800       |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.009708225 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 886         |
-----------------------------------------
Num timesteps: 1920000
Best mean reward: 2229.37 - Last mean reward per episode: 2194.05
Num timesteps: 1932000
Best mean reward: 2229.37 - Last mean reward per episode: 2241.18
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.28e+03    |
|    ep_rew_mean          | 2246.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 79          |
|    time_elapsed         | 23090       |
|    total_timesteps      | 1941504     |
| train/                  |             |
|    approx_kl            | 0.012463663 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-06       |
|    loss                 | 231         |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 859         |
-----------------------------------------
Num timesteps: 1944000
Best mean reward: 2241.18 - Last mean reward per episode: 2269.58
Saving new best model to tmp/best_model
Num timesteps: 1956000
Best mean reward: 2269.58 - Last mean reward per episode: 2273.03
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 2267.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 80          |
|    time_elapsed         | 23382       |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.011663042 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 338         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 671         |
-----------------------------------------
Num timesteps: 1968000
Best mean reward: 2273.03 - Last mean reward per episode: 2264.86
Num timesteps: 1980000
Best mean reward: 2273.03 - Last mean reward per episode: 2204.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.28e+03    |
|    ep_rew_mean          | 2214.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 81          |
|    time_elapsed         | 23670       |
|    total_timesteps      | 1990656     |
| train/                  |             |
|    approx_kl            | 0.011269095 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 537         |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00162    |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 1992000
Best mean reward: 2273.03 - Last mean reward per episode: 2214.95
Num timesteps: 2004000
Best mean reward: 2273.03 - Last mean reward per episode: 2226.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.29e+03   |
|    ep_rew_mean          | 2238.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 82         |
|    time_elapsed         | 23964      |
|    total_timesteps      | 2015232    |
| train/                  |            |
|    approx_kl            | 0.01317851 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.71      |
|    explained_variance   | 0.915      |
|    learning_rate        | 3e-06      |
|    loss                 | 340        |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.000859  |
|    value_loss           | 896        |
----------------------------------------
Num timesteps: 2016000
Best mean reward: 2273.03 - Last mean reward per episode: 2238.94
Num timesteps: 2028000
Best mean reward: 2273.03 - Last mean reward per episode: 2207.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2173.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 83          |
|    time_elapsed         | 24250       |
|    total_timesteps      | 2039808     |
| train/                  |             |
|    approx_kl            | 0.013038245 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 736         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 988         |
-----------------------------------------
Num timesteps: 2040000
Best mean reward: 2273.03 - Last mean reward per episode: 2173.83
Num timesteps: 2052000
Best mean reward: 2273.03 - Last mean reward per episode: 2163.80
Num timesteps: 2064000
Best mean reward: 2273.03 - Last mean reward per episode: 2118.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 2118.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 84          |
|    time_elapsed         | 24542       |
|    total_timesteps      | 2064384     |
| train/                  |             |
|    approx_kl            | 0.012702306 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 413         |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 883         |
-----------------------------------------
Num timesteps: 2076000
Best mean reward: 2273.03 - Last mean reward per episode: 2141.88
Num timesteps: 2088000
Best mean reward: 2273.03 - Last mean reward per episode: 2134.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.24e+03    |
|    ep_rew_mean          | 2118.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 85          |
|    time_elapsed         | 24833       |
|    total_timesteps      | 2088960     |
| train/                  |             |
|    approx_kl            | 0.011585715 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 397         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00068    |
|    value_loss           | 879         |
-----------------------------------------
Num timesteps: 2100000
Best mean reward: 2273.03 - Last mean reward per episode: 2169.57
Num timesteps: 2112000
Best mean reward: 2273.03 - Last mean reward per episode: 2150.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.24e+03    |
|    ep_rew_mean          | 2150.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 86          |
|    time_elapsed         | 25121       |
|    total_timesteps      | 2113536     |
| train/                  |             |
|    approx_kl            | 0.010577741 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 285         |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.000786   |
|    value_loss           | 817         |
-----------------------------------------
Num timesteps: 2124000
Best mean reward: 2273.03 - Last mean reward per episode: 2186.41
Num timesteps: 2136000
Best mean reward: 2273.03 - Last mean reward per episode: 2164.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 2189.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 87          |
|    time_elapsed         | 25409       |
|    total_timesteps      | 2138112     |
| train/                  |             |
|    approx_kl            | 0.012394653 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 330         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 830         |
-----------------------------------------
Num timesteps: 2148000
Best mean reward: 2273.03 - Last mean reward per episode: 2168.34
Num timesteps: 2160000
Best mean reward: 2273.03 - Last mean reward per episode: 2208.36
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.26e+03     |
|    ep_rew_mean          | 2229.55      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 88           |
|    time_elapsed         | 25702        |
|    total_timesteps      | 2162688      |
| train/                  |              |
|    approx_kl            | 0.0124074435 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.93         |
|    learning_rate        | 3e-06        |
|    loss                 | 246          |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 802          |
------------------------------------------
Num timesteps: 2172000
Best mean reward: 2273.03 - Last mean reward per episode: 2237.45
Num timesteps: 2184000
Best mean reward: 2273.03 - Last mean reward per episode: 2252.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2287.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 89          |
|    time_elapsed         | 25990       |
|    total_timesteps      | 2187264     |
| train/                  |             |
|    approx_kl            | 0.011671756 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-06       |
|    loss                 | 198         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 878         |
-----------------------------------------
Num timesteps: 2196000
Best mean reward: 2273.03 - Last mean reward per episode: 2263.88
Num timesteps: 2208000
Best mean reward: 2273.03 - Last mean reward per episode: 2219.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2236.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 90          |
|    time_elapsed         | 26285       |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.011866022 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.928       |
|    learning_rate        | 3e-06       |
|    loss                 | 298         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.000931   |
|    value_loss           | 768         |
-----------------------------------------
Num timesteps: 2220000
Best mean reward: 2273.03 - Last mean reward per episode: 2285.19
Saving new best model to tmp/best_model
Num timesteps: 2232000
Best mean reward: 2285.19 - Last mean reward per episode: 2259.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2278.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 91          |
|    time_elapsed         | 26577       |
|    total_timesteps      | 2236416     |
| train/                  |             |
|    approx_kl            | 0.012515928 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.928       |
|    learning_rate        | 3e-06       |
|    loss                 | 643         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 846         |
-----------------------------------------
Num timesteps: 2244000
Best mean reward: 2285.19 - Last mean reward per episode: 2262.09
Num timesteps: 2256000
Best mean reward: 2285.19 - Last mean reward per episode: 2253.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2271.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 92          |
|    time_elapsed         | 26864       |
|    total_timesteps      | 2260992     |
| train/                  |             |
|    approx_kl            | 0.012332812 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 457         |
|    n_updates            | 910         |
|    policy_gradient_loss | 0.000925    |
|    value_loss           | 682         |
-----------------------------------------
Num timesteps: 2268000
Best mean reward: 2285.19 - Last mean reward per episode: 2307.08
Saving new best model to tmp/best_model
Num timesteps: 2280000
Best mean reward: 2307.08 - Last mean reward per episode: 2323.46
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2329.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 93          |
|    time_elapsed         | 27156       |
|    total_timesteps      | 2285568     |
| train/                  |             |
|    approx_kl            | 0.012507718 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-06       |
|    loss                 | 405         |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.000894   |
|    value_loss           | 828         |
-----------------------------------------
Num timesteps: 2292000
Best mean reward: 2323.46 - Last mean reward per episode: 2294.62
Num timesteps: 2304000
Best mean reward: 2323.46 - Last mean reward per episode: 2362.29
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.33e+03    |
|    ep_rew_mean          | 2328.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 94          |
|    time_elapsed         | 27445       |
|    total_timesteps      | 2310144     |
| train/                  |             |
|    approx_kl            | 0.012237929 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 515         |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 798         |
-----------------------------------------
Num timesteps: 2316000
Best mean reward: 2362.29 - Last mean reward per episode: 2319.61
Num timesteps: 2328000
Best mean reward: 2362.29 - Last mean reward per episode: 2364.93
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.36e+03    |
|    ep_rew_mean          | 2396.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 95          |
|    time_elapsed         | 27741       |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.012453693 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 363         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 755         |
-----------------------------------------
Num timesteps: 2340000
Best mean reward: 2364.93 - Last mean reward per episode: 2393.49
Saving new best model to tmp/best_model
Num timesteps: 2352000
Best mean reward: 2393.49 - Last mean reward per episode: 2451.08
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.38e+03     |
|    ep_rew_mean          | 2437.31      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 96           |
|    time_elapsed         | 28034        |
|    total_timesteps      | 2359296      |
| train/                  |              |
|    approx_kl            | 0.0138937645 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.922        |
|    learning_rate        | 3e-06        |
|    loss                 | 283          |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000967    |
|    value_loss           | 803          |
------------------------------------------
Num timesteps: 2364000
Best mean reward: 2451.08 - Last mean reward per episode: 2448.47
Num timesteps: 2376000
Best mean reward: 2451.08 - Last mean reward per episode: 2456.92
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 2443.98    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 97         |
|    time_elapsed         | 28332      |
|    total_timesteps      | 2383872    |
| train/                  |            |
|    approx_kl            | 0.01297789 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.927      |
|    learning_rate        | 3e-06      |
|    loss                 | 266        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00136   |
|    value_loss           | 795        |
----------------------------------------
Num timesteps: 2388000
Best mean reward: 2456.92 - Last mean reward per episode: 2443.98
Num timesteps: 2400000
Best mean reward: 2456.92 - Last mean reward per episode: 2448.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 2455.19    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 98         |
|    time_elapsed         | 28633      |
|    total_timesteps      | 2408448    |
| train/                  |            |
|    approx_kl            | 0.01110101 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.934      |
|    learning_rate        | 3e-06      |
|    loss                 | 593        |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0019    |
|    value_loss           | 773        |
----------------------------------------
Num timesteps: 2412000
Best mean reward: 2456.92 - Last mean reward per episode: 2435.68
Num timesteps: 2424000
Best mean reward: 2456.92 - Last mean reward per episode: 2494.74
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 2515.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 99          |
|    time_elapsed         | 28922       |
|    total_timesteps      | 2433024     |
| train/                  |             |
|    approx_kl            | 0.014103527 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 602         |
-----------------------------------------
Num timesteps: 2436000
Best mean reward: 2494.74 - Last mean reward per episode: 2513.80
Saving new best model to tmp/best_model
Num timesteps: 2448000
Best mean reward: 2513.80 - Last mean reward per episode: 2521.39
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.48e+03    |
|    ep_rew_mean          | 2507.15     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 100         |
|    time_elapsed         | 29217       |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.013957006 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 167         |
|    n_updates            | 990         |
|    policy_gradient_loss | -9.73e-05   |
|    value_loss           | 744         |
-----------------------------------------
Num timesteps: 2460000
Best mean reward: 2521.39 - Last mean reward per episode: 2476.23
Num timesteps: 2472000
Best mean reward: 2521.39 - Last mean reward per episode: 2477.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.42e+03    |
|    ep_rew_mean          | 2437.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 101         |
|    time_elapsed         | 29509       |
|    total_timesteps      | 2482176     |
| train/                  |             |
|    approx_kl            | 0.014229674 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 589         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 884         |
-----------------------------------------
Num timesteps: 2484000
Best mean reward: 2521.39 - Last mean reward per episode: 2447.85
Num timesteps: 2496000
Best mean reward: 2521.39 - Last mean reward per episode: 2401.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.41e+03    |
|    ep_rew_mean          | 2402.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 102         |
|    time_elapsed         | 29796       |
|    total_timesteps      | 2506752     |
| train/                  |             |
|    approx_kl            | 0.013196009 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 462         |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 726         |
-----------------------------------------
Num timesteps: 2508000
Best mean reward: 2521.39 - Last mean reward per episode: 2381.22
Num timesteps: 2520000
Best mean reward: 2521.39 - Last mean reward per episode: 2357.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 2339.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 103         |
|    time_elapsed         | 30090       |
|    total_timesteps      | 2531328     |
| train/                  |             |
|    approx_kl            | 0.012588333 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.93        |
|    learning_rate        | 3e-06       |
|    loss                 | 171         |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 883         |
-----------------------------------------
Num timesteps: 2532000
Best mean reward: 2521.39 - Last mean reward per episode: 2339.04
Num timesteps: 2544000
Best mean reward: 2521.39 - Last mean reward per episode: 2280.47
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.32e+03    |
|    ep_rew_mean          | 2327.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 104         |
|    time_elapsed         | 30382       |
|    total_timesteps      | 2555904     |
| train/                  |             |
|    approx_kl            | 0.011977837 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 498         |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 806         |
-----------------------------------------
Num timesteps: 2556000
Best mean reward: 2521.39 - Last mean reward per episode: 2327.93
Num timesteps: 2568000
Best mean reward: 2521.39 - Last mean reward per episode: 2339.39
Num timesteps: 2580000
Best mean reward: 2521.39 - Last mean reward per episode: 2309.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2309.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 105         |
|    time_elapsed         | 30676       |
|    total_timesteps      | 2580480     |
| train/                  |             |
|    approx_kl            | 0.012505506 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.923       |
|    learning_rate        | 3e-06       |
|    loss                 | 423         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 855         |
-----------------------------------------
Num timesteps: 2592000
Best mean reward: 2521.39 - Last mean reward per episode: 2355.17
Num timesteps: 2604000
Best mean reward: 2521.39 - Last mean reward per episode: 2335.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2335.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 106         |
|    time_elapsed         | 30965       |
|    total_timesteps      | 2605056     |
| train/                  |             |
|    approx_kl            | 0.011232421 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 380         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 731         |
-----------------------------------------
Num timesteps: 2616000
Best mean reward: 2521.39 - Last mean reward per episode: 2342.77
Num timesteps: 2628000
Best mean reward: 2521.39 - Last mean reward per episode: 2355.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2355.43     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 107         |
|    time_elapsed         | 31254       |
|    total_timesteps      | 2629632     |
| train/                  |             |
|    approx_kl            | 0.013537613 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-06       |
|    loss                 | 360         |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 935         |
-----------------------------------------
Num timesteps: 2640000
Best mean reward: 2521.39 - Last mean reward per episode: 2346.75
Num timesteps: 2652000
Best mean reward: 2521.39 - Last mean reward per episode: 2412.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.34e+03    |
|    ep_rew_mean          | 2423.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 108         |
|    time_elapsed         | 31543       |
|    total_timesteps      | 2654208     |
| train/                  |             |
|    approx_kl            | 0.012660076 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 453         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.000999   |
|    value_loss           | 761         |
-----------------------------------------
Num timesteps: 2664000
Best mean reward: 2521.39 - Last mean reward per episode: 2458.67
Num timesteps: 2676000
Best mean reward: 2521.39 - Last mean reward per episode: 2475.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 2462.66    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 109        |
|    time_elapsed         | 31827      |
|    total_timesteps      | 2678784    |
| train/                  |            |
|    approx_kl            | 0.01272541 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.99      |
|    explained_variance   | 0.935      |
|    learning_rate        | 3e-06      |
|    loss                 | 522        |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.00229   |
|    value_loss           | 746        |
----------------------------------------
Num timesteps: 2688000
Best mean reward: 2521.39 - Last mean reward per episode: 2422.99
Num timesteps: 2700000
Best mean reward: 2521.39 - Last mean reward per episode: 2367.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.35e+03    |
|    ep_rew_mean          | 2360.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 110         |
|    time_elapsed         | 32113       |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.012332388 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 347         |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 765         |
-----------------------------------------
Num timesteps: 2712000
Best mean reward: 2521.39 - Last mean reward per episode: 2369.72
Num timesteps: 2724000
Best mean reward: 2521.39 - Last mean reward per episode: 2313.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.35e+03    |
|    ep_rew_mean          | 2311.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 111         |
|    time_elapsed         | 32400       |
|    total_timesteps      | 2727936     |
| train/                  |             |
|    approx_kl            | 0.012930104 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 684         |
-----------------------------------------
Num timesteps: 2736000
Best mean reward: 2521.39 - Last mean reward per episode: 2344.02
Num timesteps: 2748000
Best mean reward: 2521.39 - Last mean reward per episode: 2359.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.37e+03     |
|    ep_rew_mean          | 2344.87      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 112          |
|    time_elapsed         | 32690        |
|    total_timesteps      | 2752512      |
| train/                  |              |
|    approx_kl            | 0.0118833715 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.81        |
|    explained_variance   | 0.949        |
|    learning_rate        | 3e-06        |
|    loss                 | 217          |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.000303    |
|    value_loss           | 700          |
------------------------------------------
Num timesteps: 2760000
Best mean reward: 2521.39 - Last mean reward per episode: 2393.17
Num timesteps: 2772000
Best mean reward: 2521.39 - Last mean reward per episode: 2369.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.4e+03     |
|    ep_rew_mean          | 2381.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 113         |
|    time_elapsed         | 32977       |
|    total_timesteps      | 2777088     |
| train/                  |             |
|    approx_kl            | 0.013337155 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 699         |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 838         |
-----------------------------------------
Num timesteps: 2784000
Best mean reward: 2521.39 - Last mean reward per episode: 2373.62
Num timesteps: 2796000
Best mean reward: 2521.39 - Last mean reward per episode: 2364.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.43e+03    |
|    ep_rew_mean          | 2388.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 114         |
|    time_elapsed         | 33267       |
|    total_timesteps      | 2801664     |
| train/                  |             |
|    approx_kl            | 0.013219532 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 401         |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 793         |
-----------------------------------------
Num timesteps: 2808000
Best mean reward: 2521.39 - Last mean reward per episode: 2401.19
Num timesteps: 2820000
Best mean reward: 2521.39 - Last mean reward per episode: 2380.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 2401.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 115         |
|    time_elapsed         | 33561       |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.012826628 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 398         |
|    n_updates            | 1140        |
|    policy_gradient_loss | 0.000142    |
|    value_loss           | 631         |
-----------------------------------------
Num timesteps: 2832000
Best mean reward: 2521.39 - Last mean reward per episode: 2396.95
Num timesteps: 2844000
Best mean reward: 2521.39 - Last mean reward per episode: 2348.06
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.42e+03   |
|    ep_rew_mean          | 2356.69    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 116        |
|    time_elapsed         | 33856      |
|    total_timesteps      | 2850816    |
| train/                  |            |
|    approx_kl            | 0.01304021 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 176        |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.00183   |
|    value_loss           | 767        |
----------------------------------------
Num timesteps: 2856000
Best mean reward: 2521.39 - Last mean reward per episode: 2370.89
Num timesteps: 2868000
Best mean reward: 2521.39 - Last mean reward per episode: 2429.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 2468.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 117         |
|    time_elapsed         | 34141       |
|    total_timesteps      | 2875392     |
| train/                  |             |
|    approx_kl            | 0.013269247 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 773         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 837         |
-----------------------------------------
Num timesteps: 2880000
Best mean reward: 2521.39 - Last mean reward per episode: 2445.86
Num timesteps: 2892000
Best mean reward: 2521.39 - Last mean reward per episode: 2409.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 2401.64    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 118        |
|    time_elapsed         | 34429      |
|    total_timesteps      | 2899968    |
| train/                  |            |
|    approx_kl            | 0.01272653 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.76      |
|    explained_variance   | 0.936      |
|    learning_rate        | 3e-06      |
|    loss                 | 534        |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.000589  |
|    value_loss           | 751        |
----------------------------------------
Num timesteps: 2904000
Best mean reward: 2521.39 - Last mean reward per episode: 2398.76
Num timesteps: 2916000
Best mean reward: 2521.39 - Last mean reward per episode: 2483.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 2495.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 119         |
|    time_elapsed         | 34720       |
|    total_timesteps      | 2924544     |
| train/                  |             |
|    approx_kl            | 0.014711112 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 315         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.000711   |
|    value_loss           | 806         |
-----------------------------------------
Num timesteps: 2928000
Best mean reward: 2521.39 - Last mean reward per episode: 2487.75
Num timesteps: 2940000
Best mean reward: 2521.39 - Last mean reward per episode: 2445.03
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 2440.8     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 120        |
|    time_elapsed         | 35011      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.01349657 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.71      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 362        |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.00115   |
|    value_loss           | 788        |
----------------------------------------
Num timesteps: 2952000
Best mean reward: 2521.39 - Last mean reward per episode: 2459.72
Num timesteps: 2964000
Best mean reward: 2521.39 - Last mean reward per episode: 2500.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 2491.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 121         |
|    time_elapsed         | 35302       |
|    total_timesteps      | 2973696     |
| train/                  |             |
|    approx_kl            | 0.013193662 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 543         |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 688         |
-----------------------------------------
Num timesteps: 2976000
Best mean reward: 2521.39 - Last mean reward per episode: 2491.23
Num timesteps: 2988000
Best mean reward: 2521.39 - Last mean reward per episode: 2575.01
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.47e+03    |
|    ep_rew_mean          | 2610.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 122         |
|    time_elapsed         | 35596       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.013159342 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 371         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 655         |
-----------------------------------------
Num timesteps: 3000000
Best mean reward: 2575.01 - Last mean reward per episode: 2606.81
Saving new best model to tmp/best_model
Num timesteps: 3012000
Best mean reward: 2606.81 - Last mean reward per episode: 2543.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.42e+03    |
|    ep_rew_mean          | 2509.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 123         |
|    time_elapsed         | 35883       |
|    total_timesteps      | 3022848     |
| train/                  |             |
|    approx_kl            | 0.014322226 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 676         |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 800         |
-----------------------------------------
Num timesteps: 3024000
Best mean reward: 2606.81 - Last mean reward per episode: 2550.54
Num timesteps: 3036000
Best mean reward: 2606.81 - Last mean reward per episode: 2583.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.48e+03    |
|    ep_rew_mean          | 2596.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 124         |
|    time_elapsed         | 36175       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.014385112 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 477         |
|    n_updates            | 1230        |
|    policy_gradient_loss | 1.09e-05    |
|    value_loss           | 892         |
-----------------------------------------
Num timesteps: 3048000
Best mean reward: 2606.81 - Last mean reward per episode: 2604.35
Num timesteps: 3060000
Best mean reward: 2606.81 - Last mean reward per episode: 2612.06
Saving new best model to tmp/best_model
Num timesteps: 3072000
Best mean reward: 2612.06 - Last mean reward per episode: 2610.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.49e+03    |
|    ep_rew_mean          | 2610.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 125         |
|    time_elapsed         | 36468       |
|    total_timesteps      | 3072000     |
| train/                  |             |
|    approx_kl            | 0.013414939 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 616         |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.000834   |
|    value_loss           | 784         |
-----------------------------------------
Num timesteps: 3084000
Best mean reward: 2612.06 - Last mean reward per episode: 2660.71
Saving new best model to tmp/best_model
Num timesteps: 3096000
Best mean reward: 2660.71 - Last mean reward per episode: 2698.04
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 2698.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 126         |
|    time_elapsed         | 36761       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.015505713 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.000918   |
|    value_loss           | 654         |
-----------------------------------------
Num timesteps: 3108000
Best mean reward: 2698.04 - Last mean reward per episode: 2657.18
Num timesteps: 3120000
Best mean reward: 2698.04 - Last mean reward per episode: 2694.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.56e+03    |
|    ep_rew_mean          | 2690.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 127         |
|    time_elapsed         | 37055       |
|    total_timesteps      | 3121152     |
| train/                  |             |
|    approx_kl            | 0.015645312 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.93        |
|    learning_rate        | 3e-06       |
|    loss                 | 571         |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.000617   |
|    value_loss           | 814         |
-----------------------------------------
Num timesteps: 3132000
Best mean reward: 2698.04 - Last mean reward per episode: 2751.22
Saving new best model to tmp/best_model
Num timesteps: 3144000
Best mean reward: 2751.22 - Last mean reward per episode: 2835.59
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.6e+03    |
|    ep_rew_mean          | 2823.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 128        |
|    time_elapsed         | 37346      |
|    total_timesteps      | 3145728    |
| train/                  |            |
|    approx_kl            | 0.01345595 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.68      |
|    explained_variance   | 0.936      |
|    learning_rate        | 3e-06      |
|    loss                 | 477        |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.00203   |
|    value_loss           | 779        |
----------------------------------------
Num timesteps: 3156000
Best mean reward: 2835.59 - Last mean reward per episode: 2781.80
Num timesteps: 3168000
Best mean reward: 2835.59 - Last mean reward per episode: 2836.80
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2843.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 129         |
|    time_elapsed         | 37637       |
|    total_timesteps      | 3170304     |
| train/                  |             |
|    approx_kl            | 0.014694002 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 1280        |
|    policy_gradient_loss | 0.000688    |
|    value_loss           | 803         |
-----------------------------------------
Num timesteps: 3180000
Best mean reward: 2836.80 - Last mean reward per episode: 2891.04
Saving new best model to tmp/best_model
Num timesteps: 3192000
Best mean reward: 2891.04 - Last mean reward per episode: 2870.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2870.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 130         |
|    time_elapsed         | 37932       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.012964033 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-06       |
|    loss                 | 242         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00081    |
|    value_loss           | 923         |
-----------------------------------------
Num timesteps: 3204000
Best mean reward: 2891.04 - Last mean reward per episode: 2884.04
Num timesteps: 3216000
Best mean reward: 2891.04 - Last mean reward per episode: 2919.78
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | 2986.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 131         |
|    time_elapsed         | 38225       |
|    total_timesteps      | 3219456     |
| train/                  |             |
|    approx_kl            | 0.015605616 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 361         |
|    n_updates            | 1300        |
|    policy_gradient_loss | 0.000512    |
|    value_loss           | 761         |
-----------------------------------------
Num timesteps: 3228000
Best mean reward: 2919.78 - Last mean reward per episode: 3020.66
Saving new best model to tmp/best_model
Num timesteps: 3240000
Best mean reward: 3020.66 - Last mean reward per episode: 2975.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 2929.76     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 132         |
|    time_elapsed         | 38519       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.013380739 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 311         |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 646         |
-----------------------------------------
Num timesteps: 3252000
Best mean reward: 3020.66 - Last mean reward per episode: 2992.27
Num timesteps: 3264000
Best mean reward: 3020.66 - Last mean reward per episode: 2944.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | 2960.77     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 133         |
|    time_elapsed         | 38811       |
|    total_timesteps      | 3268608     |
| train/                  |             |
|    approx_kl            | 0.015131214 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 579         |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.000484   |
|    value_loss           | 780         |
-----------------------------------------
Num timesteps: 3276000
Best mean reward: 3020.66 - Last mean reward per episode: 2899.34
Num timesteps: 3288000
Best mean reward: 3020.66 - Last mean reward per episode: 2908.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 2924.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 134         |
|    time_elapsed         | 39102       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.014190495 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 156         |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.000298    |
|    value_loss           | 795         |
-----------------------------------------
Num timesteps: 3300000
Best mean reward: 3020.66 - Last mean reward per episode: 2940.60
Num timesteps: 3312000
Best mean reward: 3020.66 - Last mean reward per episode: 2939.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 2964.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 135         |
|    time_elapsed         | 39391       |
|    total_timesteps      | 3317760     |
| train/                  |             |
|    approx_kl            | 0.015220266 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 369         |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 3324000
Best mean reward: 3020.66 - Last mean reward per episode: 2984.18
Num timesteps: 3336000
Best mean reward: 3020.66 - Last mean reward per episode: 2999.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 3019.32    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 136        |
|    time_elapsed         | 39687      |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.01501105 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.52      |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-06      |
|    loss                 | 324        |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.000874  |
|    value_loss           | 788        |
----------------------------------------
Num timesteps: 3348000
Best mean reward: 3020.66 - Last mean reward per episode: 3036.67
Saving new best model to tmp/best_model
Num timesteps: 3360000
Best mean reward: 3036.67 - Last mean reward per episode: 3041.83
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 3034.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 137         |
|    time_elapsed         | 39982       |
|    total_timesteps      | 3366912     |
| train/                  |             |
|    approx_kl            | 0.014543829 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 318         |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.000918   |
|    value_loss           | 903         |
-----------------------------------------
Num timesteps: 3372000
Best mean reward: 3041.83 - Last mean reward per episode: 3020.75
Num timesteps: 3384000
Best mean reward: 3041.83 - Last mean reward per episode: 2905.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2968.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 138         |
|    time_elapsed         | 40272       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.013495799 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 438         |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.000891   |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 3396000
Best mean reward: 3041.83 - Last mean reward per episode: 2959.75
Num timesteps: 3408000
Best mean reward: 3041.83 - Last mean reward per episode: 2966.82
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.64e+03   |
|    ep_rew_mean          | 3003.27    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 139        |
|    time_elapsed         | 40561      |
|    total_timesteps      | 3416064    |
| train/                  |            |
|    approx_kl            | 0.01504125 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.45      |
|    explained_variance   | 0.939      |
|    learning_rate        | 3e-06      |
|    loss                 | 268        |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.000518  |
|    value_loss           | 835        |
----------------------------------------
Num timesteps: 3420000
Best mean reward: 3041.83 - Last mean reward per episode: 2979.57
Num timesteps: 3432000
Best mean reward: 3041.83 - Last mean reward per episode: 2991.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | 3008.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 140         |
|    time_elapsed         | 40849       |
|    total_timesteps      | 3440640     |
| train/                  |             |
|    approx_kl            | 0.014641945 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.934       |
|    learning_rate        | 3e-06       |
|    loss                 | 583         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 840         |
-----------------------------------------
Num timesteps: 3444000
Best mean reward: 3041.83 - Last mean reward per episode: 3000.57
Num timesteps: 3456000
Best mean reward: 3041.83 - Last mean reward per episode: 2935.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 2913.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 141         |
|    time_elapsed         | 41139       |
|    total_timesteps      | 3465216     |
| train/                  |             |
|    approx_kl            | 0.014062027 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 386         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.000148   |
|    value_loss           | 817         |
-----------------------------------------
Num timesteps: 3468000
Best mean reward: 3041.83 - Last mean reward per episode: 2890.76
Num timesteps: 3480000
Best mean reward: 3041.83 - Last mean reward per episode: 2833.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.58e+03    |
|    ep_rew_mean          | 2868.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 142         |
|    time_elapsed         | 41427       |
|    total_timesteps      | 3489792     |
| train/                  |             |
|    approx_kl            | 0.015076528 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 724         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 978         |
-----------------------------------------
Num timesteps: 3492000
Best mean reward: 3041.83 - Last mean reward per episode: 2860.71
Num timesteps: 3504000
Best mean reward: 3041.83 - Last mean reward per episode: 2778.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.55e+03    |
|    ep_rew_mean          | 2774.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 143         |
|    time_elapsed         | 41715       |
|    total_timesteps      | 3514368     |
| train/                  |             |
|    approx_kl            | 0.016032754 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 790         |
-----------------------------------------
Num timesteps: 3516000
Best mean reward: 3041.83 - Last mean reward per episode: 2776.87
Num timesteps: 3528000
Best mean reward: 3041.83 - Last mean reward per episode: 2793.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.54e+03    |
|    ep_rew_mean          | 2766.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 144         |
|    time_elapsed         | 42003       |
|    total_timesteps      | 3538944     |
| train/                  |             |
|    approx_kl            | 0.015780615 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.941       |
|    learning_rate        | 3e-06       |
|    loss                 | 349         |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.000708    |
|    value_loss           | 766         |
-----------------------------------------
Num timesteps: 3540000
Best mean reward: 3041.83 - Last mean reward per episode: 2766.19
Num timesteps: 3552000
Best mean reward: 3041.83 - Last mean reward per episode: 2759.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.49e+03    |
|    ep_rew_mean          | 2701.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 145         |
|    time_elapsed         | 42297       |
|    total_timesteps      | 3563520     |
| train/                  |             |
|    approx_kl            | 0.013999782 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 417         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 906         |
-----------------------------------------
Num timesteps: 3564000
Best mean reward: 3041.83 - Last mean reward per episode: 2701.21
Num timesteps: 3576000
Best mean reward: 3041.83 - Last mean reward per episode: 2731.44
Num timesteps: 3588000
Best mean reward: 3041.83 - Last mean reward per episode: 2722.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.49e+03    |
|    ep_rew_mean          | 2722.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 146         |
|    time_elapsed         | 42582       |
|    total_timesteps      | 3588096     |
| train/                  |             |
|    approx_kl            | 0.014627159 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 200         |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.000812   |
|    value_loss           | 900         |
-----------------------------------------
Num timesteps: 3600000
Best mean reward: 3041.83 - Last mean reward per episode: 2748.66
Num timesteps: 3612000
Best mean reward: 3041.83 - Last mean reward per episode: 2755.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.52e+03    |
|    ep_rew_mean          | 2784.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 147         |
|    time_elapsed         | 42869       |
|    total_timesteps      | 3612672     |
| train/                  |             |
|    approx_kl            | 0.013867349 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 377         |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 815         |
-----------------------------------------
Num timesteps: 3624000
Best mean reward: 3041.83 - Last mean reward per episode: 2775.31
Num timesteps: 3636000
Best mean reward: 3041.83 - Last mean reward per episode: 2815.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.51e+03     |
|    ep_rew_mean          | 2815.73      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 148          |
|    time_elapsed         | 43162        |
|    total_timesteps      | 3637248      |
| train/                  |              |
|    approx_kl            | 0.0141060455 |
|    clip_fraction        | 0.172        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.936        |
|    learning_rate        | 3e-06        |
|    loss                 | 860          |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 882          |
------------------------------------------
Num timesteps: 3648000
Best mean reward: 3041.83 - Last mean reward per episode: 2814.87
Num timesteps: 3660000
Best mean reward: 3041.83 - Last mean reward per episode: 2853.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.53e+03     |
|    ep_rew_mean          | 2853.8       |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 149          |
|    time_elapsed         | 43450        |
|    total_timesteps      | 3661824      |
| train/                  |              |
|    approx_kl            | 0.0144949285 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.938        |
|    learning_rate        | 3e-06        |
|    loss                 | 264          |
|    n_updates            | 1480         |
|    policy_gradient_loss | 0.000682     |
|    value_loss           | 798          |
------------------------------------------
Num timesteps: 3672000
Best mean reward: 3041.83 - Last mean reward per episode: 2905.70
Num timesteps: 3684000
Best mean reward: 3041.83 - Last mean reward per episode: 2908.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.56e+03    |
|    ep_rew_mean          | 2904.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 150         |
|    time_elapsed         | 43734       |
|    total_timesteps      | 3686400     |
| train/                  |             |
|    approx_kl            | 0.016059527 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 395         |
|    n_updates            | 1490        |
|    policy_gradient_loss | 0.000598    |
|    value_loss           | 691         |
-----------------------------------------
Num timesteps: 3696000
Best mean reward: 3041.83 - Last mean reward per episode: 2964.09
Num timesteps: 3708000
Best mean reward: 3041.83 - Last mean reward per episode: 2981.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2956.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 151         |
|    time_elapsed         | 44028       |
|    total_timesteps      | 3710976     |
| train/                  |             |
|    approx_kl            | 0.016128702 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 162         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 733         |
-----------------------------------------
Num timesteps: 3720000
Best mean reward: 3041.83 - Last mean reward per episode: 2998.41
Num timesteps: 3732000
Best mean reward: 3041.83 - Last mean reward per episode: 2959.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | 2931.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 152         |
|    time_elapsed         | 44316       |
|    total_timesteps      | 3735552     |
| train/                  |             |
|    approx_kl            | 0.017992176 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 515         |
|    n_updates            | 1510        |
|    policy_gradient_loss | -6.66e-05   |
|    value_loss           | 815         |
-----------------------------------------
Num timesteps: 3744000
Best mean reward: 3041.83 - Last mean reward per episode: 2939.62
Num timesteps: 3756000
Best mean reward: 3041.83 - Last mean reward per episode: 2995.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2969.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 153         |
|    time_elapsed         | 44601       |
|    total_timesteps      | 3760128     |
| train/                  |             |
|    approx_kl            | 0.014874942 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 563         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000783   |
|    value_loss           | 836         |
-----------------------------------------
Num timesteps: 3768000
Best mean reward: 3041.83 - Last mean reward per episode: 2950.16
Num timesteps: 3780000
Best mean reward: 3041.83 - Last mean reward per episode: 2948.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2930.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 154         |
|    time_elapsed         | 44891       |
|    total_timesteps      | 3784704     |
| train/                  |             |
|    approx_kl            | 0.016286185 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 540         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 791         |
-----------------------------------------
Num timesteps: 3792000
Best mean reward: 3041.83 - Last mean reward per episode: 2931.46
Num timesteps: 3804000
Best mean reward: 3041.83 - Last mean reward per episode: 2899.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2891.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 155         |
|    time_elapsed         | 45179       |
|    total_timesteps      | 3809280     |
| train/                  |             |
|    approx_kl            | 0.016315596 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.934       |
|    learning_rate        | 3e-06       |
|    loss                 | 492         |
|    n_updates            | 1540        |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 774         |
-----------------------------------------
Num timesteps: 3816000
Best mean reward: 3041.83 - Last mean reward per episode: 2880.03
Num timesteps: 3828000
Best mean reward: 3041.83 - Last mean reward per episode: 2884.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2893.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 156         |
|    time_elapsed         | 45467       |
|    total_timesteps      | 3833856     |
| train/                  |             |
|    approx_kl            | 0.015604153 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 1550        |
|    policy_gradient_loss | 0.000296    |
|    value_loss           | 650         |
-----------------------------------------
Num timesteps: 3840000
Best mean reward: 3041.83 - Last mean reward per episode: 2908.50
Num timesteps: 3852000
Best mean reward: 3041.83 - Last mean reward per episode: 2873.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 2840.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 157         |
|    time_elapsed         | 45747       |
|    total_timesteps      | 3858432     |
| train/                  |             |
|    approx_kl            | 0.015586416 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 174         |
|    n_updates            | 1560        |
|    policy_gradient_loss | 0.000261    |
|    value_loss           | 515         |
-----------------------------------------
Num timesteps: 3864000
Best mean reward: 3041.83 - Last mean reward per episode: 2813.92
Num timesteps: 3876000
Best mean reward: 3041.83 - Last mean reward per episode: 2811.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.55e+03    |
|    ep_rew_mean          | 2796.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 158         |
|    time_elapsed         | 46031       |
|    total_timesteps      | 3883008     |
| train/                  |             |
|    approx_kl            | 0.015966116 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 425         |
|    n_updates            | 1570        |
|    policy_gradient_loss | 0.000467    |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 3888000
Best mean reward: 3041.83 - Last mean reward per episode: 2833.66
Num timesteps: 3900000
Best mean reward: 3041.83 - Last mean reward per episode: 2867.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 2861.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 159         |
|    time_elapsed         | 46314       |
|    total_timesteps      | 3907584     |
| train/                  |             |
|    approx_kl            | 0.016004337 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 154         |
|    n_updates            | 1580        |
|    policy_gradient_loss | -7.43e-05   |
|    value_loss           | 670         |
-----------------------------------------
Num timesteps: 3912000
Best mean reward: 3041.83 - Last mean reward per episode: 2865.97
Num timesteps: 3924000
Best mean reward: 3041.83 - Last mean reward per episode: 2890.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 2875.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 160         |
|    time_elapsed         | 46602       |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.016894247 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 549         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 757         |
-----------------------------------------
Num timesteps: 3936000
Best mean reward: 3041.83 - Last mean reward per episode: 2893.41
Num timesteps: 3948000
Best mean reward: 3041.83 - Last mean reward per episode: 2942.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | 2968.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 161         |
|    time_elapsed         | 46885       |
|    total_timesteps      | 3956736     |
| train/                  |             |
|    approx_kl            | 0.015770683 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 198         |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.00026     |
|    value_loss           | 758         |
-----------------------------------------
Num timesteps: 3960000
Best mean reward: 3041.83 - Last mean reward per episode: 2947.49
Num timesteps: 3972000
Best mean reward: 3041.83 - Last mean reward per episode: 3003.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.67e+03   |
|    ep_rew_mean          | 3024.45    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 162        |
|    time_elapsed         | 47175      |
|    total_timesteps      | 3981312    |
| train/                  |            |
|    approx_kl            | 0.01430741 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.13      |
|    explained_variance   | 0.949      |
|    learning_rate        | 3e-06      |
|    loss                 | 531        |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0017    |
|    value_loss           | 643        |
----------------------------------------
Num timesteps: 3984000
Best mean reward: 3041.83 - Last mean reward per episode: 3024.45
Num timesteps: 3996000
Best mean reward: 3041.83 - Last mean reward per episode: 3043.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 3021.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 163         |
|    time_elapsed         | 47467       |
|    total_timesteps      | 4005888     |
| train/                  |             |
|    approx_kl            | 0.015978927 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 241         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 692         |
-----------------------------------------
Num timesteps: 4008000
Best mean reward: 3043.59 - Last mean reward per episode: 3021.26
Num timesteps: 4020000
Best mean reward: 3043.59 - Last mean reward per episode: 3051.85
Saving new best model to tmp/best_model
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.7e+03   |
|    ep_rew_mean          | 3073.19   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 164       |
|    time_elapsed         | 47756     |
|    total_timesteps      | 4030464   |
| train/                  |           |
|    approx_kl            | 0.0144925 |
|    clip_fraction        | 0.168     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.15     |
|    explained_variance   | 0.95      |
|    learning_rate        | 3e-06     |
|    loss                 | 318       |
|    n_updates            | 1630      |
|    policy_gradient_loss | -0.000691 |
|    value_loss           | 574       |
---------------------------------------
Num timesteps: 4032000
Best mean reward: 3051.85 - Last mean reward per episode: 3093.99
Saving new best model to tmp/best_model
Num timesteps: 4044000
Best mean reward: 3093.99 - Last mean reward per episode: 3166.78
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3228.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 165         |
|    time_elapsed         | 48053       |
|    total_timesteps      | 4055040     |
| train/                  |             |
|    approx_kl            | 0.018574199 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.06       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 479         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 757         |
-----------------------------------------
Num timesteps: 4056000
Best mean reward: 3166.78 - Last mean reward per episode: 3228.73
Saving new best model to tmp/best_model
Num timesteps: 4068000
Best mean reward: 3228.73 - Last mean reward per episode: 3208.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.82e+03    |
|    ep_rew_mean          | 3264.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 166         |
|    time_elapsed         | 48350       |
|    total_timesteps      | 4079616     |
| train/                  |             |
|    approx_kl            | 0.014356631 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.12       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 436         |
|    n_updates            | 1650        |
|    policy_gradient_loss | 0.000397    |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 4080000
Best mean reward: 3228.73 - Last mean reward per episode: 3264.84
Saving new best model to tmp/best_model
Num timesteps: 4092000
Best mean reward: 3264.84 - Last mean reward per episode: 3305.36
Saving new best model to tmp/best_model
Num timesteps: 4104000
Best mean reward: 3305.36 - Last mean reward per episode: 3365.07
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3365.07     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 167         |
|    time_elapsed         | 48640       |
|    total_timesteps      | 4104192     |
| train/                  |             |
|    approx_kl            | 0.015412747 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 157         |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.000299   |
|    value_loss           | 591         |
-----------------------------------------
Num timesteps: 4116000
Best mean reward: 3365.07 - Last mean reward per episode: 3405.57
Saving new best model to tmp/best_model
Num timesteps: 4128000
Best mean reward: 3405.57 - Last mean reward per episode: 3407.46
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3420.64     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 168         |
|    time_elapsed         | 48932       |
|    total_timesteps      | 4128768     |
| train/                  |             |
|    approx_kl            | 0.016776461 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.07       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 208         |
|    n_updates            | 1670        |
|    policy_gradient_loss | 0.000857    |
|    value_loss           | 641         |
-----------------------------------------
Num timesteps: 4140000
Best mean reward: 3407.46 - Last mean reward per episode: 3385.96
Num timesteps: 4152000
Best mean reward: 3407.46 - Last mean reward per episode: 3365.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3365.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 169         |
|    time_elapsed         | 49226       |
|    total_timesteps      | 4153344     |
| train/                  |             |
|    approx_kl            | 0.016707271 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.06       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 481         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.000536   |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 4164000
Best mean reward: 3407.46 - Last mean reward per episode: 3349.80
Num timesteps: 4176000
Best mean reward: 3407.46 - Last mean reward per episode: 3412.42
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3425.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 170         |
|    time_elapsed         | 49517       |
|    total_timesteps      | 4177920     |
| train/                  |             |
|    approx_kl            | 0.018226525 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.925       |
|    learning_rate        | 3e-06       |
|    loss                 | 532         |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 867         |
-----------------------------------------
Num timesteps: 4188000
Best mean reward: 3412.42 - Last mean reward per episode: 3462.72
Saving new best model to tmp/best_model
Num timesteps: 4200000
Best mean reward: 3462.72 - Last mean reward per episode: 3382.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.89e+03    |
|    ep_rew_mean          | 3373.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 171         |
|    time_elapsed         | 49813       |
|    total_timesteps      | 4202496     |
| train/                  |             |
|    approx_kl            | 0.014467967 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.95       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 259         |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 4212000
Best mean reward: 3462.72 - Last mean reward per episode: 3370.44
Num timesteps: 4224000
Best mean reward: 3462.72 - Last mean reward per episode: 3387.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3387.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 172         |
|    time_elapsed         | 50108       |
|    total_timesteps      | 4227072     |
| train/                  |             |
|    approx_kl            | 0.015969101 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 369         |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.000841   |
|    value_loss           | 785         |
-----------------------------------------
Num timesteps: 4236000
Best mean reward: 3462.72 - Last mean reward per episode: 3392.16
Num timesteps: 4248000
Best mean reward: 3462.72 - Last mean reward per episode: 3405.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3422.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 173         |
|    time_elapsed         | 50403       |
|    total_timesteps      | 4251648     |
| train/                  |             |
|    approx_kl            | 0.016785135 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 348         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 694         |
-----------------------------------------
Num timesteps: 4260000
Best mean reward: 3462.72 - Last mean reward per episode: 3424.69
Num timesteps: 4272000
Best mean reward: 3462.72 - Last mean reward per episode: 3353.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.85e+03    |
|    ep_rew_mean          | 3328.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 174         |
|    time_elapsed         | 50691       |
|    total_timesteps      | 4276224     |
| train/                  |             |
|    approx_kl            | 0.015238852 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3          |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 583         |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.000435    |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 4284000
Best mean reward: 3462.72 - Last mean reward per episode: 3280.32
Num timesteps: 4296000
Best mean reward: 3462.72 - Last mean reward per episode: 3272.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3222.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 175         |
|    time_elapsed         | 50984       |
|    total_timesteps      | 4300800     |
| train/                  |             |
|    approx_kl            | 0.017641693 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 155         |
|    n_updates            | 1740        |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 730         |
-----------------------------------------
Num timesteps: 4308000
Best mean reward: 3462.72 - Last mean reward per episode: 3265.93
Num timesteps: 4320000
Best mean reward: 3462.72 - Last mean reward per episode: 3317.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3315.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 176         |
|    time_elapsed         | 51274       |
|    total_timesteps      | 4325376     |
| train/                  |             |
|    approx_kl            | 0.017317701 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 316         |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.000255   |
|    value_loss           | 696         |
-----------------------------------------
Num timesteps: 4332000
Best mean reward: 3462.72 - Last mean reward per episode: 3285.94
Num timesteps: 4344000
Best mean reward: 3462.72 - Last mean reward per episode: 3307.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 3283.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 177         |
|    time_elapsed         | 51559       |
|    total_timesteps      | 4349952     |
| train/                  |             |
|    approx_kl            | 0.015922556 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.92       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 152         |
|    n_updates            | 1760        |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 727         |
-----------------------------------------
Num timesteps: 4356000
Best mean reward: 3462.72 - Last mean reward per episode: 3268.98
Num timesteps: 4368000
Best mean reward: 3462.72 - Last mean reward per episode: 3254.21
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.78e+03   |
|    ep_rew_mean          | 3237.62    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 178        |
|    time_elapsed         | 51848      |
|    total_timesteps      | 4374528    |
| train/                  |            |
|    approx_kl            | 0.01696252 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.81      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 152        |
|    n_updates            | 1770       |
|    policy_gradient_loss | 0.000342   |
|    value_loss           | 733        |
----------------------------------------
Num timesteps: 4380000
Best mean reward: 3462.72 - Last mean reward per episode: 3312.13
Num timesteps: 4392000
Best mean reward: 3462.72 - Last mean reward per episode: 3296.95
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.8e+03   |
|    ep_rew_mean          | 3275.97   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 179       |
|    time_elapsed         | 52142     |
|    total_timesteps      | 4399104   |
| train/                  |           |
|    approx_kl            | 0.0162282 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.9      |
|    explained_variance   | 0.938     |
|    learning_rate        | 3e-06     |
|    loss                 | 333       |
|    n_updates            | 1780      |
|    policy_gradient_loss | 0.000193  |
|    value_loss           | 738       |
---------------------------------------
Num timesteps: 4404000
Best mean reward: 3462.72 - Last mean reward per episode: 3286.23
Num timesteps: 4416000
Best mean reward: 3462.72 - Last mean reward per episode: 3248.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3277.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 180         |
|    time_elapsed         | 52431       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.017353082 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 248         |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 783         |
-----------------------------------------
Num timesteps: 4428000
Best mean reward: 3462.72 - Last mean reward per episode: 3252.52
Num timesteps: 4440000
Best mean reward: 3462.72 - Last mean reward per episode: 3290.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79e+03    |
|    ep_rew_mean          | 3294.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 181         |
|    time_elapsed         | 52715       |
|    total_timesteps      | 4448256     |
| train/                  |             |
|    approx_kl            | 0.015882721 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 229         |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.000659   |
|    value_loss           | 774         |
-----------------------------------------
Num timesteps: 4452000
Best mean reward: 3462.72 - Last mean reward per episode: 3303.42
Num timesteps: 4464000
Best mean reward: 3462.72 - Last mean reward per episode: 3274.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79e+03    |
|    ep_rew_mean          | 3321.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 182         |
|    time_elapsed         | 53007       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.016686583 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.73       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 383         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.000185   |
|    value_loss           | 750         |
-----------------------------------------
Num timesteps: 4476000
Best mean reward: 3462.72 - Last mean reward per episode: 3321.33
Num timesteps: 4488000
Best mean reward: 3462.72 - Last mean reward per episode: 3331.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.78e+03   |
|    ep_rew_mean          | 3325.39    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 183        |
|    time_elapsed         | 53296      |
|    total_timesteps      | 4497408    |
| train/                  |            |
|    approx_kl            | 0.01351476 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.82      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-06      |
|    loss                 | 614        |
|    n_updates            | 1820       |
|    policy_gradient_loss | 0.000365   |
|    value_loss           | 671        |
----------------------------------------
Num timesteps: 4500000
Best mean reward: 3462.72 - Last mean reward per episode: 3341.60
Num timesteps: 4512000
Best mean reward: 3462.72 - Last mean reward per episode: 3323.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.78e+03    |
|    ep_rew_mean          | 3305.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 184         |
|    time_elapsed         | 53581       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.015340146 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.000432    |
|    value_loss           | 660         |
-----------------------------------------
Num timesteps: 4524000
Best mean reward: 3462.72 - Last mean reward per episode: 3318.49
Num timesteps: 4536000
Best mean reward: 3462.72 - Last mean reward per episode: 3330.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.81e+03    |
|    ep_rew_mean          | 3330.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 185         |
|    time_elapsed         | 53869       |
|    total_timesteps      | 4546560     |
| train/                  |             |
|    approx_kl            | 0.017965136 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 489         |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.000822    |
|    value_loss           | 775         |
-----------------------------------------
Num timesteps: 4548000
Best mean reward: 3462.72 - Last mean reward per episode: 3328.63
Num timesteps: 4560000
Best mean reward: 3462.72 - Last mean reward per episode: 3293.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79e+03    |
|    ep_rew_mean          | 3311.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 186         |
|    time_elapsed         | 54153       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.017134087 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 1850        |
|    policy_gradient_loss | -5.82e-05   |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 4572000
Best mean reward: 3462.72 - Last mean reward per episode: 3311.44
Num timesteps: 4584000
Best mean reward: 3462.72 - Last mean reward per episode: 3362.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3417.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 187         |
|    time_elapsed         | 54444       |
|    total_timesteps      | 4595712     |
| train/                  |             |
|    approx_kl            | 0.016535142 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 107         |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.000872    |
|    value_loss           | 663         |
-----------------------------------------
Num timesteps: 4596000
Best mean reward: 3462.72 - Last mean reward per episode: 3417.14
Num timesteps: 4608000
Best mean reward: 3462.72 - Last mean reward per episode: 3390.50
Num timesteps: 4620000
Best mean reward: 3462.72 - Last mean reward per episode: 3403.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.86e+03   |
|    ep_rew_mean          | 3403.89    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 188        |
|    time_elapsed         | 54736      |
|    total_timesteps      | 4620288    |
| train/                  |            |
|    approx_kl            | 0.01622552 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.79      |
|    explained_variance   | 0.946      |
|    learning_rate        | 3e-06      |
|    loss                 | 209        |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.000251  |
|    value_loss           | 706        |
----------------------------------------
Num timesteps: 4632000
Best mean reward: 3462.72 - Last mean reward per episode: 3451.42
Num timesteps: 4644000
Best mean reward: 3462.72 - Last mean reward per episode: 3453.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3453.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 189         |
|    time_elapsed         | 55023       |
|    total_timesteps      | 4644864     |
| train/                  |             |
|    approx_kl            | 0.016912669 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00021    |
|    value_loss           | 676         |
-----------------------------------------
Num timesteps: 4656000
Best mean reward: 3462.72 - Last mean reward per episode: 3427.93
Num timesteps: 4668000
Best mean reward: 3462.72 - Last mean reward per episode: 3381.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3381.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 190         |
|    time_elapsed         | 55317       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.014539708 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 616         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.000373   |
|    value_loss           | 761         |
-----------------------------------------
Num timesteps: 4680000
Best mean reward: 3462.72 - Last mean reward per episode: 3433.41
Num timesteps: 4692000
Best mean reward: 3462.72 - Last mean reward per episode: 3484.56
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3484.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 191         |
|    time_elapsed         | 55605       |
|    total_timesteps      | 4694016     |
| train/                  |             |
|    approx_kl            | 0.014710087 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 224         |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.000622    |
|    value_loss           | 735         |
-----------------------------------------
Num timesteps: 4704000
Best mean reward: 3484.56 - Last mean reward per episode: 3519.98
Saving new best model to tmp/best_model
Num timesteps: 4716000
Best mean reward: 3519.98 - Last mean reward per episode: 3497.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3511.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 192         |
|    time_elapsed         | 55899       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.018377298 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 508         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.000702   |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 4728000
Best mean reward: 3519.98 - Last mean reward per episode: 3539.20
Saving new best model to tmp/best_model
Num timesteps: 4740000
Best mean reward: 3539.20 - Last mean reward per episode: 3574.12
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.9e+03     |
|    ep_rew_mean          | 3604.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 193         |
|    time_elapsed         | 56190       |
|    total_timesteps      | 4743168     |
| train/                  |             |
|    approx_kl            | 0.015418839 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 496         |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 708         |
-----------------------------------------
Num timesteps: 4752000
Best mean reward: 3574.12 - Last mean reward per episode: 3606.54
Saving new best model to tmp/best_model
Num timesteps: 4764000
Best mean reward: 3606.54 - Last mean reward per episode: 3654.38
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3564.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 194         |
|    time_elapsed         | 56481       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.016191138 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 410         |
|    n_updates            | 1930        |
|    policy_gradient_loss | 0.00162     |
|    value_loss           | 566         |
-----------------------------------------
Num timesteps: 4776000
Best mean reward: 3654.38 - Last mean reward per episode: 3559.98
Num timesteps: 4788000
Best mean reward: 3654.38 - Last mean reward per episode: 3543.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3559.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 195         |
|    time_elapsed         | 56774       |
|    total_timesteps      | 4792320     |
| train/                  |             |
|    approx_kl            | 0.017339587 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 253         |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 680         |
-----------------------------------------
Num timesteps: 4800000
Best mean reward: 3654.38 - Last mean reward per episode: 3638.69
Num timesteps: 4812000
Best mean reward: 3654.38 - Last mean reward per episode: 3647.18
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3598.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 196         |
|    time_elapsed         | 57063       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.016968014 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 364         |
|    n_updates            | 1950        |
|    policy_gradient_loss | 0.000774    |
|    value_loss           | 656         |
-----------------------------------------
Num timesteps: 4824000
Best mean reward: 3654.38 - Last mean reward per episode: 3634.16
Num timesteps: 4836000
Best mean reward: 3654.38 - Last mean reward per episode: 3618.71
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 3639.14    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 197        |
|    time_elapsed         | 57353      |
|    total_timesteps      | 4841472    |
| train/                  |            |
|    approx_kl            | 0.01741175 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.55      |
|    explained_variance   | 0.945      |
|    learning_rate        | 3e-06      |
|    loss                 | 271        |
|    n_updates            | 1960       |
|    policy_gradient_loss | 0.000992   |
|    value_loss           | 705        |
----------------------------------------
Num timesteps: 4848000
Best mean reward: 3654.38 - Last mean reward per episode: 3679.15
Saving new best model to tmp/best_model
Num timesteps: 4860000
Best mean reward: 3679.15 - Last mean reward per episode: 3731.73
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.03e+03   |
|    ep_rew_mean          | 3739.84    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 198        |
|    time_elapsed         | 57645      |
|    total_timesteps      | 4866048    |
| train/                  |            |
|    approx_kl            | 0.01564765 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.47      |
|    explained_variance   | 0.953      |
|    learning_rate        | 3e-06      |
|    loss                 | 96.2       |
|    n_updates            | 1970       |
|    policy_gradient_loss | -2.59e-05  |
|    value_loss           | 563        |
----------------------------------------
Num timesteps: 4872000
Best mean reward: 3731.73 - Last mean reward per episode: 3779.27
Saving new best model to tmp/best_model
Num timesteps: 4884000
Best mean reward: 3779.27 - Last mean reward per episode: 3697.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.04e+03    |
|    ep_rew_mean          | 3749.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 199         |
|    time_elapsed         | 57937       |
|    total_timesteps      | 4890624     |
| train/                  |             |
|    approx_kl            | 0.016198417 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 487         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 572         |
-----------------------------------------
Num timesteps: 4896000
Best mean reward: 3779.27 - Last mean reward per episode: 3738.88
Num timesteps: 4908000
Best mean reward: 3779.27 - Last mean reward per episode: 3700.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.03e+03    |
|    ep_rew_mean          | 3702.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 200         |
|    time_elapsed         | 58228       |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.015727252 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 615         |
-----------------------------------------
Num timesteps: 4920000
Best mean reward: 3779.27 - Last mean reward per episode: 3680.93
Num timesteps: 4932000
Best mean reward: 3779.27 - Last mean reward per episode: 3631.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.98e+03    |
|    ep_rew_mean          | 3625.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 201         |
|    time_elapsed         | 58517       |
|    total_timesteps      | 4939776     |
| train/                  |             |
|    approx_kl            | 0.016767478 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 189         |
|    n_updates            | 2000        |
|    policy_gradient_loss | 0.00127     |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 4944000
Best mean reward: 3779.27 - Last mean reward per episode: 3649.64
Num timesteps: 4956000
Best mean reward: 3779.27 - Last mean reward per episode: 3625.80
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3607.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 202         |
|    time_elapsed         | 58812       |
|    total_timesteps      | 4964352     |
| train/                  |             |
|    approx_kl            | 0.015426074 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.000123   |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 4968000
Best mean reward: 3779.27 - Last mean reward per episode: 3673.64
Num timesteps: 4980000
Best mean reward: 3779.27 - Last mean reward per episode: 3666.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 3711.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 203         |
|    time_elapsed         | 59102       |
|    total_timesteps      | 4988928     |
| train/                  |             |
|    approx_kl            | 0.014566359 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 354         |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.000896   |
|    value_loss           | 571         |
-----------------------------------------
Num timesteps: 4992000
Best mean reward: 3779.27 - Last mean reward per episode: 3711.01
Num timesteps: 5004000
Best mean reward: 3779.27 - Last mean reward per episode: 3648.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.96e+03   |
|    ep_rew_mean          | 3632.66    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 204        |
|    time_elapsed         | 59391      |
|    total_timesteps      | 5013504    |
| train/                  |            |
|    approx_kl            | 0.01694342 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.5       |
|    explained_variance   | 0.949      |
|    learning_rate        | 3e-06      |
|    loss                 | 482        |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.00074   |
|    value_loss           | 648        |
----------------------------------------
Num timesteps: 5016000
Best mean reward: 3779.27 - Last mean reward per episode: 3659.68
Num timesteps: 5028000
Best mean reward: 3779.27 - Last mean reward per episode: 3635.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.96e+03    |
|    ep_rew_mean          | 3644.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 205         |
|    time_elapsed         | 59684       |
|    total_timesteps      | 5038080     |
| train/                  |             |
|    approx_kl            | 0.017274795 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 2040        |
|    policy_gradient_loss | 0.00039     |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 5040000
Best mean reward: 3779.27 - Last mean reward per episode: 3601.72
Num timesteps: 5052000
Best mean reward: 3779.27 - Last mean reward per episode: 3524.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3530.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 206         |
|    time_elapsed         | 59976       |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.017165827 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 356         |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.000199   |
|    value_loss           | 640         |
-----------------------------------------
Num timesteps: 5064000
Best mean reward: 3779.27 - Last mean reward per episode: 3506.89
Num timesteps: 5076000
Best mean reward: 3779.27 - Last mean reward per episode: 3576.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3543.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 207         |
|    time_elapsed         | 60268       |
|    total_timesteps      | 5087232     |
| train/                  |             |
|    approx_kl            | 0.015144569 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 259         |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.000536   |
|    value_loss           | 600         |
-----------------------------------------
Num timesteps: 5088000
Best mean reward: 3779.27 - Last mean reward per episode: 3543.08
Num timesteps: 5100000
Best mean reward: 3779.27 - Last mean reward per episode: 3518.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.98e+03    |
|    ep_rew_mean          | 3624.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 208         |
|    time_elapsed         | 60562       |
|    total_timesteps      | 5111808     |
| train/                  |             |
|    approx_kl            | 0.017252347 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 575         |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.00223     |
|    value_loss           | 662         |
-----------------------------------------
Num timesteps: 5112000
Best mean reward: 3779.27 - Last mean reward per episode: 3624.32
Num timesteps: 5124000
Best mean reward: 3779.27 - Last mean reward per episode: 3593.06
Num timesteps: 5136000
Best mean reward: 3779.27 - Last mean reward per episode: 3539.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3519.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 209         |
|    time_elapsed         | 60849       |
|    total_timesteps      | 5136384     |
| train/                  |             |
|    approx_kl            | 0.014714618 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.000753   |
|    value_loss           | 606         |
-----------------------------------------
Num timesteps: 5148000
Best mean reward: 3779.27 - Last mean reward per episode: 3542.59
Num timesteps: 5160000
Best mean reward: 3779.27 - Last mean reward per episode: 3545.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3532.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 210         |
|    time_elapsed         | 61140       |
|    total_timesteps      | 5160960     |
| train/                  |             |
|    approx_kl            | 0.016082378 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 546         |
|    n_updates            | 2090        |
|    policy_gradient_loss | 0.000773    |
|    value_loss           | 818         |
-----------------------------------------
Num timesteps: 5172000
Best mean reward: 3779.27 - Last mean reward per episode: 3502.53
Num timesteps: 5184000
Best mean reward: 3779.27 - Last mean reward per episode: 3535.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3478.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 211         |
|    time_elapsed         | 61429       |
|    total_timesteps      | 5185536     |
| train/                  |             |
|    approx_kl            | 0.014460501 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 179         |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 542         |
-----------------------------------------
Num timesteps: 5196000
Best mean reward: 3779.27 - Last mean reward per episode: 3523.80
Num timesteps: 5208000
Best mean reward: 3779.27 - Last mean reward per episode: 3496.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3496.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 212         |
|    time_elapsed         | 61717       |
|    total_timesteps      | 5210112     |
| train/                  |             |
|    approx_kl            | 0.018668545 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 454         |
|    n_updates            | 2110        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 782         |
-----------------------------------------
Num timesteps: 5220000
Best mean reward: 3779.27 - Last mean reward per episode: 3503.79
Num timesteps: 5232000
Best mean reward: 3779.27 - Last mean reward per episode: 3503.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 3505.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 213         |
|    time_elapsed         | 62006       |
|    total_timesteps      | 5234688     |
| train/                  |             |
|    approx_kl            | 0.017423699 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 302         |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 777         |
-----------------------------------------
Num timesteps: 5244000
Best mean reward: 3779.27 - Last mean reward per episode: 3541.43
Num timesteps: 5256000
Best mean reward: 3779.27 - Last mean reward per episode: 3582.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3543.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 214         |
|    time_elapsed         | 62289       |
|    total_timesteps      | 5259264     |
| train/                  |             |
|    approx_kl            | 0.016555464 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 440         |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.000874   |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 5268000
Best mean reward: 3779.27 - Last mean reward per episode: 3484.85
Num timesteps: 5280000
Best mean reward: 3779.27 - Last mean reward per episode: 3566.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3578.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 215         |
|    time_elapsed         | 62576       |
|    total_timesteps      | 5283840     |
| train/                  |             |
|    approx_kl            | 0.017517606 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 217         |
|    n_updates            | 2140        |
|    policy_gradient_loss | -5.59e-05   |
|    value_loss           | 615         |
-----------------------------------------
Num timesteps: 5292000
Best mean reward: 3779.27 - Last mean reward per episode: 3569.94
Num timesteps: 5304000
Best mean reward: 3779.27 - Last mean reward per episode: 3502.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3476.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 216         |
|    time_elapsed         | 62864       |
|    total_timesteps      | 5308416     |
| train/                  |             |
|    approx_kl            | 0.018087786 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.000754   |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 5316000
Best mean reward: 3779.27 - Last mean reward per episode: 3551.09
Num timesteps: 5328000
Best mean reward: 3779.27 - Last mean reward per episode: 3586.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 3585.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 217         |
|    time_elapsed         | 63153       |
|    total_timesteps      | 5332992     |
| train/                  |             |
|    approx_kl            | 0.019146912 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 437         |
|    n_updates            | 2160        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 716         |
-----------------------------------------
Num timesteps: 5340000
Best mean reward: 3779.27 - Last mean reward per episode: 3619.67
Num timesteps: 5352000
Best mean reward: 3779.27 - Last mean reward per episode: 3600.85
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 3600.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 218        |
|    time_elapsed         | 63438      |
|    total_timesteps      | 5357568    |
| train/                  |            |
|    approx_kl            | 0.01950214 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.2       |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-06      |
|    loss                 | 280        |
|    n_updates            | 2170       |
|    policy_gradient_loss | 0.000529   |
|    value_loss           | 605        |
----------------------------------------
Num timesteps: 5364000
Best mean reward: 3779.27 - Last mean reward per episode: 3611.74
Num timesteps: 5376000
Best mean reward: 3779.27 - Last mean reward per episode: 3601.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3619.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 219         |
|    time_elapsed         | 63725       |
|    total_timesteps      | 5382144     |
| train/                  |             |
|    approx_kl            | 0.020262333 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 292         |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.000318    |
|    value_loss           | 742         |
-----------------------------------------
Num timesteps: 5388000
Best mean reward: 3779.27 - Last mean reward per episode: 3586.85
Num timesteps: 5400000
Best mean reward: 3779.27 - Last mean reward per episode: 3610.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3601.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 220         |
|    time_elapsed         | 64014       |
|    total_timesteps      | 5406720     |
| train/                  |             |
|    approx_kl            | 0.017967856 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 254         |
|    n_updates            | 2190        |
|    policy_gradient_loss | 0.000872    |
|    value_loss           | 629         |
-----------------------------------------
Num timesteps: 5412000
Best mean reward: 3779.27 - Last mean reward per episode: 3592.21
Num timesteps: 5424000
Best mean reward: 3779.27 - Last mean reward per episode: 3590.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3577.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 221         |
|    time_elapsed         | 64303       |
|    total_timesteps      | 5431296     |
| train/                  |             |
|    approx_kl            | 0.019447403 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 275         |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.00107     |
|    value_loss           | 666         |
-----------------------------------------
Num timesteps: 5436000
Best mean reward: 3779.27 - Last mean reward per episode: 3581.53
Num timesteps: 5448000
Best mean reward: 3779.27 - Last mean reward per episode: 3573.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3600.1      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 222         |
|    time_elapsed         | 64592       |
|    total_timesteps      | 5455872     |
| train/                  |             |
|    approx_kl            | 0.018407077 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 302         |
|    n_updates            | 2210        |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 5460000
Best mean reward: 3779.27 - Last mean reward per episode: 3625.36
Num timesteps: 5472000
Best mean reward: 3779.27 - Last mean reward per episode: 3567.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3566.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 223         |
|    time_elapsed         | 64878       |
|    total_timesteps      | 5480448     |
| train/                  |             |
|    approx_kl            | 0.015997674 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 210         |
|    n_updates            | 2220        |
|    policy_gradient_loss | -6.93e-05   |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 5484000
Best mean reward: 3779.27 - Last mean reward per episode: 3581.50
Num timesteps: 5496000
Best mean reward: 3779.27 - Last mean reward per episode: 3554.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3591.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 224         |
|    time_elapsed         | 65164       |
|    total_timesteps      | 5505024     |
| train/                  |             |
|    approx_kl            | 0.016519511 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 343         |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 646         |
-----------------------------------------
Num timesteps: 5508000
Best mean reward: 3779.27 - Last mean reward per episode: 3553.30
Num timesteps: 5520000
Best mean reward: 3779.27 - Last mean reward per episode: 3570.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3511.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 225         |
|    time_elapsed         | 65452       |
|    total_timesteps      | 5529600     |
| train/                  |             |
|    approx_kl            | 0.016194478 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 378         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.000398   |
|    value_loss           | 685         |
-----------------------------------------
Num timesteps: 5532000
Best mean reward: 3779.27 - Last mean reward per episode: 3492.69
Num timesteps: 5544000
Best mean reward: 3779.27 - Last mean reward per episode: 3517.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3480.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 226         |
|    time_elapsed         | 65738       |
|    total_timesteps      | 5554176     |
| train/                  |             |
|    approx_kl            | 0.016967686 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 278         |
|    n_updates            | 2250        |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 731         |
-----------------------------------------
Num timesteps: 5556000
Best mean reward: 3779.27 - Last mean reward per episode: 3504.05
Num timesteps: 5568000
Best mean reward: 3779.27 - Last mean reward per episode: 3490.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3515.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 227         |
|    time_elapsed         | 66024       |
|    total_timesteps      | 5578752     |
| train/                  |             |
|    approx_kl            | 0.019021597 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 423         |
|    n_updates            | 2260        |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 727         |
-----------------------------------------
Num timesteps: 5580000
Best mean reward: 3779.27 - Last mean reward per episode: 3515.25
Num timesteps: 5592000
Best mean reward: 3779.27 - Last mean reward per episode: 3470.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 3431.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 228         |
|    time_elapsed         | 66315       |
|    total_timesteps      | 5603328     |
| train/                  |             |
|    approx_kl            | 0.019252945 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 613         |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 5604000
Best mean reward: 3779.27 - Last mean reward per episode: 3431.61
Num timesteps: 5616000
Best mean reward: 3779.27 - Last mean reward per episode: 3443.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3475.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 229         |
|    time_elapsed         | 66600       |
|    total_timesteps      | 5627904     |
| train/                  |             |
|    approx_kl            | 0.017678538 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 2280        |
|    policy_gradient_loss | 0.0006      |
|    value_loss           | 799         |
-----------------------------------------
Num timesteps: 5628000
Best mean reward: 3779.27 - Last mean reward per episode: 3475.06
Num timesteps: 5640000
Best mean reward: 3779.27 - Last mean reward per episode: 3540.91
Num timesteps: 5652000
Best mean reward: 3779.27 - Last mean reward per episode: 3525.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.89e+03    |
|    ep_rew_mean          | 3525.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 230         |
|    time_elapsed         | 66884       |
|    total_timesteps      | 5652480     |
| train/                  |             |
|    approx_kl            | 0.016453987 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 285         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.000111   |
|    value_loss           | 571         |
-----------------------------------------
Num timesteps: 5664000
Best mean reward: 3779.27 - Last mean reward per episode: 3595.04
Num timesteps: 5676000
Best mean reward: 3779.27 - Last mean reward per episode: 3605.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 3605.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 231         |
|    time_elapsed         | 67171       |
|    total_timesteps      | 5677056     |
| train/                  |             |
|    approx_kl            | 0.017445462 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 344         |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.000996    |
|    value_loss           | 465         |
-----------------------------------------
Num timesteps: 5688000
Best mean reward: 3779.27 - Last mean reward per episode: 3634.70
Num timesteps: 5700000
Best mean reward: 3779.27 - Last mean reward per episode: 3622.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3622.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 232         |
|    time_elapsed         | 67456       |
|    total_timesteps      | 5701632     |
| train/                  |             |
|    approx_kl            | 0.018791676 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 2310        |
|    policy_gradient_loss | -5.89e-05   |
|    value_loss           | 609         |
-----------------------------------------
Num timesteps: 5712000
Best mean reward: 3779.27 - Last mean reward per episode: 3670.51
Num timesteps: 5724000
Best mean reward: 3779.27 - Last mean reward per episode: 3679.47
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 3719.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 233         |
|    time_elapsed         | 67740       |
|    total_timesteps      | 5726208     |
| train/                  |             |
|    approx_kl            | 0.016734779 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 602         |
|    n_updates            | 2320        |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 659         |
-----------------------------------------
Num timesteps: 5736000
Best mean reward: 3779.27 - Last mean reward per episode: 3754.51
Num timesteps: 5748000
Best mean reward: 3779.27 - Last mean reward per episode: 3741.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 3741.68    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 234        |
|    time_elapsed         | 68028      |
|    total_timesteps      | 5750784    |
| train/                  |            |
|    approx_kl            | 0.01842521 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.13      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-06      |
|    loss                 | 301        |
|    n_updates            | 2330       |
|    policy_gradient_loss | 0.00226    |
|    value_loss           | 726        |
----------------------------------------
Num timesteps: 5760000
Best mean reward: 3779.27 - Last mean reward per episode: 3778.87
Num timesteps: 5772000
Best mean reward: 3779.27 - Last mean reward per episode: 3753.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02e+03    |
|    ep_rew_mean          | 3739.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 235         |
|    time_elapsed         | 68311       |
|    total_timesteps      | 5775360     |
| train/                  |             |
|    approx_kl            | 0.017576674 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 563         |
-----------------------------------------
Num timesteps: 5784000
Best mean reward: 3779.27 - Last mean reward per episode: 3767.12
Num timesteps: 5796000
Best mean reward: 3779.27 - Last mean reward per episode: 3801.85
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 3808.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 236         |
|    time_elapsed         | 68606       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.016971024 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 330         |
|    n_updates            | 2350        |
|    policy_gradient_loss | 0.000806    |
|    value_loss           | 607         |
-----------------------------------------
Num timesteps: 5808000
Best mean reward: 3801.85 - Last mean reward per episode: 3881.02
Saving new best model to tmp/best_model
Num timesteps: 5820000
Best mean reward: 3881.02 - Last mean reward per episode: 3919.15
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3901.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 237         |
|    time_elapsed         | 68892       |
|    total_timesteps      | 5824512     |
| train/                  |             |
|    approx_kl            | 0.017119043 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 219         |
|    n_updates            | 2360        |
|    policy_gradient_loss | 0.000595    |
|    value_loss           | 592         |
-----------------------------------------
Num timesteps: 5832000
Best mean reward: 3919.15 - Last mean reward per episode: 3915.22
Num timesteps: 5844000
Best mean reward: 3919.15 - Last mean reward per episode: 3815.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 3815.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 238         |
|    time_elapsed         | 69182       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.018147739 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 260         |
|    n_updates            | 2370        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 681         |
-----------------------------------------
Num timesteps: 5856000
Best mean reward: 3919.15 - Last mean reward per episode: 3767.29
Num timesteps: 5868000
Best mean reward: 3919.15 - Last mean reward per episode: 3724.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.99e+03    |
|    ep_rew_mean          | 3678.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 239         |
|    time_elapsed         | 69468       |
|    total_timesteps      | 5873664     |
| train/                  |             |
|    approx_kl            | 0.018999299 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 398         |
|    n_updates            | 2380        |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 662         |
-----------------------------------------
Num timesteps: 5880000
Best mean reward: 3919.15 - Last mean reward per episode: 3611.11
Num timesteps: 5892000
Best mean reward: 3919.15 - Last mean reward per episode: 3591.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3648.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 240         |
|    time_elapsed         | 69755       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.018024618 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 611         |
|    n_updates            | 2390        |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 721         |
-----------------------------------------
Num timesteps: 5904000
Best mean reward: 3919.15 - Last mean reward per episode: 3603.66
Num timesteps: 5916000
Best mean reward: 3919.15 - Last mean reward per episode: 3656.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.96e+03    |
|    ep_rew_mean          | 3639.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 241         |
|    time_elapsed         | 70044       |
|    total_timesteps      | 5922816     |
| train/                  |             |
|    approx_kl            | 0.019709446 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 342         |
|    n_updates            | 2400        |
|    policy_gradient_loss | 0.00146     |
|    value_loss           | 632         |
-----------------------------------------
Num timesteps: 5928000
Best mean reward: 3919.15 - Last mean reward per episode: 3606.05
Num timesteps: 5940000
Best mean reward: 3919.15 - Last mean reward per episode: 3567.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3515.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 242         |
|    time_elapsed         | 70332       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.019253766 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 360         |
|    n_updates            | 2410        |
|    policy_gradient_loss | 0.000469    |
|    value_loss           | 703         |
-----------------------------------------
Num timesteps: 5952000
Best mean reward: 3919.15 - Last mean reward per episode: 3568.61
Num timesteps: 5964000
Best mean reward: 3919.15 - Last mean reward per episode: 3588.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.91e+03   |
|    ep_rew_mean          | 3572.14    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 243        |
|    time_elapsed         | 70622      |
|    total_timesteps      | 5971968    |
| train/                  |            |
|    approx_kl            | 0.01803161 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.21      |
|    explained_variance   | 0.94       |
|    learning_rate        | 3e-06      |
|    loss                 | 141        |
|    n_updates            | 2420       |
|    policy_gradient_loss | 0.00163    |
|    value_loss           | 735        |
----------------------------------------
Num timesteps: 5976000
Best mean reward: 3919.15 - Last mean reward per episode: 3537.44
Num timesteps: 5988000
Best mean reward: 3919.15 - Last mean reward per episode: 3524.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3522.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 244         |
|    time_elapsed         | 70914       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.017028103 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 457         |
|    n_updates            | 2430        |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 618         |
-----------------------------------------
Num timesteps: 6000000
Best mean reward: 3919.15 - Last mean reward per episode: 3496.32
Num timesteps: 6012000
Best mean reward: 3919.15 - Last mean reward per episode: 3444.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.85e+03   |
|    ep_rew_mean          | 3455.63    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 245        |
|    time_elapsed         | 71207      |
|    total_timesteps      | 6021120    |
| train/                  |            |
|    approx_kl            | 0.01685966 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.25      |
|    explained_variance   | 0.947      |
|    learning_rate        | 3e-06      |
|    loss                 | 307        |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.000215  |
|    value_loss           | 762        |
----------------------------------------
Num timesteps: 6024000
Best mean reward: 3919.15 - Last mean reward per episode: 3447.75
Num timesteps: 6036000
Best mean reward: 3919.15 - Last mean reward per episode: 3454.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.82e+03    |
|    ep_rew_mean          | 3413.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 246         |
|    time_elapsed         | 71496       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.017630203 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 158         |
|    n_updates            | 2450        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 580         |
-----------------------------------------
Num timesteps: 6048000
Best mean reward: 3919.15 - Last mean reward per episode: 3417.85
Num timesteps: 6060000
Best mean reward: 3919.15 - Last mean reward per episode: 3406.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3529.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 247         |
|    time_elapsed         | 71787       |
|    total_timesteps      | 6070272     |
| train/                  |             |
|    approx_kl            | 0.017468745 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 447         |
|    n_updates            | 2460        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 736         |
-----------------------------------------
Num timesteps: 6072000
Best mean reward: 3919.15 - Last mean reward per episode: 3529.99
Num timesteps: 6084000
Best mean reward: 3919.15 - Last mean reward per episode: 3511.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3445.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 248         |
|    time_elapsed         | 72076       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.014237042 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 419         |
|    n_updates            | 2470        |
|    policy_gradient_loss | 0.000188    |
|    value_loss           | 629         |
-----------------------------------------
Num timesteps: 6096000
Best mean reward: 3919.15 - Last mean reward per episode: 3451.86
Num timesteps: 6108000
Best mean reward: 3919.15 - Last mean reward per episode: 3434.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 3416.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 249         |
|    time_elapsed         | 72368       |
|    total_timesteps      | 6119424     |
| train/                  |             |
|    approx_kl            | 0.017243816 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.941       |
|    learning_rate        | 3e-06       |
|    loss                 | 118         |
|    n_updates            | 2480        |
|    policy_gradient_loss | 0.000359    |
|    value_loss           | 756         |
-----------------------------------------
Num timesteps: 6120000
Best mean reward: 3919.15 - Last mean reward per episode: 3416.62
Num timesteps: 6132000
Best mean reward: 3919.15 - Last mean reward per episode: 3456.66
Num timesteps: 6144000
Best mean reward: 3919.15 - Last mean reward per episode: 3348.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.81e+03    |
|    ep_rew_mean          | 3348.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 250         |
|    time_elapsed         | 72659       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.017931895 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 160         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.000253   |
|    value_loss           | 730         |
-----------------------------------------
Num timesteps: 6156000
Best mean reward: 3919.15 - Last mean reward per episode: 3345.51
Num timesteps: 6168000
Best mean reward: 3919.15 - Last mean reward per episode: 3323.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3323.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 251         |
|    time_elapsed         | 72947       |
|    total_timesteps      | 6168576     |
| train/                  |             |
|    approx_kl            | 0.014457566 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 199         |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00015    |
|    value_loss           | 620         |
-----------------------------------------
Num timesteps: 6180000
Best mean reward: 3919.15 - Last mean reward per episode: 3295.09
Num timesteps: 6192000
Best mean reward: 3919.15 - Last mean reward per episode: 3304.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3304.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 252         |
|    time_elapsed         | 73233       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.015899532 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 253         |
|    n_updates            | 2510        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 648         |
-----------------------------------------
Num timesteps: 6204000
Best mean reward: 3919.15 - Last mean reward per episode: 3360.36
Num timesteps: 6216000
Best mean reward: 3919.15 - Last mean reward per episode: 3373.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3375.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 253         |
|    time_elapsed         | 73524       |
|    total_timesteps      | 6217728     |
| train/                  |             |
|    approx_kl            | 0.018288903 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 506         |
|    n_updates            | 2520        |
|    policy_gradient_loss | 0.00358     |
|    value_loss           | 675         |
-----------------------------------------
Num timesteps: 6228000
Best mean reward: 3919.15 - Last mean reward per episode: 3431.76
Num timesteps: 6240000
Best mean reward: 3919.15 - Last mean reward per episode: 3538.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3538.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 254         |
|    time_elapsed         | 73814       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.017133737 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 2530        |
|    policy_gradient_loss | 0.00142     |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 6252000
Best mean reward: 3919.15 - Last mean reward per episode: 3607.96
Num timesteps: 6264000
Best mean reward: 3919.15 - Last mean reward per episode: 3593.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.01e+03   |
|    ep_rew_mean          | 3593.14    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 255        |
|    time_elapsed         | 74098      |
|    total_timesteps      | 6266880    |
| train/                  |            |
|    approx_kl            | 0.01476219 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.08      |
|    explained_variance   | 0.957      |
|    learning_rate        | 3e-06      |
|    loss                 | 238        |
|    n_updates            | 2540       |
|    policy_gradient_loss | 0.00235    |
|    value_loss           | 492        |
----------------------------------------
Num timesteps: 6276000
Best mean reward: 3919.15 - Last mean reward per episode: 3611.81
Num timesteps: 6288000
Best mean reward: 3919.15 - Last mean reward per episode: 3662.35
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 3658.87    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 256        |
|    time_elapsed         | 74389      |
|    total_timesteps      | 6291456    |
| train/                  |            |
|    approx_kl            | 0.01786853 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.16      |
|    explained_variance   | 0.945      |
|    learning_rate        | 3e-06      |
|    loss                 | 344        |
|    n_updates            | 2550       |
|    policy_gradient_loss | 0.000317   |
|    value_loss           | 597        |
----------------------------------------
Num timesteps: 6300000
Best mean reward: 3919.15 - Last mean reward per episode: 3726.47
Num timesteps: 6312000
Best mean reward: 3919.15 - Last mean reward per episode: 3748.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.1e+03     |
|    ep_rew_mean          | 3748.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 257         |
|    time_elapsed         | 74675       |
|    total_timesteps      | 6316032     |
| train/                  |             |
|    approx_kl            | 0.016294615 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 227         |
|    n_updates            | 2560        |
|    policy_gradient_loss | 0.000429    |
|    value_loss           | 508         |
-----------------------------------------
Num timesteps: 6324000
Best mean reward: 3919.15 - Last mean reward per episode: 3762.82
Num timesteps: 6336000
Best mean reward: 3919.15 - Last mean reward per episode: 3816.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 3905.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 258         |
|    time_elapsed         | 74966       |
|    total_timesteps      | 6340608     |
| train/                  |             |
|    approx_kl            | 0.016512819 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 215         |
|    n_updates            | 2570        |
|    policy_gradient_loss | 0.00031     |
|    value_loss           | 585         |
-----------------------------------------
Num timesteps: 6348000
Best mean reward: 3919.15 - Last mean reward per episode: 3942.35
Saving new best model to tmp/best_model
Num timesteps: 6360000
Best mean reward: 3942.35 - Last mean reward per episode: 3956.27
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 4010.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 259         |
|    time_elapsed         | 75257       |
|    total_timesteps      | 6365184     |
| train/                  |             |
|    approx_kl            | 0.013151406 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 6372000
Best mean reward: 3956.27 - Last mean reward per episode: 4027.52
Saving new best model to tmp/best_model
Num timesteps: 6384000
Best mean reward: 4027.52 - Last mean reward per episode: 4084.24
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4080.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 260         |
|    time_elapsed         | 75556       |
|    total_timesteps      | 6389760     |
| train/                  |             |
|    approx_kl            | 0.016379038 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 130         |
|    n_updates            | 2590        |
|    policy_gradient_loss | 0.00266     |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 6396000
Best mean reward: 4084.24 - Last mean reward per episode: 4061.73
Num timesteps: 6408000
Best mean reward: 4084.24 - Last mean reward per episode: 4056.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4120.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 261         |
|    time_elapsed         | 75846       |
|    total_timesteps      | 6414336     |
| train/                  |             |
|    approx_kl            | 0.017820157 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 346         |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 608         |
-----------------------------------------
Num timesteps: 6420000
Best mean reward: 4084.24 - Last mean reward per episode: 4151.12
Saving new best model to tmp/best_model
Num timesteps: 6432000
Best mean reward: 4151.12 - Last mean reward per episode: 4114.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 4064.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 262         |
|    time_elapsed         | 76138       |
|    total_timesteps      | 6438912     |
| train/                  |             |
|    approx_kl            | 0.016239377 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 432         |
|    n_updates            | 2610        |
|    policy_gradient_loss | 0.00253     |
|    value_loss           | 736         |
-----------------------------------------
Num timesteps: 6444000
Best mean reward: 4151.12 - Last mean reward per episode: 4060.36
Num timesteps: 6456000
Best mean reward: 4151.12 - Last mean reward per episode: 3949.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.09e+03    |
|    ep_rew_mean          | 3937.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 263         |
|    time_elapsed         | 76429       |
|    total_timesteps      | 6463488     |
| train/                  |             |
|    approx_kl            | 0.015181187 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 614         |
|    n_updates            | 2620        |
|    policy_gradient_loss | 0.00165     |
|    value_loss           | 788         |
-----------------------------------------
Num timesteps: 6468000
Best mean reward: 4151.12 - Last mean reward per episode: 3912.62
Num timesteps: 6480000
Best mean reward: 4151.12 - Last mean reward per episode: 3907.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 3832.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 264         |
|    time_elapsed         | 76720       |
|    total_timesteps      | 6488064     |
| train/                  |             |
|    approx_kl            | 0.014928181 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 137         |
|    n_updates            | 2630        |
|    policy_gradient_loss | 2.95e-05    |
|    value_loss           | 785         |
-----------------------------------------
Num timesteps: 6492000
Best mean reward: 4151.12 - Last mean reward per episode: 3774.61
Num timesteps: 6504000
Best mean reward: 4151.12 - Last mean reward per episode: 3761.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3713.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 265         |
|    time_elapsed         | 77007       |
|    total_timesteps      | 6512640     |
| train/                  |             |
|    approx_kl            | 0.016521906 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 171         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.000882   |
|    value_loss           | 845         |
-----------------------------------------
Num timesteps: 6516000
Best mean reward: 4151.12 - Last mean reward per episode: 3696.72
Num timesteps: 6528000
Best mean reward: 4151.12 - Last mean reward per episode: 3701.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3682.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 266         |
|    time_elapsed         | 77293       |
|    total_timesteps      | 6537216     |
| train/                  |             |
|    approx_kl            | 0.017519178 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 387         |
|    n_updates            | 2650        |
|    policy_gradient_loss | 0.000971    |
|    value_loss           | 682         |
-----------------------------------------
Num timesteps: 6540000
Best mean reward: 4151.12 - Last mean reward per episode: 3709.60
Num timesteps: 6552000
Best mean reward: 4151.12 - Last mean reward per episode: 3706.92
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.92e+03   |
|    ep_rew_mean          | 3694.6     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 267        |
|    time_elapsed         | 77578      |
|    total_timesteps      | 6561792    |
| train/                  |            |
|    approx_kl            | 0.01581587 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.958      |
|    learning_rate        | 3e-06      |
|    loss                 | 481        |
|    n_updates            | 2660       |
|    policy_gradient_loss | -5.78e-05  |
|    value_loss           | 570        |
----------------------------------------
Num timesteps: 6564000
Best mean reward: 4151.12 - Last mean reward per episode: 3718.62
Num timesteps: 6576000
Best mean reward: 4151.12 - Last mean reward per episode: 3676.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3700.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 268         |
|    time_elapsed         | 77859       |
|    total_timesteps      | 6586368     |
| train/                  |             |
|    approx_kl            | 0.016641645 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 376         |
|    n_updates            | 2670        |
|    policy_gradient_loss | 0.000522    |
|    value_loss           | 718         |
-----------------------------------------
Num timesteps: 6588000
Best mean reward: 4151.12 - Last mean reward per episode: 3700.61
Num timesteps: 6600000
Best mean reward: 4151.12 - Last mean reward per episode: 3774.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3777.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 269         |
|    time_elapsed         | 78147       |
|    total_timesteps      | 6610944     |
| train/                  |             |
|    approx_kl            | 0.015157097 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 451         |
|    n_updates            | 2680        |
|    policy_gradient_loss | 0.000154    |
|    value_loss           | 679         |
-----------------------------------------
Num timesteps: 6612000
Best mean reward: 4151.12 - Last mean reward per episode: 3784.66
Num timesteps: 6624000
Best mean reward: 4151.12 - Last mean reward per episode: 3784.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.98e+03    |
|    ep_rew_mean          | 3797.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 270         |
|    time_elapsed         | 78432       |
|    total_timesteps      | 6635520     |
| train/                  |             |
|    approx_kl            | 0.017190909 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 328         |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00037    |
|    value_loss           | 600         |
-----------------------------------------
Num timesteps: 6636000
Best mean reward: 4151.12 - Last mean reward per episode: 3797.91
Num timesteps: 6648000
Best mean reward: 4151.12 - Last mean reward per episode: 3793.71
Num timesteps: 6660000
Best mean reward: 4151.12 - Last mean reward per episode: 3896.60
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.03e+03   |
|    ep_rew_mean          | 3896.6     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 271        |
|    time_elapsed         | 78723      |
|    total_timesteps      | 6660096    |
| train/                  |            |
|    approx_kl            | 0.01464206 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-06      |
|    loss                 | 234        |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.000212  |
|    value_loss           | 671        |
----------------------------------------
Num timesteps: 6672000
Best mean reward: 4151.12 - Last mean reward per episode: 3969.38
Num timesteps: 6684000
Best mean reward: 4151.12 - Last mean reward per episode: 3955.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.06e+03    |
|    ep_rew_mean          | 3959.13     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 272         |
|    time_elapsed         | 79007       |
|    total_timesteps      | 6684672     |
| train/                  |             |
|    approx_kl            | 0.012097545 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 292         |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.000606   |
|    value_loss           | 544         |
-----------------------------------------
Num timesteps: 6696000
Best mean reward: 4151.12 - Last mean reward per episode: 4020.76
Num timesteps: 6708000
Best mean reward: 4151.12 - Last mean reward per episode: 4126.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16e+03    |
|    ep_rew_mean          | 4126.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 273         |
|    time_elapsed         | 79300       |
|    total_timesteps      | 6709248     |
| train/                  |             |
|    approx_kl            | 0.016959159 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 438         |
|    n_updates            | 2720        |
|    policy_gradient_loss | 6.48e-05    |
|    value_loss           | 688         |
-----------------------------------------
Num timesteps: 6720000
Best mean reward: 4151.12 - Last mean reward per episode: 4158.14
Saving new best model to tmp/best_model
Num timesteps: 6732000
Best mean reward: 4158.14 - Last mean reward per episode: 4227.41
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 4205.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 274         |
|    time_elapsed         | 79587       |
|    total_timesteps      | 6733824     |
| train/                  |             |
|    approx_kl            | 0.015613199 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00031    |
|    value_loss           | 631         |
-----------------------------------------
Num timesteps: 6744000
Best mean reward: 4227.41 - Last mean reward per episode: 4209.26
Num timesteps: 6756000
Best mean reward: 4227.41 - Last mean reward per episode: 4191.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 4191.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 275         |
|    time_elapsed         | 79887       |
|    total_timesteps      | 6758400     |
| train/                  |             |
|    approx_kl            | 0.018181214 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 888         |
|    n_updates            | 2740        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 667         |
-----------------------------------------
Num timesteps: 6768000
Best mean reward: 4227.41 - Last mean reward per episode: 4185.78
Num timesteps: 6780000
Best mean reward: 4227.41 - Last mean reward per episode: 4195.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 4178.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 276         |
|    time_elapsed         | 80181       |
|    total_timesteps      | 6782976     |
| train/                  |             |
|    approx_kl            | 0.017357908 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 725         |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.000698   |
|    value_loss           | 640         |
-----------------------------------------
Num timesteps: 6792000
Best mean reward: 4227.41 - Last mean reward per episode: 4190.86
Num timesteps: 6804000
Best mean reward: 4227.41 - Last mean reward per episode: 4181.72
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.19e+03   |
|    ep_rew_mean          | 4181.72    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 277        |
|    time_elapsed         | 80478      |
|    total_timesteps      | 6807552    |
| train/                  |            |
|    approx_kl            | 0.01595109 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.05      |
|    explained_variance   | 0.952      |
|    learning_rate        | 3e-06      |
|    loss                 | 509        |
|    n_updates            | 2760       |
|    policy_gradient_loss | 0.000864   |
|    value_loss           | 639        |
----------------------------------------
Num timesteps: 6816000
Best mean reward: 4227.41 - Last mean reward per episode: 4273.19
Saving new best model to tmp/best_model
Num timesteps: 6828000
Best mean reward: 4273.19 - Last mean reward per episode: 4279.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 4259.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 278         |
|    time_elapsed         | 80774       |
|    total_timesteps      | 6832128     |
| train/                  |             |
|    approx_kl            | 0.017008437 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 284         |
|    n_updates            | 2770        |
|    policy_gradient_loss | 0.00149     |
|    value_loss           | 582         |
-----------------------------------------
Num timesteps: 6840000
Best mean reward: 4279.59 - Last mean reward per episode: 4215.50
Num timesteps: 6852000
Best mean reward: 4279.59 - Last mean reward per episode: 4223.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4215.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 279         |
|    time_elapsed         | 81062       |
|    total_timesteps      | 6856704     |
| train/                  |             |
|    approx_kl            | 0.014638805 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 446         |
|    n_updates            | 2780        |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 658         |
-----------------------------------------
Num timesteps: 6864000
Best mean reward: 4279.59 - Last mean reward per episode: 4199.69
Num timesteps: 6876000
Best mean reward: 4279.59 - Last mean reward per episode: 4132.09
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 4101.44    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 280        |
|    time_elapsed         | 81355      |
|    total_timesteps      | 6881280    |
| train/                  |            |
|    approx_kl            | 0.01931112 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 297        |
|    n_updates            | 2790       |
|    policy_gradient_loss | 0.000932   |
|    value_loss           | 828        |
----------------------------------------
Num timesteps: 6888000
Best mean reward: 4279.59 - Last mean reward per episode: 4037.23
Num timesteps: 6900000
Best mean reward: 4279.59 - Last mean reward per episode: 4078.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 4078.43    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 281        |
|    time_elapsed         | 81643      |
|    total_timesteps      | 6905856    |
| train/                  |            |
|    approx_kl            | 0.01837156 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-06      |
|    loss                 | 158        |
|    n_updates            | 2800       |
|    policy_gradient_loss | 0.00181    |
|    value_loss           | 675        |
----------------------------------------
Num timesteps: 6912000
Best mean reward: 4279.59 - Last mean reward per episode: 4078.38
Num timesteps: 6924000
Best mean reward: 4279.59 - Last mean reward per episode: 4043.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.12e+03    |
|    ep_rew_mean          | 3998.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 282         |
|    time_elapsed         | 81931       |
|    total_timesteps      | 6930432     |
| train/                  |             |
|    approx_kl            | 0.016751915 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 669         |
|    n_updates            | 2810        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 520         |
-----------------------------------------
Num timesteps: 6936000
Best mean reward: 4279.59 - Last mean reward per episode: 3980.97
Num timesteps: 6948000
Best mean reward: 4279.59 - Last mean reward per episode: 3951.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.14e+03    |
|    ep_rew_mean          | 3907.75     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 283         |
|    time_elapsed         | 82219       |
|    total_timesteps      | 6955008     |
| train/                  |             |
|    approx_kl            | 0.017386034 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 625         |
|    n_updates            | 2820        |
|    policy_gradient_loss | 0.00065     |
|    value_loss           | 679         |
-----------------------------------------
Num timesteps: 6960000
Best mean reward: 4279.59 - Last mean reward per episode: 3920.05
Num timesteps: 6972000
Best mean reward: 4279.59 - Last mean reward per episode: 3893.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3891.64     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 284         |
|    time_elapsed         | 82510       |
|    total_timesteps      | 6979584     |
| train/                  |             |
|    approx_kl            | 0.015911696 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 152         |
|    n_updates            | 2830        |
|    policy_gradient_loss | 0.000719    |
|    value_loss           | 585         |
-----------------------------------------
Num timesteps: 6984000
Best mean reward: 4279.59 - Last mean reward per episode: 3896.48
Num timesteps: 6996000
Best mean reward: 4279.59 - Last mean reward per episode: 3930.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16e+03    |
|    ep_rew_mean          | 3981.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 285         |
|    time_elapsed         | 82801       |
|    total_timesteps      | 7004160     |
| train/                  |             |
|    approx_kl            | 0.017226363 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 248         |
|    n_updates            | 2840        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 604         |
-----------------------------------------
Num timesteps: 7008000
Best mean reward: 4279.59 - Last mean reward per episode: 4006.92
Num timesteps: 7020000
Best mean reward: 4279.59 - Last mean reward per episode: 3983.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 3886.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 286         |
|    time_elapsed         | 83096       |
|    total_timesteps      | 7028736     |
| train/                  |             |
|    approx_kl            | 0.016687933 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 169         |
|    n_updates            | 2850        |
|    policy_gradient_loss | 0.0026      |
|    value_loss           | 525         |
-----------------------------------------
Num timesteps: 7032000
Best mean reward: 4279.59 - Last mean reward per episode: 3907.78
Num timesteps: 7044000
Best mean reward: 4279.59 - Last mean reward per episode: 3911.48
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.12e+03     |
|    ep_rew_mean          | 3928.29      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 287          |
|    time_elapsed         | 83400        |
|    total_timesteps      | 7053312      |
| train/                  |              |
|    approx_kl            | 0.0155957425 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.957        |
|    learning_rate        | 3e-06        |
|    loss                 | 110          |
|    n_updates            | 2860         |
|    policy_gradient_loss | 0.00031      |
|    value_loss           | 566          |
------------------------------------------
Num timesteps: 7056000
Best mean reward: 4279.59 - Last mean reward per episode: 3956.98
Num timesteps: 7068000
Best mean reward: 4279.59 - Last mean reward per episode: 4000.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 3991.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 288         |
|    time_elapsed         | 83694       |
|    total_timesteps      | 7077888     |
| train/                  |             |
|    approx_kl            | 0.013727456 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.000968   |
|    value_loss           | 527         |
-----------------------------------------
Num timesteps: 7080000
Best mean reward: 4279.59 - Last mean reward per episode: 3991.80
Num timesteps: 7092000
Best mean reward: 4279.59 - Last mean reward per episode: 4026.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4078.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 289         |
|    time_elapsed         | 83993       |
|    total_timesteps      | 7102464     |
| train/                  |             |
|    approx_kl            | 0.018628003 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 263         |
|    n_updates            | 2880        |
|    policy_gradient_loss | 0.00199     |
|    value_loss           | 806         |
-----------------------------------------
Num timesteps: 7104000
Best mean reward: 4279.59 - Last mean reward per episode: 4065.45
Num timesteps: 7116000
Best mean reward: 4279.59 - Last mean reward per episode: 4072.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4108.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 290         |
|    time_elapsed         | 84282       |
|    total_timesteps      | 7127040     |
| train/                  |             |
|    approx_kl            | 0.017661292 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 253         |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 727         |
-----------------------------------------
Num timesteps: 7128000
Best mean reward: 4279.59 - Last mean reward per episode: 4093.90
Num timesteps: 7140000
Best mean reward: 4279.59 - Last mean reward per episode: 4107.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4141.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 291         |
|    time_elapsed         | 84575       |
|    total_timesteps      | 7151616     |
| train/                  |             |
|    approx_kl            | 0.017762953 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 342         |
|    n_updates            | 2900        |
|    policy_gradient_loss | 0.000169    |
|    value_loss           | 645         |
-----------------------------------------
Num timesteps: 7152000
Best mean reward: 4279.59 - Last mean reward per episode: 4141.45
Num timesteps: 7164000
Best mean reward: 4279.59 - Last mean reward per episode: 4239.45
Num timesteps: 7176000
Best mean reward: 4279.59 - Last mean reward per episode: 4225.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.29e+03   |
|    ep_rew_mean          | 4225.34    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 292        |
|    time_elapsed         | 84867      |
|    total_timesteps      | 7176192    |
| train/                  |            |
|    approx_kl            | 0.01431597 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.964      |
|    learning_rate        | 3e-06      |
|    loss                 | 175        |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.00176   |
|    value_loss           | 511        |
----------------------------------------
Num timesteps: 7188000
Best mean reward: 4279.59 - Last mean reward per episode: 4242.30
Num timesteps: 7200000
Best mean reward: 4279.59 - Last mean reward per episode: 4242.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4242.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 293         |
|    time_elapsed         | 85154       |
|    total_timesteps      | 7200768     |
| train/                  |             |
|    approx_kl            | 0.015828034 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 147         |
|    n_updates            | 2920        |
|    policy_gradient_loss | 0.000881    |
|    value_loss           | 673         |
-----------------------------------------
Num timesteps: 7212000
Best mean reward: 4279.59 - Last mean reward per episode: 4157.32
Num timesteps: 7224000
Best mean reward: 4279.59 - Last mean reward per episode: 4153.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 4159.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 294         |
|    time_elapsed         | 85444       |
|    total_timesteps      | 7225344     |
| train/                  |             |
|    approx_kl            | 0.018185195 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 454         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.000188   |
|    value_loss           | 837         |
-----------------------------------------
Num timesteps: 7236000
Best mean reward: 4279.59 - Last mean reward per episode: 4174.47
Num timesteps: 7248000
Best mean reward: 4279.59 - Last mean reward per episode: 4176.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4215.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 295         |
|    time_elapsed         | 85731       |
|    total_timesteps      | 7249920     |
| train/                  |             |
|    approx_kl            | 0.017168047 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 202         |
|    n_updates            | 2940        |
|    policy_gradient_loss | 0.001       |
|    value_loss           | 595         |
-----------------------------------------
Num timesteps: 7260000
Best mean reward: 4279.59 - Last mean reward per episode: 4237.55
Num timesteps: 7272000
Best mean reward: 4279.59 - Last mean reward per episode: 4198.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4198.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 296         |
|    time_elapsed         | 86019       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.015197553 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 251         |
|    n_updates            | 2950        |
|    policy_gradient_loss | 0.000466    |
|    value_loss           | 583         |
-----------------------------------------
Num timesteps: 7284000
Best mean reward: 4279.59 - Last mean reward per episode: 4143.67
Num timesteps: 7296000
Best mean reward: 4279.59 - Last mean reward per episode: 4164.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4201.63     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 297         |
|    time_elapsed         | 86309       |
|    total_timesteps      | 7299072     |
| train/                  |             |
|    approx_kl            | 0.015453234 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 340         |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.000307   |
|    value_loss           | 579         |
-----------------------------------------
Num timesteps: 7308000
Best mean reward: 4279.59 - Last mean reward per episode: 4254.96
Num timesteps: 7320000
Best mean reward: 4279.59 - Last mean reward per episode: 4310.77
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4309.4      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 298         |
|    time_elapsed         | 86599       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.016761266 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 288         |
|    n_updates            | 2970        |
|    policy_gradient_loss | 0.000594    |
|    value_loss           | 728         |
-----------------------------------------
Num timesteps: 7332000
Best mean reward: 4310.77 - Last mean reward per episode: 4298.53
Num timesteps: 7344000
Best mean reward: 4310.77 - Last mean reward per episode: 4337.56
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 4351.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 299         |
|    time_elapsed         | 86886       |
|    total_timesteps      | 7348224     |
| train/                  |             |
|    approx_kl            | 0.015590479 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 229         |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.000607   |
|    value_loss           | 568         |
-----------------------------------------
Num timesteps: 7356000
Best mean reward: 4337.56 - Last mean reward per episode: 4348.55
Saving new best model to tmp/best_model
Num timesteps: 7368000
Best mean reward: 4348.55 - Last mean reward per episode: 4386.55
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4386.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 300         |
|    time_elapsed         | 87171       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.019293094 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 344         |
|    n_updates            | 2990        |
|    policy_gradient_loss | 0.000756    |
|    value_loss           | 674         |
-----------------------------------------
Num timesteps: 7380000
Best mean reward: 4386.55 - Last mean reward per episode: 4373.98
Num timesteps: 7392000
Best mean reward: 4386.55 - Last mean reward per episode: 4354.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4392.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 301         |
|    time_elapsed         | 87457       |
|    total_timesteps      | 7397376     |
| train/                  |             |
|    approx_kl            | 0.015814483 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 167         |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.000762   |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 7404000
Best mean reward: 4386.55 - Last mean reward per episode: 4419.10
Saving new best model to tmp/best_model
Num timesteps: 7416000
Best mean reward: 4419.10 - Last mean reward per episode: 4471.95
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4436.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 302         |
|    time_elapsed         | 87745       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.015627496 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 223         |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.000155   |
|    value_loss           | 439         |
-----------------------------------------
Num timesteps: 7428000
Best mean reward: 4471.95 - Last mean reward per episode: 4434.17
Num timesteps: 7440000
Best mean reward: 4471.95 - Last mean reward per episode: 4436.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4441.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 303         |
|    time_elapsed         | 88032       |
|    total_timesteps      | 7446528     |
| train/                  |             |
|    approx_kl            | 0.016305527 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 3020        |
|    policy_gradient_loss | 0.00135     |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 7452000
Best mean reward: 4471.95 - Last mean reward per episode: 4407.45
Num timesteps: 7464000
Best mean reward: 4471.95 - Last mean reward per episode: 4313.80
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.32e+03   |
|    ep_rew_mean          | 4313.8     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 304        |
|    time_elapsed         | 88317      |
|    total_timesteps      | 7471104    |
| train/                  |            |
|    approx_kl            | 0.01816682 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.958      |
|    learning_rate        | 3e-06      |
|    loss                 | 276        |
|    n_updates            | 3030       |
|    policy_gradient_loss | 1.31e-05   |
|    value_loss           | 623        |
----------------------------------------
Num timesteps: 7476000
Best mean reward: 4471.95 - Last mean reward per episode: 4313.80
Num timesteps: 7488000
Best mean reward: 4471.95 - Last mean reward per episode: 4268.02
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.31e+03  |
|    ep_rew_mean          | 4268.84   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 305       |
|    time_elapsed         | 88605     |
|    total_timesteps      | 7495680   |
| train/                  |           |
|    approx_kl            | 0.0195875 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.54     |
|    explained_variance   | 0.96      |
|    learning_rate        | 3e-06     |
|    loss                 | 121       |
|    n_updates            | 3040      |
|    policy_gradient_loss | 0.00323   |
|    value_loss           | 545       |
---------------------------------------
Num timesteps: 7500000
Best mean reward: 4471.95 - Last mean reward per episode: 4268.84
Num timesteps: 7512000
Best mean reward: 4471.95 - Last mean reward per episode: 4229.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4212.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 306         |
|    time_elapsed         | 88889       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.014813937 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 181         |
|    n_updates            | 3050        |
|    policy_gradient_loss | 0.000279    |
|    value_loss           | 631         |
-----------------------------------------
Num timesteps: 7524000
Best mean reward: 4471.95 - Last mean reward per episode: 4200.86
Num timesteps: 7536000
Best mean reward: 4471.95 - Last mean reward per episode: 4131.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4057.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 307         |
|    time_elapsed         | 89175       |
|    total_timesteps      | 7544832     |
| train/                  |             |
|    approx_kl            | 0.015307202 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 416         |
|    n_updates            | 3060        |
|    policy_gradient_loss | 0.000127    |
|    value_loss           | 666         |
-----------------------------------------
Num timesteps: 7548000
Best mean reward: 4471.95 - Last mean reward per episode: 4020.69
Num timesteps: 7560000
Best mean reward: 4471.95 - Last mean reward per episode: 3978.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 3980.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 308         |
|    time_elapsed         | 89459       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.015788194 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 125         |
|    n_updates            | 3070        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 508         |
-----------------------------------------
Num timesteps: 7572000
Best mean reward: 4471.95 - Last mean reward per episode: 3951.81
Num timesteps: 7584000
Best mean reward: 4471.95 - Last mean reward per episode: 3940.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.14e+03    |
|    ep_rew_mean          | 3853.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 309         |
|    time_elapsed         | 89745       |
|    total_timesteps      | 7593984     |
| train/                  |             |
|    approx_kl            | 0.021126384 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 177         |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 707         |
-----------------------------------------
Num timesteps: 7596000
Best mean reward: 4471.95 - Last mean reward per episode: 3874.66
Num timesteps: 7608000
Best mean reward: 4471.95 - Last mean reward per episode: 3826.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 3807.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 310         |
|    time_elapsed         | 90035       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.017743872 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 407         |
|    n_updates            | 3090        |
|    policy_gradient_loss | 0.00282     |
|    value_loss           | 735         |
-----------------------------------------
Num timesteps: 7620000
Best mean reward: 4471.95 - Last mean reward per episode: 3826.27
Num timesteps: 7632000
Best mean reward: 4471.95 - Last mean reward per episode: 3809.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 3801.19    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 311        |
|    time_elapsed         | 90324      |
|    total_timesteps      | 7643136    |
| train/                  |            |
|    approx_kl            | 0.01885411 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-06      |
|    loss                 | 188        |
|    n_updates            | 3100       |
|    policy_gradient_loss | 0.000413   |
|    value_loss           | 664        |
----------------------------------------
Num timesteps: 7644000
Best mean reward: 4471.95 - Last mean reward per episode: 3791.96
Num timesteps: 7656000
Best mean reward: 4471.95 - Last mean reward per episode: 3856.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3864.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 312         |
|    time_elapsed         | 90618       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.013781144 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 3110        |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 520         |
-----------------------------------------
Num timesteps: 7668000
Best mean reward: 4471.95 - Last mean reward per episode: 3877.56
Num timesteps: 7680000
Best mean reward: 4471.95 - Last mean reward per episode: 3895.82
Num timesteps: 7692000
Best mean reward: 4471.95 - Last mean reward per episode: 3947.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 3947.15     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 313         |
|    time_elapsed         | 90906       |
|    total_timesteps      | 7692288     |
| train/                  |             |
|    approx_kl            | 0.017858038 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 271         |
|    n_updates            | 3120        |
|    policy_gradient_loss | 0.00398     |
|    value_loss           | 667         |
-----------------------------------------
Num timesteps: 7704000
Best mean reward: 4471.95 - Last mean reward per episode: 3893.78
Num timesteps: 7716000
Best mean reward: 4471.95 - Last mean reward per episode: 3892.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 3892.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 314         |
|    time_elapsed         | 91196       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.017752724 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 502         |
|    n_updates            | 3130        |
|    policy_gradient_loss | 0.000646    |
|    value_loss           | 595         |
-----------------------------------------
Num timesteps: 7728000
Best mean reward: 4471.95 - Last mean reward per episode: 3947.16
Num timesteps: 7740000
Best mean reward: 4471.95 - Last mean reward per episode: 4019.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 4019.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 315         |
|    time_elapsed         | 91487       |
|    total_timesteps      | 7741440     |
| train/                  |             |
|    approx_kl            | 0.017806573 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 220         |
|    n_updates            | 3140        |
|    policy_gradient_loss | -2.74e-05   |
|    value_loss           | 596         |
-----------------------------------------
Num timesteps: 7752000
Best mean reward: 4471.95 - Last mean reward per episode: 4011.42
Num timesteps: 7764000
Best mean reward: 4471.95 - Last mean reward per episode: 4109.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4114.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 316         |
|    time_elapsed         | 91777       |
|    total_timesteps      | 7766016     |
| train/                  |             |
|    approx_kl            | 0.016432486 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 255         |
|    n_updates            | 3150        |
|    policy_gradient_loss | 0.00169     |
|    value_loss           | 562         |
-----------------------------------------
Num timesteps: 7776000
Best mean reward: 4471.95 - Last mean reward per episode: 4136.86
Num timesteps: 7788000
Best mean reward: 4471.95 - Last mean reward per episode: 4111.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4065.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 317         |
|    time_elapsed         | 92066       |
|    total_timesteps      | 7790592     |
| train/                  |             |
|    approx_kl            | 0.016766021 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 184         |
|    n_updates            | 3160        |
|    policy_gradient_loss | 0.000213    |
|    value_loss           | 566         |
-----------------------------------------
Num timesteps: 7800000
Best mean reward: 4471.95 - Last mean reward per episode: 4040.81
Num timesteps: 7812000
Best mean reward: 4471.95 - Last mean reward per episode: 4028.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4028.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 318         |
|    time_elapsed         | 92357       |
|    total_timesteps      | 7815168     |
| train/                  |             |
|    approx_kl            | 0.018249178 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 525         |
|    n_updates            | 3170        |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 808         |
-----------------------------------------
Num timesteps: 7824000
Best mean reward: 4471.95 - Last mean reward per episode: 4065.55
Num timesteps: 7836000
Best mean reward: 4471.95 - Last mean reward per episode: 4079.32
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4079.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 319         |
|    time_elapsed         | 92645       |
|    total_timesteps      | 7839744     |
| train/                  |             |
|    approx_kl            | 0.019673683 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 377         |
|    n_updates            | 3180        |
|    policy_gradient_loss | 0.00368     |
|    value_loss           | 737         |
-----------------------------------------
Num timesteps: 7848000
Best mean reward: 4471.95 - Last mean reward per episode: 4143.83
Num timesteps: 7860000
Best mean reward: 4471.95 - Last mean reward per episode: 4215.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4201.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 320         |
|    time_elapsed         | 92931       |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.014654775 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 226         |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 568         |
-----------------------------------------
Num timesteps: 7872000
Best mean reward: 4471.95 - Last mean reward per episode: 4224.52
Num timesteps: 7884000
Best mean reward: 4471.95 - Last mean reward per episode: 4194.41
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.27e+03  |
|    ep_rew_mean          | 4195.79   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 321       |
|    time_elapsed         | 93220     |
|    total_timesteps      | 7888896   |
| train/                  |           |
|    approx_kl            | 0.0161421 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.5      |
|    explained_variance   | 0.947     |
|    learning_rate        | 3e-06     |
|    loss                 | 296       |
|    n_updates            | 3200      |
|    policy_gradient_loss | -9.79e-05 |
|    value_loss           | 661       |
---------------------------------------
Num timesteps: 7896000
Best mean reward: 4471.95 - Last mean reward per episode: 4184.59
Num timesteps: 7908000
Best mean reward: 4471.95 - Last mean reward per episode: 4184.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4188.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 322         |
|    time_elapsed         | 93508       |
|    total_timesteps      | 7913472     |
| train/                  |             |
|    approx_kl            | 0.016504316 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 3210        |
|    policy_gradient_loss | 0.00039     |
|    value_loss           | 576         |
-----------------------------------------
Num timesteps: 7920000
Best mean reward: 4471.95 - Last mean reward per episode: 4255.26
Num timesteps: 7932000
Best mean reward: 4471.95 - Last mean reward per episode: 4316.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4336.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 323         |
|    time_elapsed         | 93800       |
|    total_timesteps      | 7938048     |
| train/                  |             |
|    approx_kl            | 0.015827885 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 496         |
|    n_updates            | 3220        |
|    policy_gradient_loss | 0.000105    |
|    value_loss           | 555         |
-----------------------------------------
Num timesteps: 7944000
Best mean reward: 4471.95 - Last mean reward per episode: 4336.26
Num timesteps: 7956000
Best mean reward: 4471.95 - Last mean reward per episode: 4338.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.24e+03    |
|    ep_rew_mean          | 4313.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 324         |
|    time_elapsed         | 94088       |
|    total_timesteps      | 7962624     |
| train/                  |             |
|    approx_kl            | 0.015654888 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 205         |
|    n_updates            | 3230        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 593         |
-----------------------------------------
Num timesteps: 7968000
Best mean reward: 4471.95 - Last mean reward per episode: 4355.26
Num timesteps: 7980000
Best mean reward: 4471.95 - Last mean reward per episode: 4252.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 4203.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 325         |
|    time_elapsed         | 94379       |
|    total_timesteps      | 7987200     |
| train/                  |             |
|    approx_kl            | 0.013797618 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 89          |
|    n_updates            | 3240        |
|    policy_gradient_loss | -4.1e-05    |
|    value_loss           | 519         |
-----------------------------------------
Num timesteps: 7992000
Best mean reward: 4471.95 - Last mean reward per episode: 4203.93
Num timesteps: 8004000
Best mean reward: 4471.95 - Last mean reward per episode: 4313.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4349.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 326         |
|    time_elapsed         | 94680       |
|    total_timesteps      | 8011776     |
| train/                  |             |
|    approx_kl            | 0.017806439 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 227         |
|    n_updates            | 3250        |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 663         |
-----------------------------------------
Num timesteps: 8016000
Best mean reward: 4471.95 - Last mean reward per episode: 4346.50
Num timesteps: 8028000
Best mean reward: 4471.95 - Last mean reward per episode: 4377.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.29e+03   |
|    ep_rew_mean          | 4463.83    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 327        |
|    time_elapsed         | 94968      |
|    total_timesteps      | 8036352    |
| train/                  |            |
|    approx_kl            | 0.01803949 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.41      |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-06      |
|    loss                 | 151        |
|    n_updates            | 3260       |
|    policy_gradient_loss | 0.00266    |
|    value_loss           | 605        |
----------------------------------------
Num timesteps: 8040000
Best mean reward: 4471.95 - Last mean reward per episode: 4478.60
Saving new best model to tmp/best_model
Num timesteps: 8052000
Best mean reward: 4478.60 - Last mean reward per episode: 4471.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.31e+03   |
|    ep_rew_mean          | 4490.4     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 328        |
|    time_elapsed         | 95261      |
|    total_timesteps      | 8060928    |
| train/                  |            |
|    approx_kl            | 0.01838169 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-06      |
|    loss                 | 299        |
|    n_updates            | 3270       |
|    policy_gradient_loss | 0.000746   |
|    value_loss           | 644        |
----------------------------------------
Num timesteps: 8064000
Best mean reward: 4478.60 - Last mean reward per episode: 4505.07
Saving new best model to tmp/best_model
Num timesteps: 8076000
Best mean reward: 4505.07 - Last mean reward per episode: 4450.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4437.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 329         |
|    time_elapsed         | 95556       |
|    total_timesteps      | 8085504     |
| train/                  |             |
|    approx_kl            | 0.016672859 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 203         |
|    n_updates            | 3280        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 535         |
-----------------------------------------
Num timesteps: 8088000
Best mean reward: 4505.07 - Last mean reward per episode: 4401.57
Num timesteps: 8100000
Best mean reward: 4505.07 - Last mean reward per episode: 4423.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4457.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 330         |
|    time_elapsed         | 95843       |
|    total_timesteps      | 8110080     |
| train/                  |             |
|    approx_kl            | 0.015233251 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 258         |
|    n_updates            | 3290        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 511         |
-----------------------------------------
Num timesteps: 8112000
Best mean reward: 4505.07 - Last mean reward per episode: 4459.16
Num timesteps: 8124000
Best mean reward: 4505.07 - Last mean reward per episode: 4442.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4442.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 331         |
|    time_elapsed         | 96131       |
|    total_timesteps      | 8134656     |
| train/                  |             |
|    approx_kl            | 0.014584926 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 282         |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.000566   |
|    value_loss           | 579         |
-----------------------------------------
Num timesteps: 8136000
Best mean reward: 4505.07 - Last mean reward per episode: 4437.43
Num timesteps: 8148000
Best mean reward: 4505.07 - Last mean reward per episode: 4374.99
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.27e+03  |
|    ep_rew_mean          | 4411.24   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 332       |
|    time_elapsed         | 96422     |
|    total_timesteps      | 8159232   |
| train/                  |           |
|    approx_kl            | 0.0164338 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36     |
|    explained_variance   | 0.957     |
|    learning_rate        | 3e-06     |
|    loss                 | 382       |
|    n_updates            | 3310      |
|    policy_gradient_loss | 0.000278  |
|    value_loss           | 647       |
---------------------------------------
Num timesteps: 8160000
Best mean reward: 4505.07 - Last mean reward per episode: 4417.63
Num timesteps: 8172000
Best mean reward: 4505.07 - Last mean reward per episode: 4428.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4384.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 333         |
|    time_elapsed         | 96714       |
|    total_timesteps      | 8183808     |
| train/                  |             |
|    approx_kl            | 0.013357334 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 3320        |
|    policy_gradient_loss | 0.000681    |
|    value_loss           | 496         |
-----------------------------------------
Num timesteps: 8184000
Best mean reward: 4505.07 - Last mean reward per episode: 4377.19
Num timesteps: 8196000
Best mean reward: 4505.07 - Last mean reward per episode: 4377.99
Num timesteps: 8208000
Best mean reward: 4505.07 - Last mean reward per episode: 4409.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 4409.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 334         |
|    time_elapsed         | 97001       |
|    total_timesteps      | 8208384     |
| train/                  |             |
|    approx_kl            | 0.019752866 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 715         |
|    n_updates            | 3330        |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 722         |
-----------------------------------------
Num timesteps: 8220000
Best mean reward: 4505.07 - Last mean reward per episode: 4522.28
Saving new best model to tmp/best_model
Num timesteps: 8232000
Best mean reward: 4522.28 - Last mean reward per episode: 4446.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4446.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 335         |
|    time_elapsed         | 97292       |
|    total_timesteps      | 8232960     |
| train/                  |             |
|    approx_kl            | 0.016899372 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 295         |
|    n_updates            | 3340        |
|    policy_gradient_loss | 0.00233     |
|    value_loss           | 559         |
-----------------------------------------
Num timesteps: 8244000
Best mean reward: 4522.28 - Last mean reward per episode: 4466.72
Num timesteps: 8256000
Best mean reward: 4522.28 - Last mean reward per episode: 4463.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4468.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 336         |
|    time_elapsed         | 97583       |
|    total_timesteps      | 8257536     |
| train/                  |             |
|    approx_kl            | 0.019003583 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 215         |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.000242   |
|    value_loss           | 621         |
-----------------------------------------
Num timesteps: 8268000
Best mean reward: 4522.28 - Last mean reward per episode: 4539.59
Saving new best model to tmp/best_model
Num timesteps: 8280000
Best mean reward: 4539.59 - Last mean reward per episode: 4538.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4538.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 337         |
|    time_elapsed         | 97878       |
|    total_timesteps      | 8282112     |
| train/                  |             |
|    approx_kl            | 0.017258309 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 93.4        |
|    n_updates            | 3360        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 539         |
-----------------------------------------
Num timesteps: 8292000
Best mean reward: 4539.59 - Last mean reward per episode: 4555.62
Saving new best model to tmp/best_model
Num timesteps: 8304000
Best mean reward: 4555.62 - Last mean reward per episode: 4560.11
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4560.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 338         |
|    time_elapsed         | 98169       |
|    total_timesteps      | 8306688     |
| train/                  |             |
|    approx_kl            | 0.016968567 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 102         |
|    n_updates            | 3370        |
|    policy_gradient_loss | 0.000574    |
|    value_loss           | 538         |
-----------------------------------------
Num timesteps: 8316000
Best mean reward: 4560.11 - Last mean reward per episode: 4562.98
Saving new best model to tmp/best_model
Num timesteps: 8328000
Best mean reward: 4562.98 - Last mean reward per episode: 4596.85
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4612.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 339         |
|    time_elapsed         | 98458       |
|    total_timesteps      | 8331264     |
| train/                  |             |
|    approx_kl            | 0.014576383 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 8340000
Best mean reward: 4596.85 - Last mean reward per episode: 4637.80
Saving new best model to tmp/best_model
Num timesteps: 8352000
Best mean reward: 4637.80 - Last mean reward per episode: 4678.43
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4737.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 340         |
|    time_elapsed         | 98765       |
|    total_timesteps      | 8355840     |
| train/                  |             |
|    approx_kl            | 0.014423638 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 746         |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.000309   |
|    value_loss           | 719         |
-----------------------------------------
Num timesteps: 8364000
Best mean reward: 4678.43 - Last mean reward per episode: 4712.32
Saving new best model to tmp/best_model
Num timesteps: 8376000
Best mean reward: 4712.32 - Last mean reward per episode: 4745.64
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4745.64     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 341         |
|    time_elapsed         | 99062       |
|    total_timesteps      | 8380416     |
| train/                  |             |
|    approx_kl            | 0.013825472 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.000732   |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 8388000
Best mean reward: 4745.64 - Last mean reward per episode: 4739.70
Num timesteps: 8400000
Best mean reward: 4745.64 - Last mean reward per episode: 4730.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4730.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 342         |
|    time_elapsed         | 99358       |
|    total_timesteps      | 8404992     |
| train/                  |             |
|    approx_kl            | 0.019858258 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 512         |
|    n_updates            | 3410        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 686         |
-----------------------------------------
Num timesteps: 8412000
Best mean reward: 4745.64 - Last mean reward per episode: 4762.45
Saving new best model to tmp/best_model
Num timesteps: 8424000
Best mean reward: 4762.45 - Last mean reward per episode: 4787.50
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4783.63     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 343         |
|    time_elapsed         | 99652       |
|    total_timesteps      | 8429568     |
| train/                  |             |
|    approx_kl            | 0.018615028 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 436         |
|    n_updates            | 3420        |
|    policy_gradient_loss | 0.00318     |
|    value_loss           | 585         |
-----------------------------------------
Num timesteps: 8436000
Best mean reward: 4787.50 - Last mean reward per episode: 4757.11
Num timesteps: 8448000
Best mean reward: 4787.50 - Last mean reward per episode: 4729.30
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.37e+03   |
|    ep_rew_mean          | 4692.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 344        |
|    time_elapsed         | 99951      |
|    total_timesteps      | 8454144    |
| train/                  |            |
|    approx_kl            | 0.01500545 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.96       |
|    learning_rate        | 3e-06      |
|    loss                 | 181        |
|    n_updates            | 3430       |
|    policy_gradient_loss | -0.000732  |
|    value_loss           | 565        |
----------------------------------------
Num timesteps: 8460000
Best mean reward: 4787.50 - Last mean reward per episode: 4675.41
Num timesteps: 8472000
Best mean reward: 4787.50 - Last mean reward per episode: 4708.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4744.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 345         |
|    time_elapsed         | 100247      |
|    total_timesteps      | 8478720     |
| train/                  |             |
|    approx_kl            | 0.020056937 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 618         |
|    n_updates            | 3440        |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 777         |
-----------------------------------------
Num timesteps: 8484000
Best mean reward: 4787.50 - Last mean reward per episode: 4730.41
Num timesteps: 8496000
Best mean reward: 4787.50 - Last mean reward per episode: 4759.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4725.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 346         |
|    time_elapsed         | 100543      |
|    total_timesteps      | 8503296     |
| train/                  |             |
|    approx_kl            | 0.017019408 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 3450        |
|    policy_gradient_loss | 0.000466    |
|    value_loss           | 528         |
-----------------------------------------
Num timesteps: 8508000
Best mean reward: 4787.50 - Last mean reward per episode: 4677.21
Num timesteps: 8520000
Best mean reward: 4787.50 - Last mean reward per episode: 4640.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4635.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 347         |
|    time_elapsed         | 100836      |
|    total_timesteps      | 8527872     |
| train/                  |             |
|    approx_kl            | 0.015482637 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 379         |
|    n_updates            | 3460        |
|    policy_gradient_loss | 0.00234     |
|    value_loss           | 581         |
-----------------------------------------
Num timesteps: 8532000
Best mean reward: 4787.50 - Last mean reward per episode: 4595.86
Num timesteps: 8544000
Best mean reward: 4787.50 - Last mean reward per episode: 4604.71
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.34e+03   |
|    ep_rew_mean          | 4638.25    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 348        |
|    time_elapsed         | 101132     |
|    total_timesteps      | 8552448    |
| train/                  |            |
|    approx_kl            | 0.01672769 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.962      |
|    learning_rate        | 3e-06      |
|    loss                 | 154        |
|    n_updates            | 3470       |
|    policy_gradient_loss | 0.000894   |
|    value_loss           | 653        |
----------------------------------------
Num timesteps: 8556000
Best mean reward: 4787.50 - Last mean reward per episode: 4638.25
Num timesteps: 8568000
Best mean reward: 4787.50 - Last mean reward per episode: 4620.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4577.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 349         |
|    time_elapsed         | 101430      |
|    total_timesteps      | 8577024     |
| train/                  |             |
|    approx_kl            | 0.017380046 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 201         |
|    n_updates            | 3480        |
|    policy_gradient_loss | 0.000816    |
|    value_loss           | 509         |
-----------------------------------------
Num timesteps: 8580000
Best mean reward: 4787.50 - Last mean reward per episode: 4573.78
Num timesteps: 8592000
Best mean reward: 4787.50 - Last mean reward per episode: 4573.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4639.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 350         |
|    time_elapsed         | 101731      |
|    total_timesteps      | 8601600     |
| train/                  |             |
|    approx_kl            | 0.025694484 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 313         |
|    n_updates            | 3490        |
|    policy_gradient_loss | 0.000796    |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 8604000
Best mean reward: 4787.50 - Last mean reward per episode: 4639.12
Num timesteps: 8616000
Best mean reward: 4787.50 - Last mean reward per episode: 4597.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4599.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 351         |
|    time_elapsed         | 102029      |
|    total_timesteps      | 8626176     |
| train/                  |             |
|    approx_kl            | 0.018210018 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 170         |
|    n_updates            | 3500        |
|    policy_gradient_loss | 0.00245     |
|    value_loss           | 556         |
-----------------------------------------
Num timesteps: 8628000
Best mean reward: 4787.50 - Last mean reward per episode: 4599.82
Num timesteps: 8640000
Best mean reward: 4787.50 - Last mean reward per episode: 4527.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4551.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 352         |
|    time_elapsed         | 102320      |
|    total_timesteps      | 8650752     |
| train/                  |             |
|    approx_kl            | 0.015416746 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 175         |
|    n_updates            | 3510        |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 412         |
-----------------------------------------
Num timesteps: 8652000
Best mean reward: 4787.50 - Last mean reward per episode: 4551.05
Num timesteps: 8664000
Best mean reward: 4787.50 - Last mean reward per episode: 4531.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4592.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 353         |
|    time_elapsed         | 102618      |
|    total_timesteps      | 8675328     |
| train/                  |             |
|    approx_kl            | 0.018252105 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 3520        |
|    policy_gradient_loss | 0.000683    |
|    value_loss           | 550         |
-----------------------------------------
Num timesteps: 8676000
Best mean reward: 4787.50 - Last mean reward per episode: 4627.80
Num timesteps: 8688000
Best mean reward: 4787.50 - Last mean reward per episode: 4659.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4688.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 354         |
|    time_elapsed         | 102903      |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.024291666 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 84.9        |
|    n_updates            | 3530        |
|    policy_gradient_loss | 0.00409     |
|    value_loss           | 469         |
-----------------------------------------
Num timesteps: 8700000
Best mean reward: 4787.50 - Last mean reward per episode: 4688.61
Num timesteps: 8712000
Best mean reward: 4787.50 - Last mean reward per episode: 4735.81
Num timesteps: 8724000
Best mean reward: 4787.50 - Last mean reward per episode: 4704.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4704.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 355         |
|    time_elapsed         | 103190      |
|    total_timesteps      | 8724480     |
| train/                  |             |
|    approx_kl            | 0.014285233 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 235         |
|    n_updates            | 3540        |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 427         |
-----------------------------------------
Num timesteps: 8736000
Best mean reward: 4787.50 - Last mean reward per episode: 4632.88
Num timesteps: 8748000
Best mean reward: 4787.50 - Last mean reward per episode: 4673.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4673.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 356         |
|    time_elapsed         | 103479      |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.017399339 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 367         |
|    n_updates            | 3550        |
|    policy_gradient_loss | 0.00137     |
|    value_loss           | 488         |
-----------------------------------------
Num timesteps: 8760000
Best mean reward: 4787.50 - Last mean reward per episode: 4707.21
Num timesteps: 8772000
Best mean reward: 4787.50 - Last mean reward per episode: 4757.33
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.41e+03     |
|    ep_rew_mean          | 4757.33      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 357          |
|    time_elapsed         | 103769       |
|    total_timesteps      | 8773632      |
| train/                  |              |
|    approx_kl            | 0.0146535635 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.974        |
|    learning_rate        | 3e-06        |
|    loss                 | 193          |
|    n_updates            | 3560         |
|    policy_gradient_loss | 0.00232      |
|    value_loss           | 460          |
------------------------------------------
Num timesteps: 8784000
Best mean reward: 4787.50 - Last mean reward per episode: 4791.20
Saving new best model to tmp/best_model
Num timesteps: 8796000
Best mean reward: 4791.20 - Last mean reward per episode: 4792.21
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4803.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 358         |
|    time_elapsed         | 104061      |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.016088659 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 166         |
|    n_updates            | 3570        |
|    policy_gradient_loss | 0.000562    |
|    value_loss           | 443         |
-----------------------------------------
Num timesteps: 8808000
Best mean reward: 4792.21 - Last mean reward per episode: 4795.65
Saving new best model to tmp/best_model
Num timesteps: 8820000
Best mean reward: 4795.65 - Last mean reward per episode: 4814.60
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4828.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 359         |
|    time_elapsed         | 104350      |
|    total_timesteps      | 8822784     |
| train/                  |             |
|    approx_kl            | 0.014836547 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 86.5        |
|    n_updates            | 3580        |
|    policy_gradient_loss | 0.000787    |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 8832000
Best mean reward: 4814.60 - Last mean reward per episode: 4812.90
Num timesteps: 8844000
Best mean reward: 4814.60 - Last mean reward per episode: 4850.63
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 4800.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 360         |
|    time_elapsed         | 104651      |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.018412217 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 261         |
|    n_updates            | 3590        |
|    policy_gradient_loss | 0.000939    |
|    value_loss           | 591         |
-----------------------------------------
Num timesteps: 8856000
Best mean reward: 4850.63 - Last mean reward per episode: 4833.43
Num timesteps: 8868000
Best mean reward: 4850.63 - Last mean reward per episode: 4739.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 4740.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 361         |
|    time_elapsed         | 104949      |
|    total_timesteps      | 8871936     |
| train/                  |             |
|    approx_kl            | 0.019764438 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 385         |
|    n_updates            | 3600        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 582         |
-----------------------------------------
Num timesteps: 8880000
Best mean reward: 4850.63 - Last mean reward per episode: 4732.41
Num timesteps: 8892000
Best mean reward: 4850.63 - Last mean reward per episode: 4751.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 4742.07     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 362         |
|    time_elapsed         | 105242      |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.021084862 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 3610        |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 654         |
-----------------------------------------
Num timesteps: 8904000
Best mean reward: 4850.63 - Last mean reward per episode: 4705.97
Num timesteps: 8916000
Best mean reward: 4850.63 - Last mean reward per episode: 4749.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 4718.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 363         |
|    time_elapsed         | 105534      |
|    total_timesteps      | 8921088     |
| train/                  |             |
|    approx_kl            | 0.016004222 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 3620        |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 526         |
-----------------------------------------
Num timesteps: 8928000
Best mean reward: 4850.63 - Last mean reward per episode: 4715.61
Num timesteps: 8940000
Best mean reward: 4850.63 - Last mean reward per episode: 4644.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4646.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 364         |
|    time_elapsed         | 105826      |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.016519478 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 318         |
|    n_updates            | 3630        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 8952000
Best mean reward: 4850.63 - Last mean reward per episode: 4636.51
Num timesteps: 8964000
Best mean reward: 4850.63 - Last mean reward per episode: 4721.85
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 4753.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 365         |
|    time_elapsed         | 106114      |
|    total_timesteps      | 8970240     |
| train/                  |             |
|    approx_kl            | 0.022039449 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 322         |
|    n_updates            | 3640        |
|    policy_gradient_loss | 0.00415     |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 8976000
Best mean reward: 4850.63 - Last mean reward per episode: 4753.66
Num timesteps: 8988000
Best mean reward: 4850.63 - Last mean reward per episode: 4729.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4680.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 366         |
|    time_elapsed         | 106401      |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.017653199 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 450         |
|    n_updates            | 3650        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 637         |
-----------------------------------------
Num timesteps: 9000000
Best mean reward: 4850.63 - Last mean reward per episode: 4659.72
Num timesteps: 9012000
Best mean reward: 4850.63 - Last mean reward per episode: 4604.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 4604.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 367         |
|    time_elapsed         | 106683      |
|    total_timesteps      | 9019392     |
| train/                  |             |
|    approx_kl            | 0.016219793 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 357         |
|    n_updates            | 3660        |
|    policy_gradient_loss | 0.00431     |
|    value_loss           | 651         |
-----------------------------------------
Num timesteps: 9024000
Best mean reward: 4850.63 - Last mean reward per episode: 4594.71
Num timesteps: 9036000
Best mean reward: 4850.63 - Last mean reward per episode: 4553.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4487.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 368         |
|    time_elapsed         | 106966      |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.014665221 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 164         |
|    n_updates            | 3670        |
|    policy_gradient_loss | 0.00201     |
|    value_loss           | 523         |
-----------------------------------------
Num timesteps: 9048000
Best mean reward: 4850.63 - Last mean reward per episode: 4487.48
Num timesteps: 9060000
Best mean reward: 4850.63 - Last mean reward per episode: 4507.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.29e+03   |
|    ep_rew_mean          | 4490.77    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 369        |
|    time_elapsed         | 107250     |
|    total_timesteps      | 9068544    |
| train/                  |            |
|    approx_kl            | 0.01615119 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.969      |
|    learning_rate        | 3e-06      |
|    loss                 | 297        |
|    n_updates            | 3680       |
|    policy_gradient_loss | 0.00177    |
|    value_loss           | 487        |
----------------------------------------
Num timesteps: 9072000
Best mean reward: 4850.63 - Last mean reward per episode: 4528.88
Num timesteps: 9084000
Best mean reward: 4850.63 - Last mean reward per episode: 4515.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4505.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 370         |
|    time_elapsed         | 107538      |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.016052293 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.000208   |
|    value_loss           | 536         |
-----------------------------------------
Num timesteps: 9096000
Best mean reward: 4850.63 - Last mean reward per episode: 4540.31
Num timesteps: 9108000
Best mean reward: 4850.63 - Last mean reward per episode: 4624.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4617.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 371         |
|    time_elapsed         | 107826      |
|    total_timesteps      | 9117696     |
| train/                  |             |
|    approx_kl            | 0.015908325 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 239         |
|    n_updates            | 3700        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 9120000
Best mean reward: 4850.63 - Last mean reward per episode: 4617.18
Num timesteps: 9132000
Best mean reward: 4850.63 - Last mean reward per episode: 4594.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4636.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 372         |
|    time_elapsed         | 108120      |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.017252699 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 96.1        |
|    n_updates            | 3710        |
|    policy_gradient_loss | 0.00182     |
|    value_loss           | 599         |
-----------------------------------------
Num timesteps: 9144000
Best mean reward: 4850.63 - Last mean reward per episode: 4635.33
Num timesteps: 9156000
Best mean reward: 4850.63 - Last mean reward per episode: 4626.90
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.39e+03   |
|    ep_rew_mean          | 4676.33    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 373        |
|    time_elapsed         | 108405     |
|    total_timesteps      | 9166848    |
| train/                  |            |
|    approx_kl            | 0.01494218 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 93.3       |
|    n_updates            | 3720       |
|    policy_gradient_loss | 0.000706   |
|    value_loss           | 469        |
----------------------------------------
Num timesteps: 9168000
Best mean reward: 4850.63 - Last mean reward per episode: 4676.33
Num timesteps: 9180000
Best mean reward: 4850.63 - Last mean reward per episode: 4712.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4692.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 374         |
|    time_elapsed         | 108692      |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.012486294 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 128         |
|    n_updates            | 3730        |
|    policy_gradient_loss | 0.000315    |
|    value_loss           | 451         |
-----------------------------------------
Num timesteps: 9192000
Best mean reward: 4850.63 - Last mean reward per episode: 4700.64
Num timesteps: 9204000
Best mean reward: 4850.63 - Last mean reward per episode: 4651.53
Num timesteps: 9216000
Best mean reward: 4850.63 - Last mean reward per episode: 4613.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4613.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 375         |
|    time_elapsed         | 108976      |
|    total_timesteps      | 9216000     |
| train/                  |             |
|    approx_kl            | 0.017212784 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 352         |
|    n_updates            | 3740        |
|    policy_gradient_loss | 0.00244     |
|    value_loss           | 669         |
-----------------------------------------
Num timesteps: 9228000
Best mean reward: 4850.63 - Last mean reward per episode: 4630.14
Num timesteps: 9240000
Best mean reward: 4850.63 - Last mean reward per episode: 4657.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4657.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 376         |
|    time_elapsed         | 109260      |
|    total_timesteps      | 9240576     |
| train/                  |             |
|    approx_kl            | 0.017443703 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 471         |
|    n_updates            | 3750        |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 562         |
-----------------------------------------
Num timesteps: 9252000
Best mean reward: 4850.63 - Last mean reward per episode: 4632.65
Num timesteps: 9264000
Best mean reward: 4850.63 - Last mean reward per episode: 4642.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4642.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 377         |
|    time_elapsed         | 109549      |
|    total_timesteps      | 9265152     |
| train/                  |             |
|    approx_kl            | 0.016528236 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 316         |
|    n_updates            | 3760        |
|    policy_gradient_loss | 0.000592    |
|    value_loss           | 548         |
-----------------------------------------
Num timesteps: 9276000
Best mean reward: 4850.63 - Last mean reward per episode: 4690.30
Num timesteps: 9288000
Best mean reward: 4850.63 - Last mean reward per episode: 4702.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4672.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 378         |
|    time_elapsed         | 109832      |
|    total_timesteps      | 9289728     |
| train/                  |             |
|    approx_kl            | 0.017643178 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 265         |
|    n_updates            | 3770        |
|    policy_gradient_loss | 0.00205     |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 9300000
Best mean reward: 4850.63 - Last mean reward per episode: 4655.72
Num timesteps: 9312000
Best mean reward: 4850.63 - Last mean reward per episode: 4630.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4623.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 379         |
|    time_elapsed         | 110118      |
|    total_timesteps      | 9314304     |
| train/                  |             |
|    approx_kl            | 0.012455498 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 3780        |
|    policy_gradient_loss | -2.59e-05   |
|    value_loss           | 694         |
-----------------------------------------
Num timesteps: 9324000
Best mean reward: 4850.63 - Last mean reward per episode: 4647.12
Num timesteps: 9336000
Best mean reward: 4850.63 - Last mean reward per episode: 4635.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4641.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 380         |
|    time_elapsed         | 110401      |
|    total_timesteps      | 9338880     |
| train/                  |             |
|    approx_kl            | 0.017070675 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 200         |
|    n_updates            | 3790        |
|    policy_gradient_loss | 0.00243     |
|    value_loss           | 540         |
-----------------------------------------
Num timesteps: 9348000
Best mean reward: 4850.63 - Last mean reward per episode: 4693.85
Num timesteps: 9360000
Best mean reward: 4850.63 - Last mean reward per episode: 4732.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4732.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 381         |
|    time_elapsed         | 110685      |
|    total_timesteps      | 9363456     |
| train/                  |             |
|    approx_kl            | 0.016686564 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 167         |
|    n_updates            | 3800        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 517         |
-----------------------------------------
Num timesteps: 9372000
Best mean reward: 4850.63 - Last mean reward per episode: 4721.76
Num timesteps: 9384000
Best mean reward: 4850.63 - Last mean reward per episode: 4734.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4739.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 382         |
|    time_elapsed         | 110987      |
|    total_timesteps      | 9388032     |
| train/                  |             |
|    approx_kl            | 0.014761817 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 3810        |
|    policy_gradient_loss | 9.19e-05    |
|    value_loss           | 476         |
-----------------------------------------
Num timesteps: 9396000
Best mean reward: 4850.63 - Last mean reward per episode: 4721.05
Num timesteps: 9408000
Best mean reward: 4850.63 - Last mean reward per episode: 4729.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.35e+03   |
|    ep_rew_mean          | 4700.36    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 383        |
|    time_elapsed         | 111285     |
|    total_timesteps      | 9412608    |
| train/                  |            |
|    approx_kl            | 0.01428451 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 215        |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.00163   |
|    value_loss           | 386        |
----------------------------------------
Num timesteps: 9420000
Best mean reward: 4850.63 - Last mean reward per episode: 4744.90
Num timesteps: 9432000
Best mean reward: 4850.63 - Last mean reward per episode: 4793.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4795.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 384         |
|    time_elapsed         | 111580      |
|    total_timesteps      | 9437184     |
| train/                  |             |
|    approx_kl            | 0.016208904 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.000729   |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 9444000
Best mean reward: 4850.63 - Last mean reward per episode: 4817.63
Num timesteps: 9456000
Best mean reward: 4850.63 - Last mean reward per episode: 4875.08
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4846.47     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 385         |
|    time_elapsed         | 111880      |
|    total_timesteps      | 9461760     |
| train/                  |             |
|    approx_kl            | 0.016885906 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 3840        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 9468000
Best mean reward: 4875.08 - Last mean reward per episode: 4855.43
Num timesteps: 9480000
Best mean reward: 4875.08 - Last mean reward per episode: 4819.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 4856.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 386         |
|    time_elapsed         | 112179      |
|    total_timesteps      | 9486336     |
| train/                  |             |
|    approx_kl            | 0.016466284 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 123         |
|    n_updates            | 3850        |
|    policy_gradient_loss | 0.00331     |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 9492000
Best mean reward: 4875.08 - Last mean reward per episode: 4853.75
Num timesteps: 9504000
Best mean reward: 4875.08 - Last mean reward per episode: 4885.75
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 4887.77     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 387         |
|    time_elapsed         | 112479      |
|    total_timesteps      | 9510912     |
| train/                  |             |
|    approx_kl            | 0.015036948 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 181         |
|    n_updates            | 3860        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 645         |
-----------------------------------------
Num timesteps: 9516000
Best mean reward: 4885.75 - Last mean reward per episode: 4882.12
Num timesteps: 9528000
Best mean reward: 4885.75 - Last mean reward per episode: 4833.87
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.5e+03    |
|    ep_rew_mean          | 4836.17    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 388        |
|    time_elapsed         | 112770     |
|    total_timesteps      | 9535488    |
| train/                  |            |
|    approx_kl            | 0.01312726 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.922     |
|    explained_variance   | 0.964      |
|    learning_rate        | 3e-06      |
|    loss                 | 221        |
|    n_updates            | 3870       |
|    policy_gradient_loss | 0.000498   |
|    value_loss           | 497        |
----------------------------------------
Num timesteps: 9540000
Best mean reward: 4885.75 - Last mean reward per episode: 4883.44
Num timesteps: 9552000
Best mean reward: 4885.75 - Last mean reward per episode: 4859.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.54e+03    |
|    ep_rew_mean          | 4911.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 389         |
|    time_elapsed         | 113059      |
|    total_timesteps      | 9560064     |
| train/                  |             |
|    approx_kl            | 0.015871106 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 422         |
|    n_updates            | 3880        |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 717         |
-----------------------------------------
Num timesteps: 9564000
Best mean reward: 4885.75 - Last mean reward per episode: 4911.81
Saving new best model to tmp/best_model
Num timesteps: 9576000
Best mean reward: 4911.81 - Last mean reward per episode: 4924.44
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4896.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 390         |
|    time_elapsed         | 113353      |
|    total_timesteps      | 9584640     |
| train/                  |             |
|    approx_kl            | 0.014117355 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 361         |
|    n_updates            | 3890        |
|    policy_gradient_loss | 0.00111     |
|    value_loss           | 512         |
-----------------------------------------
Num timesteps: 9588000
Best mean reward: 4924.44 - Last mean reward per episode: 4896.66
Num timesteps: 9600000
Best mean reward: 4924.44 - Last mean reward per episode: 4825.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 4849.43     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 391         |
|    time_elapsed         | 113643      |
|    total_timesteps      | 9609216     |
| train/                  |             |
|    approx_kl            | 0.016866526 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 275         |
|    n_updates            | 3900        |
|    policy_gradient_loss | 8.77e-05    |
|    value_loss           | 649         |
-----------------------------------------
Num timesteps: 9612000
Best mean reward: 4924.44 - Last mean reward per episode: 4844.48
Num timesteps: 9624000
Best mean reward: 4924.44 - Last mean reward per episode: 4899.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 4878.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 392         |
|    time_elapsed         | 113937      |
|    total_timesteps      | 9633792     |
| train/                  |             |
|    approx_kl            | 0.013361365 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 343         |
|    n_updates            | 3910        |
|    policy_gradient_loss | 0.000467    |
|    value_loss           | 434         |
-----------------------------------------
Num timesteps: 9636000
Best mean reward: 4924.44 - Last mean reward per episode: 4880.57
Num timesteps: 9648000
Best mean reward: 4924.44 - Last mean reward per episode: 4837.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 4825.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 393         |
|    time_elapsed         | 114227      |
|    total_timesteps      | 9658368     |
| train/                  |             |
|    approx_kl            | 0.013870659 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 199         |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.000197   |
|    value_loss           | 422         |
-----------------------------------------
Num timesteps: 9660000
Best mean reward: 4924.44 - Last mean reward per episode: 4792.25
Num timesteps: 9672000
Best mean reward: 4924.44 - Last mean reward per episode: 4777.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.5e+03    |
|    ep_rew_mean          | 4772.34    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 394        |
|    time_elapsed         | 114520     |
|    total_timesteps      | 9682944    |
| train/                  |            |
|    approx_kl            | 0.02056151 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.957      |
|    learning_rate        | 3e-06      |
|    loss                 | 396        |
|    n_updates            | 3930       |
|    policy_gradient_loss | 0.00325    |
|    value_loss           | 667        |
----------------------------------------
Num timesteps: 9684000
Best mean reward: 4924.44 - Last mean reward per episode: 4772.34
Num timesteps: 9696000
Best mean reward: 4924.44 - Last mean reward per episode: 4770.18
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.49e+03   |
|    ep_rew_mean          | 4753.78    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 395        |
|    time_elapsed         | 114810     |
|    total_timesteps      | 9707520    |
| train/                  |            |
|    approx_kl            | 0.02391847 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.964      |
|    learning_rate        | 3e-06      |
|    loss                 | 225        |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.00258   |
|    value_loss           | 476        |
----------------------------------------
Num timesteps: 9708000
Best mean reward: 4924.44 - Last mean reward per episode: 4753.78
Num timesteps: 9720000
Best mean reward: 4924.44 - Last mean reward per episode: 4783.93
Num timesteps: 9732000
Best mean reward: 4924.44 - Last mean reward per episode: 4825.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 4825.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 396         |
|    time_elapsed         | 115098      |
|    total_timesteps      | 9732096     |
| train/                  |             |
|    approx_kl            | 0.023928516 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 199         |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.000345   |
|    value_loss           | 333         |
-----------------------------------------
Num timesteps: 9744000
Best mean reward: 4924.44 - Last mean reward per episode: 4877.35
Num timesteps: 9756000
Best mean reward: 4924.44 - Last mean reward per episode: 4901.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4901.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 397         |
|    time_elapsed         | 115386      |
|    total_timesteps      | 9756672     |
| train/                  |             |
|    approx_kl            | 0.013381432 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 225         |
|    n_updates            | 3960        |
|    policy_gradient_loss | 0.000779    |
|    value_loss           | 432         |
-----------------------------------------
Num timesteps: 9768000
Best mean reward: 4924.44 - Last mean reward per episode: 4947.77
Saving new best model to tmp/best_model
Num timesteps: 9780000
Best mean reward: 4947.77 - Last mean reward per episode: 4950.51
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.59e+03    |
|    ep_rew_mean          | 4960.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 398         |
|    time_elapsed         | 115679      |
|    total_timesteps      | 9781248     |
| train/                  |             |
|    approx_kl            | 0.009507871 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 262         |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 348         |
-----------------------------------------
Num timesteps: 9792000
Best mean reward: 4950.51 - Last mean reward per episode: 5028.14
Saving new best model to tmp/best_model
Num timesteps: 9804000
Best mean reward: 5028.14 - Last mean reward per episode: 5027.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61e+03    |
|    ep_rew_mean          | 5018.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 399         |
|    time_elapsed         | 115975      |
|    total_timesteps      | 9805824     |
| train/                  |             |
|    approx_kl            | 0.011519375 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 85.2        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.000739   |
|    value_loss           | 364         |
-----------------------------------------
Num timesteps: 9816000
Best mean reward: 5028.14 - Last mean reward per episode: 5033.59
Saving new best model to tmp/best_model
Num timesteps: 9828000
Best mean reward: 5033.59 - Last mean reward per episode: 5017.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61e+03    |
|    ep_rew_mean          | 5017.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 400         |
|    time_elapsed         | 116262      |
|    total_timesteps      | 9830400     |
| train/                  |             |
|    approx_kl            | 0.016391821 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 123         |
|    n_updates            | 3990        |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 539         |
-----------------------------------------
Num timesteps: 9840000
Best mean reward: 5033.59 - Last mean reward per episode: 5005.75
Num timesteps: 9852000
Best mean reward: 5033.59 - Last mean reward per episode: 5058.16
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64e+03     |
|    ep_rew_mean          | 5058.16      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 401          |
|    time_elapsed         | 116550       |
|    total_timesteps      | 9854976      |
| train/                  |              |
|    approx_kl            | 0.0131799085 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.907       |
|    explained_variance   | 0.97         |
|    learning_rate        | 3e-06        |
|    loss                 | 183          |
|    n_updates            | 4000         |
|    policy_gradient_loss | 0.000916     |
|    value_loss           | 474          |
------------------------------------------
Num timesteps: 9864000
Best mean reward: 5058.16 - Last mean reward per episode: 4996.81
Num timesteps: 9876000
Best mean reward: 5058.16 - Last mean reward per episode: 4926.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.58e+03   |
|    ep_rew_mean          | 4921.3     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 402        |
|    time_elapsed         | 116837     |
|    total_timesteps      | 9879552    |
| train/                  |            |
|    approx_kl            | 0.01647196 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | 0.963      |
|    learning_rate        | 3e-06      |
|    loss                 | 237        |
|    n_updates            | 4010       |
|    policy_gradient_loss | -0.000866  |
|    value_loss           | 579        |
----------------------------------------
Num timesteps: 9888000
Best mean reward: 5058.16 - Last mean reward per episode: 4870.12
Num timesteps: 9900000
Best mean reward: 5058.16 - Last mean reward per episode: 4860.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.55e+03     |
|    ep_rew_mean          | 4852.03      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 403          |
|    time_elapsed         | 117128       |
|    total_timesteps      | 9904128      |
| train/                  |              |
|    approx_kl            | 0.0151804015 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.967       |
|    explained_variance   | 0.967        |
|    learning_rate        | 3e-06        |
|    loss                 | 448          |
|    n_updates            | 4020         |
|    policy_gradient_loss | 1.72e-05     |
|    value_loss           | 531          |
------------------------------------------
Num timesteps: 9912000
Best mean reward: 5058.16 - Last mean reward per episode: 4861.59
Num timesteps: 9924000
Best mean reward: 5058.16 - Last mean reward per episode: 4891.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.57e+03   |
|    ep_rew_mean          | 4894.65    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 404        |
|    time_elapsed         | 117416     |
|    total_timesteps      | 9928704    |
| train/                  |            |
|    approx_kl            | 0.01644642 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.901     |
|    explained_variance   | 0.967      |
|    learning_rate        | 3e-06      |
|    loss                 | 429        |
|    n_updates            | 4030       |
|    policy_gradient_loss | 0.00232    |
|    value_loss           | 579        |
----------------------------------------
Num timesteps: 9936000
Best mean reward: 5058.16 - Last mean reward per episode: 4854.68
Num timesteps: 9948000
Best mean reward: 5058.16 - Last mean reward per episode: 4845.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.55e+03     |
|    ep_rew_mean          | 4850.43      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 405          |
|    time_elapsed         | 117702       |
|    total_timesteps      | 9953280      |
| train/                  |              |
|    approx_kl            | 0.0142869055 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.909       |
|    explained_variance   | 0.964        |
|    learning_rate        | 3e-06        |
|    loss                 | 138          |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.000517    |
|    value_loss           | 606          |
------------------------------------------
Num timesteps: 9960000
Best mean reward: 5058.16 - Last mean reward per episode: 4821.20
Num timesteps: 9972000
Best mean reward: 5058.16 - Last mean reward per episode: 4803.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 4771.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 406         |
|    time_elapsed         | 117992      |
|    total_timesteps      | 9977856     |
| train/                  |             |
|    approx_kl            | 0.016683226 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 256         |
|    n_updates            | 4050        |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 414         |
-----------------------------------------
Num timesteps: 9984000
Best mean reward: 5058.16 - Last mean reward per episode: 4780.35
Num timesteps: 9996000
Best mean reward: 5058.16 - Last mean reward per episode: 4686.17
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.44e+03   |
|    ep_rew_mean          | 4687.46    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 407        |
|    time_elapsed         | 118278     |
|    total_timesteps      | 10002432   |
| train/                  |            |
|    approx_kl            | 0.01780815 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.913     |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 140        |
|    n_updates            | 4060       |
|    policy_gradient_loss | 0.00199    |
|    value_loss           | 469        |
----------------------------------------
Num timesteps: 10008000
Best mean reward: 5058.16 - Last mean reward per episode: 4674.65
Num timesteps: 10020000
Best mean reward: 5058.16 - Last mean reward per episode: 4677.03
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.38e+03   |
|    ep_rew_mean          | 4667.36    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 408        |
|    time_elapsed         | 118563     |
|    total_timesteps      | 10027008   |
| train/                  |            |
|    approx_kl            | 0.02055479 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.909     |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 235        |
|    n_updates            | 4070       |
|    policy_gradient_loss | -0.00162   |
|    value_loss           | 490        |
----------------------------------------
Num timesteps: 10032000
Best mean reward: 5058.16 - Last mean reward per episode: 4591.82
Num timesteps: 10044000
Best mean reward: 5058.16 - Last mean reward per episode: 4629.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4647.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 409         |
|    time_elapsed         | 118847      |
|    total_timesteps      | 10051584    |
| train/                  |             |
|    approx_kl            | 0.019193126 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.954      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 895         |
|    n_updates            | 4080        |
|    policy_gradient_loss | 0.00326     |
|    value_loss           | 515         |
-----------------------------------------
Num timesteps: 10056000
Best mean reward: 5058.16 - Last mean reward per episode: 4661.36
Num timesteps: 10068000
Best mean reward: 5058.16 - Last mean reward per episode: 4663.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4681.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 410         |
|    time_elapsed         | 119133      |
|    total_timesteps      | 10076160    |
| train/                  |             |
|    approx_kl            | 0.014961027 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 470         |
-----------------------------------------
Num timesteps: 10080000
Best mean reward: 5058.16 - Last mean reward per episode: 4688.50
Num timesteps: 10092000
Best mean reward: 5058.16 - Last mean reward per episode: 4616.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4705.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 411         |
|    time_elapsed         | 119423      |
|    total_timesteps      | 10100736    |
| train/                  |             |
|    approx_kl            | 0.011526086 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 230         |
|    n_updates            | 4100        |
|    policy_gradient_loss | 0.000654    |
|    value_loss           | 490         |
-----------------------------------------
Num timesteps: 10104000
Best mean reward: 5058.16 - Last mean reward per episode: 4696.05
Num timesteps: 10116000
Best mean reward: 5058.16 - Last mean reward per episode: 4673.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4698.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 412         |
|    time_elapsed         | 119718      |
|    total_timesteps      | 10125312    |
| train/                  |             |
|    approx_kl            | 0.015730558 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 517         |
|    n_updates            | 4110        |
|    policy_gradient_loss | 0.00254     |
|    value_loss           | 629         |
-----------------------------------------
Num timesteps: 10128000
Best mean reward: 5058.16 - Last mean reward per episode: 4711.58
Num timesteps: 10140000
Best mean reward: 5058.16 - Last mean reward per episode: 4719.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4738.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 413         |
|    time_elapsed         | 120005      |
|    total_timesteps      | 10149888    |
| train/                  |             |
|    approx_kl            | 0.014673295 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 932         |
|    n_updates            | 4120        |
|    policy_gradient_loss | 0.000552    |
|    value_loss           | 720         |
-----------------------------------------
Num timesteps: 10152000
Best mean reward: 5058.16 - Last mean reward per episode: 4735.28
Num timesteps: 10164000
Best mean reward: 5058.16 - Last mean reward per episode: 4756.57
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.37e+03   |
|    ep_rew_mean          | 4781.95    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 414        |
|    time_elapsed         | 120293     |
|    total_timesteps      | 10174464   |
| train/                  |            |
|    approx_kl            | 0.01721459 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.902     |
|    explained_variance   | 0.972      |
|    learning_rate        | 3e-06      |
|    loss                 | 136        |
|    n_updates            | 4130       |
|    policy_gradient_loss | 0.00155    |
|    value_loss           | 514        |
----------------------------------------
Num timesteps: 10176000
Best mean reward: 5058.16 - Last mean reward per episode: 4781.95
Num timesteps: 10188000
Best mean reward: 5058.16 - Last mean reward per episode: 4738.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4719.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 415         |
|    time_elapsed         | 120583      |
|    total_timesteps      | 10199040    |
| train/                  |             |
|    approx_kl            | 0.015430274 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 4140        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 10200000
Best mean reward: 5058.16 - Last mean reward per episode: 4719.14
Num timesteps: 10212000
Best mean reward: 5058.16 - Last mean reward per episode: 4764.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4757.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 416         |
|    time_elapsed         | 120870      |
|    total_timesteps      | 10223616    |
| train/                  |             |
|    approx_kl            | 0.016870992 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 255         |
|    n_updates            | 4150        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 600         |
-----------------------------------------
Num timesteps: 10224000
Best mean reward: 5058.16 - Last mean reward per episode: 4757.66
Num timesteps: 10236000
Best mean reward: 5058.16 - Last mean reward per episode: 4821.15
Num timesteps: 10248000
Best mean reward: 5058.16 - Last mean reward per episode: 4816.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4816.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 417         |
|    time_elapsed         | 121158      |
|    total_timesteps      | 10248192    |
| train/                  |             |
|    approx_kl            | 0.015586823 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 131         |
|    n_updates            | 4160        |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 10260000
Best mean reward: 5058.16 - Last mean reward per episode: 4818.96
Num timesteps: 10272000
Best mean reward: 5058.16 - Last mean reward per episode: 4844.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4844.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 418         |
|    time_elapsed         | 121444      |
|    total_timesteps      | 10272768    |
| train/                  |             |
|    approx_kl            | 0.011793081 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.876      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 4170        |
|    policy_gradient_loss | 0.000275    |
|    value_loss           | 409         |
-----------------------------------------
Num timesteps: 10284000
Best mean reward: 5058.16 - Last mean reward per episode: 4805.66
Num timesteps: 10296000
Best mean reward: 5058.16 - Last mean reward per episode: 4779.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4779.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 419         |
|    time_elapsed         | 121733      |
|    total_timesteps      | 10297344    |
| train/                  |             |
|    approx_kl            | 0.012879561 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 251         |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.000151   |
|    value_loss           | 484         |
-----------------------------------------
Num timesteps: 10308000
Best mean reward: 5058.16 - Last mean reward per episode: 4714.12
Num timesteps: 10320000
Best mean reward: 5058.16 - Last mean reward per episode: 4726.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.35e+03     |
|    ep_rew_mean          | 4726.09      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 420          |
|    time_elapsed         | 122025       |
|    total_timesteps      | 10321920     |
| train/                  |              |
|    approx_kl            | 0.0143312095 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.934       |
|    explained_variance   | 0.964        |
|    learning_rate        | 3e-06        |
|    loss                 | 313          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -6.42e-05    |
|    value_loss           | 594          |
------------------------------------------
Num timesteps: 10332000
Best mean reward: 5058.16 - Last mean reward per episode: 4774.22
Num timesteps: 10344000
Best mean reward: 5058.16 - Last mean reward per episode: 4813.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4812.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 421         |
|    time_elapsed         | 122317      |
|    total_timesteps      | 10346496    |
| train/                  |             |
|    approx_kl            | 0.015520665 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 246         |
|    n_updates            | 4200        |
|    policy_gradient_loss | 0.000775    |
|    value_loss           | 581         |
-----------------------------------------
Num timesteps: 10356000
Best mean reward: 5058.16 - Last mean reward per episode: 4826.35
Num timesteps: 10368000
Best mean reward: 5058.16 - Last mean reward per episode: 4871.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4870.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 422         |
|    time_elapsed         | 122607      |
|    total_timesteps      | 10371072    |
| train/                  |             |
|    approx_kl            | 0.013750821 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 132         |
|    n_updates            | 4210        |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 472         |
-----------------------------------------
Num timesteps: 10380000
Best mean reward: 5058.16 - Last mean reward per episode: 4859.17
Num timesteps: 10392000
Best mean reward: 5058.16 - Last mean reward per episode: 4867.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4869.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 423         |
|    time_elapsed         | 122890      |
|    total_timesteps      | 10395648    |
| train/                  |             |
|    approx_kl            | 0.016512923 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.917      |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 344         |
|    n_updates            | 4220        |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 540         |
-----------------------------------------
Num timesteps: 10404000
Best mean reward: 5058.16 - Last mean reward per episode: 4872.84
Num timesteps: 10416000
Best mean reward: 5058.16 - Last mean reward per episode: 4877.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 4877.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 424         |
|    time_elapsed         | 123182      |
|    total_timesteps      | 10420224    |
| train/                  |             |
|    approx_kl            | 0.024077944 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 111         |
|    n_updates            | 4230        |
|    policy_gradient_loss | 0.00595     |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 10428000
Best mean reward: 5058.16 - Last mean reward per episode: 4838.60
Num timesteps: 10440000
Best mean reward: 5058.16 - Last mean reward per episode: 4805.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.55e+03    |
|    ep_rew_mean          | 4809.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 425         |
|    time_elapsed         | 123474      |
|    total_timesteps      | 10444800    |
| train/                  |             |
|    approx_kl            | 0.015355646 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 59          |
|    n_updates            | 4240        |
|    policy_gradient_loss | 0.000218    |
|    value_loss           | 212         |
-----------------------------------------
Num timesteps: 10452000
Best mean reward: 5058.16 - Last mean reward per episode: 4790.46
Num timesteps: 10464000
Best mean reward: 5058.16 - Last mean reward per episode: 4748.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.62e+03    |
|    ep_rew_mean          | 4748.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 426         |
|    time_elapsed         | 123766      |
|    total_timesteps      | 10469376    |
| train/                  |             |
|    approx_kl            | 0.019183235 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 75.8        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.000334   |
|    value_loss           | 285         |
-----------------------------------------
Num timesteps: 10476000
Best mean reward: 5058.16 - Last mean reward per episode: 4784.29
Num timesteps: 10488000
Best mean reward: 5058.16 - Last mean reward per episode: 4792.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.68e+03    |
|    ep_rew_mean          | 4759.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 427         |
|    time_elapsed         | 124057      |
|    total_timesteps      | 10493952    |
| train/                  |             |
|    approx_kl            | 0.018968737 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 50          |
|    n_updates            | 4260        |
|    policy_gradient_loss | 0.0018      |
|    value_loss           | 267         |
-----------------------------------------
Num timesteps: 10500000
Best mean reward: 5058.16 - Last mean reward per episode: 4759.90
Num timesteps: 10512000
Best mean reward: 5058.16 - Last mean reward per episode: 4760.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.68e+03   |
|    ep_rew_mean          | 4763.09    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 428        |
|    time_elapsed         | 124345     |
|    total_timesteps      | 10518528   |
| train/                  |            |
|    approx_kl            | 0.01800365 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.977      |
|    learning_rate        | 3e-06      |
|    loss                 | 102        |
|    n_updates            | 4270       |
|    policy_gradient_loss | 0.00104    |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 10524000
Best mean reward: 5058.16 - Last mean reward per episode: 4753.93
Num timesteps: 10536000
Best mean reward: 5058.16 - Last mean reward per episode: 4720.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8e+03     |
|    ep_rew_mean          | 4720.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 429         |
|    time_elapsed         | 124633      |
|    total_timesteps      | 10543104    |
| train/                  |             |
|    approx_kl            | 0.014839974 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 95.4        |
|    n_updates            | 4280        |
|    policy_gradient_loss | 3.89e-05    |
|    value_loss           | 221         |
-----------------------------------------
Num timesteps: 10548000
Best mean reward: 5058.16 - Last mean reward per episode: 4725.35
Num timesteps: 10560000
Best mean reward: 5058.16 - Last mean reward per episode: 4661.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.96e+03    |
|    ep_rew_mean          | 4659.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 430         |
|    time_elapsed         | 124924      |
|    total_timesteps      | 10567680    |
| train/                  |             |
|    approx_kl            | 0.012819548 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 195         |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 243         |
-----------------------------------------
Num timesteps: 10572000
Best mean reward: 5058.16 - Last mean reward per episode: 4671.49
Num timesteps: 10584000
Best mean reward: 5058.16 - Last mean reward per episode: 4667.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.98e+03   |
|    ep_rew_mean          | 4732.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 431        |
|    time_elapsed         | 125212     |
|    total_timesteps      | 10592256   |
| train/                  |            |
|    approx_kl            | 0.02414453 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.999     |
|    explained_variance   | 0.97       |
|    learning_rate        | 3e-06      |
|    loss                 | 264        |
|    n_updates            | 4300       |
|    policy_gradient_loss | 0.00262    |
|    value_loss           | 478        |
----------------------------------------
Num timesteps: 10596000
Best mean reward: 5058.16 - Last mean reward per episode: 4703.68
Num timesteps: 10608000
Best mean reward: 5058.16 - Last mean reward per episode: 4756.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.99e+03    |
|    ep_rew_mean          | 4752.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 432         |
|    time_elapsed         | 125499      |
|    total_timesteps      | 10616832    |
| train/                  |             |
|    approx_kl            | 0.014044632 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 214         |
|    n_updates            | 4310        |
|    policy_gradient_loss | 0.00334     |
|    value_loss           | 525         |
-----------------------------------------
Num timesteps: 10620000
Best mean reward: 5058.16 - Last mean reward per episode: 4751.77
Num timesteps: 10632000
Best mean reward: 5058.16 - Last mean reward per episode: 4728.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.98e+03   |
|    ep_rew_mean          | 4751.95    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 433        |
|    time_elapsed         | 125790     |
|    total_timesteps      | 10641408   |
| train/                  |            |
|    approx_kl            | 0.01272821 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.971      |
|    learning_rate        | 3e-06      |
|    loss                 | 212        |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.000325  |
|    value_loss           | 464        |
----------------------------------------
Num timesteps: 10644000
Best mean reward: 5058.16 - Last mean reward per episode: 4751.95
Num timesteps: 10656000
Best mean reward: 5058.16 - Last mean reward per episode: 4786.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.99e+03    |
|    ep_rew_mean          | 4784.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 434         |
|    time_elapsed         | 126079      |
|    total_timesteps      | 10665984    |
| train/                  |             |
|    approx_kl            | 0.014256892 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.838      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 173         |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.000915   |
|    value_loss           | 429         |
-----------------------------------------
Num timesteps: 10668000
Best mean reward: 5058.16 - Last mean reward per episode: 4786.85
Num timesteps: 10680000
Best mean reward: 5058.16 - Last mean reward per episode: 4817.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | 4798.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 435         |
|    time_elapsed         | 126371      |
|    total_timesteps      | 10690560    |
| train/                  |             |
|    approx_kl            | 0.022330945 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 4340        |
|    policy_gradient_loss | 0.00488     |
|    value_loss           | 565         |
-----------------------------------------
Num timesteps: 10692000
Best mean reward: 5058.16 - Last mean reward per episode: 4796.65
Num timesteps: 10704000
Best mean reward: 5058.16 - Last mean reward per episode: 4731.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.85e+03    |
|    ep_rew_mean          | 4790.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 436         |
|    time_elapsed         | 126656      |
|    total_timesteps      | 10715136    |
| train/                  |             |
|    approx_kl            | 0.014766551 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 411         |
|    n_updates            | 4350        |
|    policy_gradient_loss | -6.37e-05   |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 10716000
Best mean reward: 5058.16 - Last mean reward per episode: 4790.24
Num timesteps: 10728000
Best mean reward: 5058.16 - Last mean reward per episode: 4741.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.81e+03    |
|    ep_rew_mean          | 4813.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 437         |
|    time_elapsed         | 126947      |
|    total_timesteps      | 10739712    |
| train/                  |             |
|    approx_kl            | 0.015369288 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 277         |
|    n_updates            | 4360        |
|    policy_gradient_loss | 0.00149     |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 10740000
Best mean reward: 5058.16 - Last mean reward per episode: 4813.44
Num timesteps: 10752000
Best mean reward: 5058.16 - Last mean reward per episode: 4835.25
Num timesteps: 10764000
Best mean reward: 5058.16 - Last mean reward per episode: 4815.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.77e+03    |
|    ep_rew_mean          | 4815.92     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 438         |
|    time_elapsed         | 127230      |
|    total_timesteps      | 10764288    |
| train/                  |             |
|    approx_kl            | 0.015423421 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.775      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 161         |
|    n_updates            | 4370        |
|    policy_gradient_loss | 0.000327    |
|    value_loss           | 436         |
-----------------------------------------
Num timesteps: 10776000
Best mean reward: 5058.16 - Last mean reward per episode: 4836.16
Num timesteps: 10788000
Best mean reward: 5058.16 - Last mean reward per episode: 4867.16
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.66e+03   |
|    ep_rew_mean          | 4872.92    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 439        |
|    time_elapsed         | 127515     |
|    total_timesteps      | 10788864   |
| train/                  |            |
|    approx_kl            | 0.01787819 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0.963      |
|    learning_rate        | 3e-06      |
|    loss                 | 109        |
|    n_updates            | 4380       |
|    policy_gradient_loss | 0.00246    |
|    value_loss           | 521        |
----------------------------------------
Num timesteps: 10800000
Best mean reward: 5058.16 - Last mean reward per episode: 4874.32
Num timesteps: 10812000
Best mean reward: 5058.16 - Last mean reward per episode: 4952.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.55e+03    |
|    ep_rew_mean          | 4952.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 440         |
|    time_elapsed         | 127806      |
|    total_timesteps      | 10813440    |
| train/                  |             |
|    approx_kl            | 0.010889057 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 153         |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.00049    |
|    value_loss           | 414         |
-----------------------------------------
Num timesteps: 10824000
Best mean reward: 5058.16 - Last mean reward per episode: 5033.90
Num timesteps: 10836000
Best mean reward: 5058.16 - Last mean reward per episode: 5052.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.55e+03    |
|    ep_rew_mean          | 5052.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 441         |
|    time_elapsed         | 128096      |
|    total_timesteps      | 10838016    |
| train/                  |             |
|    approx_kl            | 0.010435338 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.000501   |
|    value_loss           | 341         |
-----------------------------------------
Num timesteps: 10848000
Best mean reward: 5058.16 - Last mean reward per episode: 5062.76
Saving new best model to tmp/best_model
Num timesteps: 10860000
Best mean reward: 5062.76 - Last mean reward per episode: 5069.86
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.54e+03   |
|    ep_rew_mean          | 5052.6     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 442        |
|    time_elapsed         | 128383     |
|    total_timesteps      | 10862592   |
| train/                  |            |
|    approx_kl            | 0.01112664 |
|    clip_fraction        | 0.0916     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.981      |
|    learning_rate        | 3e-06      |
|    loss                 | 219        |
|    n_updates            | 4410       |
|    policy_gradient_loss | -0.000329  |
|    value_loss           | 337        |
----------------------------------------
Num timesteps: 10872000
Best mean reward: 5069.86 - Last mean reward per episode: 5049.11
Num timesteps: 10884000
Best mean reward: 5069.86 - Last mean reward per episode: 5045.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5078.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 443         |
|    time_elapsed         | 128663      |
|    total_timesteps      | 10887168    |
| train/                  |             |
|    approx_kl            | 0.013622894 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 565         |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00082    |
|    value_loss           | 502         |
-----------------------------------------
Num timesteps: 10896000
Best mean reward: 5069.86 - Last mean reward per episode: 5097.83
Saving new best model to tmp/best_model
Num timesteps: 10908000
Best mean reward: 5097.83 - Last mean reward per episode: 5103.16
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.57e+03    |
|    ep_rew_mean          | 5103.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 444         |
|    time_elapsed         | 128955      |
|    total_timesteps      | 10911744    |
| train/                  |             |
|    approx_kl            | 0.012884024 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 4430        |
|    policy_gradient_loss | 0.00204     |
|    value_loss           | 436         |
-----------------------------------------
Num timesteps: 10920000
Best mean reward: 5103.16 - Last mean reward per episode: 5150.19
Saving new best model to tmp/best_model
Num timesteps: 10932000
Best mean reward: 5150.19 - Last mean reward per episode: 5143.92
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.57e+03   |
|    ep_rew_mean          | 5115.28    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 445        |
|    time_elapsed         | 129249     |
|    total_timesteps      | 10936320   |
| train/                  |            |
|    approx_kl            | 0.01278673 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.759     |
|    explained_variance   | 0.977      |
|    learning_rate        | 3e-06      |
|    loss                 | 153        |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.00164   |
|    value_loss           | 367        |
----------------------------------------
Num timesteps: 10944000
Best mean reward: 5150.19 - Last mean reward per episode: 5118.16
Num timesteps: 10956000
Best mean reward: 5150.19 - Last mean reward per episode: 5194.59
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.6e+03    |
|    ep_rew_mean          | 5194.59    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 446        |
|    time_elapsed         | 129537     |
|    total_timesteps      | 10960896   |
| train/                  |            |
|    approx_kl            | 0.01947012 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.781     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 181        |
|    n_updates            | 4450       |
|    policy_gradient_loss | -0.00201   |
|    value_loss           | 383        |
----------------------------------------
Num timesteps: 10968000
Best mean reward: 5194.59 - Last mean reward per episode: 5225.77
Saving new best model to tmp/best_model
Num timesteps: 10980000
Best mean reward: 5225.77 - Last mean reward per episode: 5240.37
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.63e+03   |
|    ep_rew_mean          | 5254.96    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 447        |
|    time_elapsed         | 129831     |
|    total_timesteps      | 10985472   |
| train/                  |            |
|    approx_kl            | 0.02438291 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.971      |
|    learning_rate        | 3e-06      |
|    loss                 | 169        |
|    n_updates            | 4460       |
|    policy_gradient_loss | -7.29e-05  |
|    value_loss           | 389        |
----------------------------------------
Num timesteps: 10992000
Best mean reward: 5240.37 - Last mean reward per episode: 5259.66
Saving new best model to tmp/best_model
Num timesteps: 11004000
Best mean reward: 5259.66 - Last mean reward per episode: 5299.16
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.63e+03    |
|    ep_rew_mean          | 5370.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 448         |
|    time_elapsed         | 130134      |
|    total_timesteps      | 11010048    |
| train/                  |             |
|    approx_kl            | 0.017738251 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 138         |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.000349   |
|    value_loss           | 218         |
-----------------------------------------
Num timesteps: 11016000
Best mean reward: 5299.16 - Last mean reward per episode: 5370.56
Saving new best model to tmp/best_model
Num timesteps: 11028000
Best mean reward: 5370.56 - Last mean reward per episode: 5379.41
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.62e+03     |
|    ep_rew_mean          | 5376.05      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 449          |
|    time_elapsed         | 130429       |
|    total_timesteps      | 11034624     |
| train/                  |              |
|    approx_kl            | 0.0140895955 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.02        |
|    explained_variance   | 0.979        |
|    learning_rate        | 3e-06        |
|    loss                 | 179          |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.000658    |
|    value_loss           | 191          |
------------------------------------------
Num timesteps: 11040000
Best mean reward: 5379.41 - Last mean reward per episode: 5365.38
Num timesteps: 11052000
Best mean reward: 5379.41 - Last mean reward per episode: 5369.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.69e+03    |
|    ep_rew_mean          | 5327.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 450         |
|    time_elapsed         | 130716      |
|    total_timesteps      | 11059200    |
| train/                  |             |
|    approx_kl            | 0.016972914 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 189         |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.000783   |
|    value_loss           | 224         |
-----------------------------------------
Num timesteps: 11064000
Best mean reward: 5379.41 - Last mean reward per episode: 5327.68
Num timesteps: 11076000
Best mean reward: 5379.41 - Last mean reward per episode: 5326.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.06e+03    |
|    ep_rew_mean          | 5187.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 451         |
|    time_elapsed         | 131005      |
|    total_timesteps      | 11083776    |
| train/                  |             |
|    approx_kl            | 0.013355025 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 55.8        |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.000776   |
|    value_loss           | 153         |
-----------------------------------------
Num timesteps: 11088000
Best mean reward: 5379.41 - Last mean reward per episode: 5187.25
Num timesteps: 11100000
Best mean reward: 5379.41 - Last mean reward per episode: 5162.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.14e+03    |
|    ep_rew_mean          | 5154.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 452         |
|    time_elapsed         | 131292      |
|    total_timesteps      | 11108352    |
| train/                  |             |
|    approx_kl            | 0.015244938 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.99        |
|    learning_rate        | 3e-06       |
|    loss                 | 31.3        |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 112         |
-----------------------------------------
Num timesteps: 11112000
Best mean reward: 5379.41 - Last mean reward per episode: 5157.24
Num timesteps: 11124000
Best mean reward: 5379.41 - Last mean reward per episode: 5171.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.13e+03    |
|    ep_rew_mean          | 5165.85     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 453         |
|    time_elapsed         | 131581      |
|    total_timesteps      | 11132928    |
| train/                  |             |
|    approx_kl            | 0.013473619 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 91          |
|    n_updates            | 4520        |
|    policy_gradient_loss | 0.000214    |
|    value_loss           | 300         |
-----------------------------------------
Num timesteps: 11136000
Best mean reward: 5379.41 - Last mean reward per episode: 5162.74
Num timesteps: 11148000
Best mean reward: 5379.41 - Last mean reward per episode: 5151.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.2e+03     |
|    ep_rew_mean          | 5116.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 454         |
|    time_elapsed         | 131867      |
|    total_timesteps      | 11157504    |
| train/                  |             |
|    approx_kl            | 0.012853734 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 94.8        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.000921   |
|    value_loss           | 282         |
-----------------------------------------
Num timesteps: 11160000
Best mean reward: 5379.41 - Last mean reward per episode: 5116.02
Num timesteps: 11172000
Best mean reward: 5379.41 - Last mean reward per episode: 5071.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.28e+03   |
|    ep_rew_mean          | 5052.08    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 455        |
|    time_elapsed         | 132155     |
|    total_timesteps      | 11182080   |
| train/                  |            |
|    approx_kl            | 0.01837968 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.23      |
|    explained_variance   | 0.98       |
|    learning_rate        | 3e-06      |
|    loss                 | 148        |
|    n_updates            | 4540       |
|    policy_gradient_loss | 0.00104    |
|    value_loss           | 272        |
----------------------------------------
Num timesteps: 11184000
Best mean reward: 5379.41 - Last mean reward per episode: 5052.08
Num timesteps: 11196000
Best mean reward: 5379.41 - Last mean reward per episode: 5054.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.35e+03    |
|    ep_rew_mean          | 4985.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 456         |
|    time_elapsed         | 132443      |
|    total_timesteps      | 11206656    |
| train/                  |             |
|    approx_kl            | 0.017706292 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 33.8        |
|    n_updates            | 4550        |
|    policy_gradient_loss | 0.000111    |
|    value_loss           | 261         |
-----------------------------------------
Num timesteps: 11208000
Best mean reward: 5379.41 - Last mean reward per episode: 4985.32
Num timesteps: 11220000
Best mean reward: 5379.41 - Last mean reward per episode: 4952.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.45e+03    |
|    ep_rew_mean          | 4925.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 457         |
|    time_elapsed         | 132734      |
|    total_timesteps      | 11231232    |
| train/                  |             |
|    approx_kl            | 0.029742107 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 38.8        |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 208         |
-----------------------------------------
Num timesteps: 11232000
Best mean reward: 5379.41 - Last mean reward per episode: 4927.20
Num timesteps: 11244000
Best mean reward: 5379.41 - Last mean reward per episode: 4900.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.62e+03    |
|    ep_rew_mean          | 4856.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 458         |
|    time_elapsed         | 133021      |
|    total_timesteps      | 11255808    |
| train/                  |             |
|    approx_kl            | 0.012371622 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 4570        |
|    policy_gradient_loss | -4.5e-05    |
|    value_loss           | 201         |
-----------------------------------------
Num timesteps: 11256000
Best mean reward: 5379.41 - Last mean reward per episode: 4856.81
Num timesteps: 11268000
Best mean reward: 5379.41 - Last mean reward per episode: 4842.93
Num timesteps: 11280000
Best mean reward: 5379.41 - Last mean reward per episode: 4790.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.78e+03    |
|    ep_rew_mean          | 4790.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 459         |
|    time_elapsed         | 133303      |
|    total_timesteps      | 11280384    |
| train/                  |             |
|    approx_kl            | 0.018606951 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 48.7        |
|    n_updates            | 4580        |
|    policy_gradient_loss | 0.000263    |
|    value_loss           | 380         |
-----------------------------------------
Num timesteps: 11292000
Best mean reward: 5379.41 - Last mean reward per episode: 4765.05
Num timesteps: 11304000
Best mean reward: 5379.41 - Last mean reward per episode: 4725.48
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.9e+03      |
|    ep_rew_mean          | 4725.48      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 460          |
|    time_elapsed         | 133588       |
|    total_timesteps      | 11304960     |
| train/                  |              |
|    approx_kl            | 0.0131447865 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.984        |
|    learning_rate        | 3e-06        |
|    loss                 | 55.3         |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.000791    |
|    value_loss           | 251          |
------------------------------------------
Num timesteps: 11316000
Best mean reward: 5379.41 - Last mean reward per episode: 4659.31
Num timesteps: 11328000
Best mean reward: 5379.41 - Last mean reward per episode: 4662.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.96e+03    |
|    ep_rew_mean          | 4662.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 461         |
|    time_elapsed         | 133879      |
|    total_timesteps      | 11329536    |
| train/                  |             |
|    approx_kl            | 0.014233979 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 39.5        |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.000993   |
|    value_loss           | 244         |
-----------------------------------------
Num timesteps: 11340000
Best mean reward: 5379.41 - Last mean reward per episode: 4610.49
Num timesteps: 11352000
Best mean reward: 5379.41 - Last mean reward per episode: 4610.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.99e+03    |
|    ep_rew_mean          | 4610.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 462         |
|    time_elapsed         | 134169      |
|    total_timesteps      | 11354112    |
| train/                  |             |
|    approx_kl            | 0.017814968 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.987       |
|    learning_rate        | 3e-06       |
|    loss                 | 51.9        |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.000227   |
|    value_loss           | 196         |
-----------------------------------------
Num timesteps: 11364000
Best mean reward: 5379.41 - Last mean reward per episode: 4608.66
Num timesteps: 11376000
Best mean reward: 5379.41 - Last mean reward per episode: 4611.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.04e+03    |
|    ep_rew_mean          | 4611.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 463         |
|    time_elapsed         | 134455      |
|    total_timesteps      | 11378688    |
| train/                  |             |
|    approx_kl            | 0.021853551 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 37.2        |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00618    |
|    value_loss           | 143         |
-----------------------------------------
Num timesteps: 11388000
Best mean reward: 5379.41 - Last mean reward per episode: 4584.16
Num timesteps: 11400000
Best mean reward: 5379.41 - Last mean reward per episode: 4533.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.15e+03    |
|    ep_rew_mean          | 4531.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 464         |
|    time_elapsed         | 134740      |
|    total_timesteps      | 11403264    |
| train/                  |             |
|    approx_kl            | 0.013747737 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 141         |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.000639   |
|    value_loss           | 190         |
-----------------------------------------
Num timesteps: 11412000
Best mean reward: 5379.41 - Last mean reward per episode: 4459.57
Num timesteps: 11424000
Best mean reward: 5379.41 - Last mean reward per episode: 4421.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.39e+03    |
|    ep_rew_mean          | 4421.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 465         |
|    time_elapsed         | 135026      |
|    total_timesteps      | 11427840    |
| train/                  |             |
|    approx_kl            | 0.014615321 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 94.2        |
|    n_updates            | 4640        |
|    policy_gradient_loss | 0.00019     |
|    value_loss           | 296         |
-----------------------------------------
Num timesteps: 11436000
Best mean reward: 5379.41 - Last mean reward per episode: 4420.31
Num timesteps: 11448000
Best mean reward: 5379.41 - Last mean reward per episode: 4366.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.47e+03    |
|    ep_rew_mean          | 4366.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 466         |
|    time_elapsed         | 135311      |
|    total_timesteps      | 11452416    |
| train/                  |             |
|    approx_kl            | 0.022009263 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 53.5        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 202         |
-----------------------------------------
Num timesteps: 11460000
Best mean reward: 5379.41 - Last mean reward per episode: 4261.32
Num timesteps: 11472000
Best mean reward: 5379.41 - Last mean reward per episode: 4196.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.64e+03    |
|    ep_rew_mean          | 4196.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 467         |
|    time_elapsed         | 135603      |
|    total_timesteps      | 11476992    |
| train/                  |             |
|    approx_kl            | 0.013835966 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 80.4        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.000577   |
|    value_loss           | 238         |
-----------------------------------------
Num timesteps: 11484000
Best mean reward: 5379.41 - Last mean reward per episode: 4201.98
Num timesteps: 11496000
Best mean reward: 5379.41 - Last mean reward per episode: 4173.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.57e+03    |
|    ep_rew_mean          | 4206.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 468         |
|    time_elapsed         | 135891      |
|    total_timesteps      | 11501568    |
| train/                  |             |
|    approx_kl            | 0.015995907 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 109         |
|    n_updates            | 4670        |
|    policy_gradient_loss | 0.00141     |
|    value_loss           | 293         |
-----------------------------------------
Num timesteps: 11508000
Best mean reward: 5379.41 - Last mean reward per episode: 4241.91
Num timesteps: 11520000
Best mean reward: 5379.41 - Last mean reward per episode: 4273.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.29e+03    |
|    ep_rew_mean          | 4303.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 469         |
|    time_elapsed         | 136178      |
|    total_timesteps      | 11526144    |
| train/                  |             |
|    approx_kl            | 0.016548688 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 80.9        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.000127   |
|    value_loss           | 357         |
-----------------------------------------
Num timesteps: 11532000
Best mean reward: 5379.41 - Last mean reward per episode: 4184.52
Num timesteps: 11544000
Best mean reward: 5379.41 - Last mean reward per episode: 4157.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.3e+03     |
|    ep_rew_mean          | 4090.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 470         |
|    time_elapsed         | 136465      |
|    total_timesteps      | 11550720    |
| train/                  |             |
|    approx_kl            | 0.014954117 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 260         |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 370         |
-----------------------------------------
Num timesteps: 11556000
Best mean reward: 5379.41 - Last mean reward per episode: 4090.33
Num timesteps: 11568000
Best mean reward: 5379.41 - Last mean reward per episode: 4092.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.04e+03   |
|    ep_rew_mean          | 4113.3     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 471        |
|    time_elapsed         | 136749     |
|    total_timesteps      | 11575296   |
| train/                  |            |
|    approx_kl            | 0.01351469 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.978      |
|    learning_rate        | 3e-06      |
|    loss                 | 384        |
|    n_updates            | 4700       |
|    policy_gradient_loss | 0.00105    |
|    value_loss           | 459        |
----------------------------------------
Num timesteps: 11580000
Best mean reward: 5379.41 - Last mean reward per episode: 4148.07
Num timesteps: 11592000
Best mean reward: 5379.41 - Last mean reward per episode: 4207.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.86e+03    |
|    ep_rew_mean          | 4212.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 472         |
|    time_elapsed         | 137036      |
|    total_timesteps      | 11599872    |
| train/                  |             |
|    approx_kl            | 0.015529387 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 367         |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00155    |
|    value_loss           | 447         |
-----------------------------------------
Num timesteps: 11604000
Best mean reward: 5379.41 - Last mean reward per episode: 4253.22
Num timesteps: 11616000
Best mean reward: 5379.41 - Last mean reward per episode: 4248.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.61e+03    |
|    ep_rew_mean          | 4282.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 473         |
|    time_elapsed         | 137315      |
|    total_timesteps      | 11624448    |
| train/                  |             |
|    approx_kl            | 0.013043232 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 153         |
|    n_updates            | 4720        |
|    policy_gradient_loss | 0.000673    |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 11628000
Best mean reward: 5379.41 - Last mean reward per episode: 4309.53
Num timesteps: 11640000
Best mean reward: 5379.41 - Last mean reward per episode: 4323.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.4e+03     |
|    ep_rew_mean          | 4340.47     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 474         |
|    time_elapsed         | 137600      |
|    total_timesteps      | 11649024    |
| train/                  |             |
|    approx_kl            | 0.011441988 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 113         |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.000462   |
|    value_loss           | 554         |
-----------------------------------------
Num timesteps: 11652000
Best mean reward: 5379.41 - Last mean reward per episode: 4315.62
Num timesteps: 11664000
Best mean reward: 5379.41 - Last mean reward per episode: 4301.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.13e+03    |
|    ep_rew_mean          | 4213.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 475         |
|    time_elapsed         | 137884      |
|    total_timesteps      | 11673600    |
| train/                  |             |
|    approx_kl            | 0.011873324 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.898      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 131         |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.000224   |
|    value_loss           | 388         |
-----------------------------------------
Num timesteps: 11676000
Best mean reward: 5379.41 - Last mean reward per episode: 4213.83
Num timesteps: 11688000
Best mean reward: 5379.41 - Last mean reward per episode: 4255.85
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.78e+03    |
|    ep_rew_mean          | 4295.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 476         |
|    time_elapsed         | 138169      |
|    total_timesteps      | 11698176    |
| train/                  |             |
|    approx_kl            | 0.021041984 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 407         |
|    n_updates            | 4750        |
|    policy_gradient_loss | 0.00626     |
|    value_loss           | 478         |
-----------------------------------------
Num timesteps: 11700000
Best mean reward: 5379.41 - Last mean reward per episode: 4281.80
Num timesteps: 11712000
Best mean reward: 5379.41 - Last mean reward per episode: 4337.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 4321.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 477         |
|    time_elapsed         | 138457      |
|    total_timesteps      | 11722752    |
| train/                  |             |
|    approx_kl            | 0.014912985 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 357         |
|    n_updates            | 4760        |
|    policy_gradient_loss | 0.00162     |
|    value_loss           | 482         |
-----------------------------------------
Num timesteps: 11724000
Best mean reward: 5379.41 - Last mean reward per episode: 4321.29
Num timesteps: 11736000
Best mean reward: 5379.41 - Last mean reward per episode: 4260.05
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.27e+03   |
|    ep_rew_mean          | 4193.52    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 478        |
|    time_elapsed         | 138738     |
|    total_timesteps      | 11747328   |
| train/                  |            |
|    approx_kl            | 0.01593353 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0.971      |
|    learning_rate        | 3e-06      |
|    loss                 | 376        |
|    n_updates            | 4770       |
|    policy_gradient_loss | -0.00066   |
|    value_loss           | 594        |
----------------------------------------
Num timesteps: 11748000
Best mean reward: 5379.41 - Last mean reward per episode: 4220.79
Num timesteps: 11760000
Best mean reward: 5379.41 - Last mean reward per episode: 4234.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 4239.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 479         |
|    time_elapsed         | 139029      |
|    total_timesteps      | 11771904    |
| train/                  |             |
|    approx_kl            | 0.017481558 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 324         |
|    n_updates            | 4780        |
|    policy_gradient_loss | 0.00292     |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 11772000
Best mean reward: 5379.41 - Last mean reward per episode: 4239.66
Num timesteps: 11784000
Best mean reward: 5379.41 - Last mean reward per episode: 4328.18
Num timesteps: 11796000
Best mean reward: 5379.41 - Last mean reward per episode: 4342.80
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.21e+03   |
|    ep_rew_mean          | 4342.8     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 480        |
|    time_elapsed         | 139315     |
|    total_timesteps      | 11796480   |
| train/                  |            |
|    approx_kl            | 0.01737125 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 126        |
|    n_updates            | 4790       |
|    policy_gradient_loss | 0.0012     |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 11808000
Best mean reward: 5379.41 - Last mean reward per episode: 4351.50
Num timesteps: 11820000
Best mean reward: 5379.41 - Last mean reward per episode: 4262.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16e+03    |
|    ep_rew_mean          | 4262.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 481         |
|    time_elapsed         | 139610      |
|    total_timesteps      | 11821056    |
| train/                  |             |
|    approx_kl            | 0.014447704 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 321         |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.000418   |
|    value_loss           | 426         |
-----------------------------------------
Num timesteps: 11832000
Best mean reward: 5379.41 - Last mean reward per episode: 4178.24
Num timesteps: 11844000
Best mean reward: 5379.41 - Last mean reward per episode: 4077.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 4077.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 482         |
|    time_elapsed         | 139901      |
|    total_timesteps      | 11845632    |
| train/                  |             |
|    approx_kl            | 0.015019993 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 151         |
|    n_updates            | 4810        |
|    policy_gradient_loss | 0.000458    |
|    value_loss           | 664         |
-----------------------------------------
Num timesteps: 11856000
Best mean reward: 5379.41 - Last mean reward per episode: 4003.48
Num timesteps: 11868000
Best mean reward: 5379.41 - Last mean reward per episode: 4032.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 4032.77     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 483         |
|    time_elapsed         | 140195      |
|    total_timesteps      | 11870208    |
| train/                  |             |
|    approx_kl            | 0.018492468 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 293         |
|    n_updates            | 4820        |
|    policy_gradient_loss | 0.00273     |
|    value_loss           | 583         |
-----------------------------------------
Num timesteps: 11880000
Best mean reward: 5379.41 - Last mean reward per episode: 4097.53
Num timesteps: 11892000
Best mean reward: 5379.41 - Last mean reward per episode: 4218.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 4214.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 484         |
|    time_elapsed         | 140488      |
|    total_timesteps      | 11894784    |
| train/                  |             |
|    approx_kl            | 0.013266963 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 101         |
|    n_updates            | 4830        |
|    policy_gradient_loss | 0.000696    |
|    value_loss           | 487         |
-----------------------------------------
Num timesteps: 11904000
Best mean reward: 5379.41 - Last mean reward per episode: 4138.20
Num timesteps: 11916000
Best mean reward: 5379.41 - Last mean reward per episode: 4069.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.07e+03     |
|    ep_rew_mean          | 4167.43      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 485          |
|    time_elapsed         | 140790       |
|    total_timesteps      | 11919360     |
| train/                  |              |
|    approx_kl            | 0.0141783105 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.803       |
|    explained_variance   | 0.97         |
|    learning_rate        | 3e-06        |
|    loss                 | 108          |
|    n_updates            | 4840         |
|    policy_gradient_loss | 0.000817     |
|    value_loss           | 578          |
------------------------------------------
Num timesteps: 11928000
Best mean reward: 5379.41 - Last mean reward per episode: 4167.43
Num timesteps: 11940000
Best mean reward: 5379.41 - Last mean reward per episode: 4188.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 4171.76     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 486         |
|    time_elapsed         | 141082      |
|    total_timesteps      | 11943936    |
| train/                  |             |
|    approx_kl            | 0.016641984 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 175         |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.000454   |
|    value_loss           | 488         |
-----------------------------------------
Num timesteps: 11952000
Best mean reward: 5379.41 - Last mean reward per episode: 4213.95
Num timesteps: 11964000
Best mean reward: 5379.41 - Last mean reward per episode: 4244.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.1e+03     |
|    ep_rew_mean          | 4267.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 487         |
|    time_elapsed         | 141384      |
|    total_timesteps      | 11968512    |
| train/                  |             |
|    approx_kl            | 0.013403266 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 101         |
|    n_updates            | 4860        |
|    policy_gradient_loss | 0.000505    |
|    value_loss           | 475         |
-----------------------------------------
Num timesteps: 11976000
Best mean reward: 5379.41 - Last mean reward per episode: 4279.44
Num timesteps: 11988000
Best mean reward: 5379.41 - Last mean reward per episode: 4272.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.1e+03     |
|    ep_rew_mean          | 4276.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 488         |
|    time_elapsed         | 141680      |
|    total_timesteps      | 11993088    |
| train/                  |             |
|    approx_kl            | 0.013671349 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.796      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 169         |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.000975   |
|    value_loss           | 442         |
-----------------------------------------
Num timesteps: 12000000
Best mean reward: 5379.41 - Last mean reward per episode: 4288.74
Num timesteps: 12012000
Best mean reward: 5379.41 - Last mean reward per episode: 4251.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.08e+03    |
|    ep_rew_mean          | 4255.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 489         |
|    time_elapsed         | 141978      |
|    total_timesteps      | 12017664    |
| train/                  |             |
|    approx_kl            | 0.011310917 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 120         |
|    n_updates            | 4880        |
|    policy_gradient_loss | 0.000624    |
|    value_loss           | 352         |
-----------------------------------------
Num timesteps: 12024000
Best mean reward: 5379.41 - Last mean reward per episode: 4239.29
Num timesteps: 12036000
Best mean reward: 5379.41 - Last mean reward per episode: 4336.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 4336.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 490         |
|    time_elapsed         | 142274      |
|    total_timesteps      | 12042240    |
| train/                  |             |
|    approx_kl            | 0.020450959 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 252         |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 446         |
-----------------------------------------
Num timesteps: 12048000
Best mean reward: 5379.41 - Last mean reward per episode: 4416.16
Num timesteps: 12060000
Best mean reward: 5379.41 - Last mean reward per episode: 4488.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 4437.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 491         |
|    time_elapsed         | 142564      |
|    total_timesteps      | 12066816    |
| train/                  |             |
|    approx_kl            | 0.019001491 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.975      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 289         |
|    n_updates            | 4900        |
|    policy_gradient_loss | 0.000592    |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 12072000
Best mean reward: 5379.41 - Last mean reward per episode: 4409.70
Num timesteps: 12084000
Best mean reward: 5379.41 - Last mean reward per episode: 4441.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 4452.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 492         |
|    time_elapsed         | 142854      |
|    total_timesteps      | 12091392    |
| train/                  |             |
|    approx_kl            | 0.017438808 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 323         |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.000819   |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 12096000
Best mean reward: 5379.41 - Last mean reward per episode: 4458.98
Num timesteps: 12108000
Best mean reward: 5379.41 - Last mean reward per episode: 4442.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.24e+03    |
|    ep_rew_mean          | 4414.63     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 493         |
|    time_elapsed         | 143142      |
|    total_timesteps      | 12115968    |
| train/                  |             |
|    approx_kl            | 0.014312287 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 331         |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.000154   |
|    value_loss           | 395         |
-----------------------------------------
Num timesteps: 12120000
Best mean reward: 5379.41 - Last mean reward per episode: 4444.96
Num timesteps: 12132000
Best mean reward: 5379.41 - Last mean reward per episode: 4497.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4551.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 494         |
|    time_elapsed         | 143433      |
|    total_timesteps      | 12140544    |
| train/                  |             |
|    approx_kl            | 0.014902356 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 544         |
|    n_updates            | 4930        |
|    policy_gradient_loss | 0.000995    |
|    value_loss           | 473         |
-----------------------------------------
Num timesteps: 12144000
Best mean reward: 5379.41 - Last mean reward per episode: 4551.41
Num timesteps: 12156000
Best mean reward: 5379.41 - Last mean reward per episode: 4654.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4649.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 495         |
|    time_elapsed         | 143722      |
|    total_timesteps      | 12165120    |
| train/                  |             |
|    approx_kl            | 0.012072421 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 161         |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 403         |
-----------------------------------------
Num timesteps: 12168000
Best mean reward: 5379.41 - Last mean reward per episode: 4649.88
Num timesteps: 12180000
Best mean reward: 5379.41 - Last mean reward per episode: 4566.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4547.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 496         |
|    time_elapsed         | 144008      |
|    total_timesteps      | 12189696    |
| train/                  |             |
|    approx_kl            | 0.011949797 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 195         |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 500         |
-----------------------------------------
Num timesteps: 12192000
Best mean reward: 5379.41 - Last mean reward per episode: 4564.12
Num timesteps: 12204000
Best mean reward: 5379.41 - Last mean reward per episode: 4453.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4371.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 497         |
|    time_elapsed         | 144297      |
|    total_timesteps      | 12214272    |
| train/                  |             |
|    approx_kl            | 0.018392634 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 206         |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.000443   |
|    value_loss           | 830         |
-----------------------------------------
Num timesteps: 12216000
Best mean reward: 5379.41 - Last mean reward per episode: 4371.71
Num timesteps: 12228000
Best mean reward: 5379.41 - Last mean reward per episode: 4233.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4154.13     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 498         |
|    time_elapsed         | 144586      |
|    total_timesteps      | 12238848    |
| train/                  |             |
|    approx_kl            | 0.013556323 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 276         |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.000997   |
|    value_loss           | 593         |
-----------------------------------------
Num timesteps: 12240000
Best mean reward: 5379.41 - Last mean reward per episode: 4165.19
Num timesteps: 12252000
Best mean reward: 5379.41 - Last mean reward per episode: 4084.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 4071.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 499         |
|    time_elapsed         | 144872      |
|    total_timesteps      | 12263424    |
| train/                  |             |
|    approx_kl            | 0.017350934 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 134         |
|    n_updates            | 4980        |
|    policy_gradient_loss | 0.000936    |
|    value_loss           | 736         |
-----------------------------------------
Num timesteps: 12264000
Best mean reward: 5379.41 - Last mean reward per episode: 4066.48
Num timesteps: 12276000
Best mean reward: 5379.41 - Last mean reward per episode: 4079.02
Num timesteps: 12288000
Best mean reward: 5379.41 - Last mean reward per episode: 4095.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.09e+03    |
|    ep_rew_mean          | 4095.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 500         |
|    time_elapsed         | 145163      |
|    total_timesteps      | 12288000    |
| train/                  |             |
|    approx_kl            | 0.018138627 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 101         |
|    n_updates            | 4990        |
|    policy_gradient_loss | 0.00215     |
|    value_loss           | 503         |
-----------------------------------------
Num timesteps: 12300000
Best mean reward: 5379.41 - Last mean reward per episode: 4034.96
Num timesteps: 12312000
Best mean reward: 5379.41 - Last mean reward per episode: 4045.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.03e+03    |
|    ep_rew_mean          | 4070.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 501         |
|    time_elapsed         | 145447      |
|    total_timesteps      | 12312576    |
| train/                  |             |
|    approx_kl            | 0.014468313 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 181         |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.00051    |
|    value_loss           | 617         |
-----------------------------------------
Num timesteps: 12324000
Best mean reward: 5379.41 - Last mean reward per episode: 4098.48
Num timesteps: 12336000
Best mean reward: 5379.41 - Last mean reward per episode: 3960.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3960.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 502         |
|    time_elapsed         | 145728      |
|    total_timesteps      | 12337152    |
| train/                  |             |
|    approx_kl            | 0.011110115 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 306         |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 404         |
-----------------------------------------
Num timesteps: 12348000
Best mean reward: 5379.41 - Last mean reward per episode: 3975.60
Num timesteps: 12360000
Best mean reward: 5379.41 - Last mean reward per episode: 3853.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.9e+03     |
|    ep_rew_mean          | 3882.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 503         |
|    time_elapsed         | 146018      |
|    total_timesteps      | 12361728    |
| train/                  |             |
|    approx_kl            | 0.013310857 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 83.1        |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.000598   |
|    value_loss           | 426         |
-----------------------------------------
Num timesteps: 12372000
Best mean reward: 5379.41 - Last mean reward per episode: 4000.51
Num timesteps: 12384000
Best mean reward: 5379.41 - Last mean reward per episode: 3943.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3964.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 504         |
|    time_elapsed         | 146309      |
|    total_timesteps      | 12386304    |
| train/                  |             |
|    approx_kl            | 0.011851671 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 168         |
|    n_updates            | 5030        |
|    policy_gradient_loss | 0.000385    |
|    value_loss           | 728         |
-----------------------------------------
Num timesteps: 12396000
Best mean reward: 5379.41 - Last mean reward per episode: 4047.15
Num timesteps: 12408000
Best mean reward: 5379.41 - Last mean reward per episode: 4044.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 4094.07     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 505         |
|    time_elapsed         | 146594      |
|    total_timesteps      | 12410880    |
| train/                  |             |
|    approx_kl            | 0.015550817 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 352         |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 594         |
-----------------------------------------
Num timesteps: 12420000
Best mean reward: 5379.41 - Last mean reward per episode: 4163.71
Num timesteps: 12432000
Best mean reward: 5379.41 - Last mean reward per episode: 4166.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | 4294.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 506         |
|    time_elapsed         | 146881      |
|    total_timesteps      | 12435456    |
| train/                  |             |
|    approx_kl            | 0.011647047 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 945         |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.000339   |
|    value_loss           | 557         |
-----------------------------------------
Num timesteps: 12444000
Best mean reward: 5379.41 - Last mean reward per episode: 4341.51
Num timesteps: 12456000
Best mean reward: 5379.41 - Last mean reward per episode: 4354.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 4389.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 507         |
|    time_elapsed         | 147168      |
|    total_timesteps      | 12460032    |
| train/                  |             |
|    approx_kl            | 0.010514091 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 96.2        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 447         |
-----------------------------------------
Num timesteps: 12468000
Best mean reward: 5379.41 - Last mean reward per episode: 4461.46
Num timesteps: 12480000
Best mean reward: 5379.41 - Last mean reward per episode: 4501.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.12e+03    |
|    ep_rew_mean          | 4501.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 508         |
|    time_elapsed         | 147454      |
|    total_timesteps      | 12484608    |
| train/                  |             |
|    approx_kl            | 0.012530337 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 146         |
|    n_updates            | 5070        |
|    policy_gradient_loss | 0.00039     |
|    value_loss           | 471         |
-----------------------------------------
Num timesteps: 12492000
Best mean reward: 5379.41 - Last mean reward per episode: 4499.19
Num timesteps: 12504000
Best mean reward: 5379.41 - Last mean reward per episode: 4589.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 4621.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 509         |
|    time_elapsed         | 147739      |
|    total_timesteps      | 12509184    |
| train/                  |             |
|    approx_kl            | 0.012717634 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 153         |
|    n_updates            | 5080        |
|    policy_gradient_loss | 5.99e-06    |
|    value_loss           | 477         |
-----------------------------------------
Num timesteps: 12516000
Best mean reward: 5379.41 - Last mean reward per episode: 4602.27
Num timesteps: 12528000
Best mean reward: 5379.41 - Last mean reward per episode: 4594.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 4589.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 510         |
|    time_elapsed         | 148026      |
|    total_timesteps      | 12533760    |
| train/                  |             |
|    approx_kl            | 0.013782325 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.791      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 157         |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.000353   |
|    value_loss           | 544         |
-----------------------------------------
Num timesteps: 12540000
Best mean reward: 5379.41 - Last mean reward per episode: 4515.35
Num timesteps: 12552000
Best mean reward: 5379.41 - Last mean reward per episode: 4591.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16e+03    |
|    ep_rew_mean          | 4628.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 511         |
|    time_elapsed         | 148311      |
|    total_timesteps      | 12558336    |
| train/                  |             |
|    approx_kl            | 0.011662602 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 282         |
|    n_updates            | 5100        |
|    policy_gradient_loss | 2.27e-05    |
|    value_loss           | 508         |
-----------------------------------------
Num timesteps: 12564000
Best mean reward: 5379.41 - Last mean reward per episode: 4631.23
Num timesteps: 12576000
Best mean reward: 5379.41 - Last mean reward per episode: 4752.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 4745.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 512         |
|    time_elapsed         | 148600      |
|    total_timesteps      | 12582912    |
| train/                  |             |
|    approx_kl            | 0.009440384 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 53.1        |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.000661   |
|    value_loss           | 342         |
-----------------------------------------
Num timesteps: 12588000
Best mean reward: 5379.41 - Last mean reward per episode: 4746.02
Num timesteps: 12600000
Best mean reward: 5379.41 - Last mean reward per episode: 4810.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4811.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 513         |
|    time_elapsed         | 148885      |
|    total_timesteps      | 12607488    |
| train/                  |             |
|    approx_kl            | 0.013388685 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.709      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.000429   |
|    value_loss           | 565         |
-----------------------------------------
Num timesteps: 12612000
Best mean reward: 5379.41 - Last mean reward per episode: 4811.30
Num timesteps: 12624000
Best mean reward: 5379.41 - Last mean reward per episode: 4871.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4791.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 514         |
|    time_elapsed         | 149169      |
|    total_timesteps      | 12632064    |
| train/                  |             |
|    approx_kl            | 0.013599053 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 303         |
|    n_updates            | 5130        |
|    policy_gradient_loss | 0.000755    |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 12636000
Best mean reward: 5379.41 - Last mean reward per episode: 4784.61
Num timesteps: 12648000
Best mean reward: 5379.41 - Last mean reward per episode: 4787.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4792.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 515         |
|    time_elapsed         | 149460      |
|    total_timesteps      | 12656640    |
| train/                  |             |
|    approx_kl            | 0.022400074 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 194         |
|    n_updates            | 5140        |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 537         |
-----------------------------------------
Num timesteps: 12660000
Best mean reward: 5379.41 - Last mean reward per episode: 4786.88
Num timesteps: 12672000
Best mean reward: 5379.41 - Last mean reward per episode: 4731.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 4760.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 516         |
|    time_elapsed         | 149748      |
|    total_timesteps      | 12681216    |
| train/                  |             |
|    approx_kl            | 0.020709725 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 626         |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 549         |
-----------------------------------------
Num timesteps: 12684000
Best mean reward: 5379.41 - Last mean reward per episode: 4798.17
Num timesteps: 12696000
Best mean reward: 5379.41 - Last mean reward per episode: 4800.26
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.22e+03   |
|    ep_rew_mean          | 4770.54    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 517        |
|    time_elapsed         | 150037     |
|    total_timesteps      | 12705792   |
| train/                  |            |
|    approx_kl            | 0.01493911 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.967      |
|    learning_rate        | 3e-06      |
|    loss                 | 399        |
|    n_updates            | 5160       |
|    policy_gradient_loss | 0.00214    |
|    value_loss           | 597        |
----------------------------------------
Num timesteps: 12708000
Best mean reward: 5379.41 - Last mean reward per episode: 4770.54
Num timesteps: 12720000
Best mean reward: 5379.41 - Last mean reward per episode: 4792.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 4862.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 518         |
|    time_elapsed         | 150326      |
|    total_timesteps      | 12730368    |
| train/                  |             |
|    approx_kl            | 0.016910316 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 153         |
|    n_updates            | 5170        |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 472         |
-----------------------------------------
Num timesteps: 12732000
Best mean reward: 5379.41 - Last mean reward per episode: 4831.72
Num timesteps: 12744000
Best mean reward: 5379.41 - Last mean reward per episode: 4830.80
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.28e+03   |
|    ep_rew_mean          | 4866.67    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 519        |
|    time_elapsed         | 150614     |
|    total_timesteps      | 12754944   |
| train/                  |            |
|    approx_kl            | 0.01628224 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.763     |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 222        |
|    n_updates            | 5180       |
|    policy_gradient_loss | 0.000116   |
|    value_loss           | 540        |
----------------------------------------
Num timesteps: 12756000
Best mean reward: 5379.41 - Last mean reward per episode: 4866.67
Num timesteps: 12768000
Best mean reward: 5379.41 - Last mean reward per episode: 4844.60
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.29e+03  |
|    ep_rew_mean          | 4892.57   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 520       |
|    time_elapsed         | 150905    |
|    total_timesteps      | 12779520  |
| train/                  |           |
|    approx_kl            | 0.0157922 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.687    |
|    explained_variance   | 0.967     |
|    learning_rate        | 3e-06     |
|    loss                 | 178       |
|    n_updates            | 5190      |
|    policy_gradient_loss | 0.00163   |
|    value_loss           | 568       |
---------------------------------------
Num timesteps: 12780000
Best mean reward: 5379.41 - Last mean reward per episode: 4892.09
Num timesteps: 12792000
Best mean reward: 5379.41 - Last mean reward per episode: 4887.17
Num timesteps: 12804000
Best mean reward: 5379.41 - Last mean reward per episode: 4738.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4738.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 521         |
|    time_elapsed         | 151194      |
|    total_timesteps      | 12804096    |
| train/                  |             |
|    approx_kl            | 0.012315735 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 214         |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.000513   |
|    value_loss           | 460         |
-----------------------------------------
Num timesteps: 12816000
Best mean reward: 5379.41 - Last mean reward per episode: 4760.40
Num timesteps: 12828000
Best mean reward: 5379.41 - Last mean reward per episode: 4735.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4737.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 522         |
|    time_elapsed         | 151482      |
|    total_timesteps      | 12828672    |
| train/                  |             |
|    approx_kl            | 0.016016556 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 249         |
|    n_updates            | 5210        |
|    policy_gradient_loss | 0.00094     |
|    value_loss           | 644         |
-----------------------------------------
Num timesteps: 12840000
Best mean reward: 5379.41 - Last mean reward per episode: 4692.41
Num timesteps: 12852000
Best mean reward: 5379.41 - Last mean reward per episode: 4653.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.18e+03   |
|    ep_rew_mean          | 4626.67    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 523        |
|    time_elapsed         | 151767     |
|    total_timesteps      | 12853248   |
| train/                  |            |
|    approx_kl            | 0.01214011 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.97       |
|    learning_rate        | 3e-06      |
|    loss                 | 276        |
|    n_updates            | 5220       |
|    policy_gradient_loss | 0.00166    |
|    value_loss           | 581        |
----------------------------------------
Num timesteps: 12864000
Best mean reward: 5379.41 - Last mean reward per episode: 4578.65
Num timesteps: 12876000
Best mean reward: 5379.41 - Last mean reward per episode: 4435.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.12e+03    |
|    ep_rew_mean          | 4435.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 524         |
|    time_elapsed         | 152054      |
|    total_timesteps      | 12877824    |
| train/                  |             |
|    approx_kl            | 0.013673227 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 249         |
|    n_updates            | 5230        |
|    policy_gradient_loss | 0.000167    |
|    value_loss           | 590         |
-----------------------------------------
Num timesteps: 12888000
Best mean reward: 5379.41 - Last mean reward per episode: 4446.65
Num timesteps: 12900000
Best mean reward: 5379.41 - Last mean reward per episode: 4439.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 4440.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 525         |
|    time_elapsed         | 152337      |
|    total_timesteps      | 12902400    |
| train/                  |             |
|    approx_kl            | 0.013271456 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 174         |
|    n_updates            | 5240        |
|    policy_gradient_loss | -0.000893   |
|    value_loss           | 603         |
-----------------------------------------
Num timesteps: 12912000
Best mean reward: 5379.41 - Last mean reward per episode: 4476.10
Num timesteps: 12924000
Best mean reward: 5379.41 - Last mean reward per episode: 4446.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.1e+03    |
|    ep_rew_mean          | 4435.24    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 526        |
|    time_elapsed         | 152624     |
|    total_timesteps      | 12926976   |
| train/                  |            |
|    approx_kl            | 0.01657263 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.967      |
|    learning_rate        | 3e-06      |
|    loss                 | 383        |
|    n_updates            | 5250       |
|    policy_gradient_loss | 0.00314    |
|    value_loss           | 496        |
----------------------------------------
Num timesteps: 12936000
Best mean reward: 5379.41 - Last mean reward per episode: 4450.52
Num timesteps: 12948000
Best mean reward: 5379.41 - Last mean reward per episode: 4409.10
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.07e+03   |
|    ep_rew_mean          | 4409.1     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 527        |
|    time_elapsed         | 152917     |
|    total_timesteps      | 12951552   |
| train/                  |            |
|    approx_kl            | 0.01147341 |
|    clip_fraction        | 0.0914     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.978      |
|    learning_rate        | 3e-06      |
|    loss                 | 188        |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.000191  |
|    value_loss           | 424        |
----------------------------------------
Num timesteps: 12960000
Best mean reward: 5379.41 - Last mean reward per episode: 4444.27
Num timesteps: 12972000
Best mean reward: 5379.41 - Last mean reward per episode: 4436.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.08e+03    |
|    ep_rew_mean          | 4431.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 528         |
|    time_elapsed         | 153208      |
|    total_timesteps      | 12976128    |
| train/                  |             |
|    approx_kl            | 0.016540602 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 88.3        |
|    n_updates            | 5270        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 570         |
-----------------------------------------
Num timesteps: 12984000
Best mean reward: 5379.41 - Last mean reward per episode: 4462.67
Num timesteps: 12996000
Best mean reward: 5379.41 - Last mean reward per episode: 4454.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 4460.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 529         |
|    time_elapsed         | 153498      |
|    total_timesteps      | 13000704    |
| train/                  |             |
|    approx_kl            | 0.034432504 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 366         |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 555         |
-----------------------------------------
Num timesteps: 13008000
Best mean reward: 5379.41 - Last mean reward per episode: 4510.33
Num timesteps: 13020000
Best mean reward: 5379.41 - Last mean reward per episode: 4581.18
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 4650.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 530         |
|    time_elapsed         | 153791      |
|    total_timesteps      | 13025280    |
| train/                  |             |
|    approx_kl            | 0.013422062 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 500         |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 534         |
-----------------------------------------
Num timesteps: 13032000
Best mean reward: 5379.41 - Last mean reward per episode: 4642.04
Num timesteps: 13044000
Best mean reward: 5379.41 - Last mean reward per episode: 4666.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 4622.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 531         |
|    time_elapsed         | 154085      |
|    total_timesteps      | 13049856    |
| train/                  |             |
|    approx_kl            | 0.019684682 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 132         |
|    n_updates            | 5300        |
|    policy_gradient_loss | 0.00259     |
|    value_loss           | 622         |
-----------------------------------------
Num timesteps: 13056000
Best mean reward: 5379.41 - Last mean reward per episode: 4704.59
Num timesteps: 13068000
Best mean reward: 5379.41 - Last mean reward per episode: 4750.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4799.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 532         |
|    time_elapsed         | 154388      |
|    total_timesteps      | 13074432    |
| train/                  |             |
|    approx_kl            | 0.021026606 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 328         |
|    n_updates            | 5310        |
|    policy_gradient_loss | 0.0024      |
|    value_loss           | 706         |
-----------------------------------------
Num timesteps: 13080000
Best mean reward: 5379.41 - Last mean reward per episode: 4778.48
Num timesteps: 13092000
Best mean reward: 5379.41 - Last mean reward per episode: 4873.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4932.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 533         |
|    time_elapsed         | 154686      |
|    total_timesteps      | 13099008    |
| train/                  |             |
|    approx_kl            | 0.017085513 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 287         |
|    n_updates            | 5320        |
|    policy_gradient_loss | 0.00235     |
|    value_loss           | 593         |
-----------------------------------------
Num timesteps: 13104000
Best mean reward: 5379.41 - Last mean reward per episode: 4940.08
Num timesteps: 13116000
Best mean reward: 5379.41 - Last mean reward per episode: 5011.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.35e+03     |
|    ep_rew_mean          | 5018.43      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 534          |
|    time_elapsed         | 154985       |
|    total_timesteps      | 13123584     |
| train/                  |              |
|    approx_kl            | 0.0146279605 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | 0.966        |
|    learning_rate        | 3e-06        |
|    loss                 | 581          |
|    n_updates            | 5330         |
|    policy_gradient_loss | 0.000727     |
|    value_loss           | 615          |
------------------------------------------
Num timesteps: 13128000
Best mean reward: 5379.41 - Last mean reward per episode: 5018.43
Num timesteps: 13140000
Best mean reward: 5379.41 - Last mean reward per episode: 4986.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4981.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 535         |
|    time_elapsed         | 155280      |
|    total_timesteps      | 13148160    |
| train/                  |             |
|    approx_kl            | 0.018146962 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 299         |
|    n_updates            | 5340        |
|    policy_gradient_loss | 0.00237     |
|    value_loss           | 580         |
-----------------------------------------
Num timesteps: 13152000
Best mean reward: 5379.41 - Last mean reward per episode: 5029.27
Num timesteps: 13164000
Best mean reward: 5379.41 - Last mean reward per episode: 5047.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 5023.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 536         |
|    time_elapsed         | 155575      |
|    total_timesteps      | 13172736    |
| train/                  |             |
|    approx_kl            | 0.015412125 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 102         |
|    n_updates            | 5350        |
|    policy_gradient_loss | 0.00168     |
|    value_loss           | 617         |
-----------------------------------------
Num timesteps: 13176000
Best mean reward: 5379.41 - Last mean reward per episode: 5021.22
Num timesteps: 13188000
Best mean reward: 5379.41 - Last mean reward per episode: 5026.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4996.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 537         |
|    time_elapsed         | 155870      |
|    total_timesteps      | 13197312    |
| train/                  |             |
|    approx_kl            | 0.017675174 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 225         |
|    n_updates            | 5360        |
|    policy_gradient_loss | 0.00423     |
|    value_loss           | 418         |
-----------------------------------------
Num timesteps: 13200000
Best mean reward: 5379.41 - Last mean reward per episode: 4996.96
Num timesteps: 13212000
Best mean reward: 5379.41 - Last mean reward per episode: 4998.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4929.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 538         |
|    time_elapsed         | 156167      |
|    total_timesteps      | 13221888    |
| train/                  |             |
|    approx_kl            | 0.014698218 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 5370        |
|    policy_gradient_loss | 0.000827    |
|    value_loss           | 430         |
-----------------------------------------
Num timesteps: 13224000
Best mean reward: 5379.41 - Last mean reward per episode: 4929.71
Num timesteps: 13236000
Best mean reward: 5379.41 - Last mean reward per episode: 4851.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4911.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 539         |
|    time_elapsed         | 156461      |
|    total_timesteps      | 13246464    |
| train/                  |             |
|    approx_kl            | 0.021079304 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 396         |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.000118   |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 13248000
Best mean reward: 5379.41 - Last mean reward per episode: 4911.91
Num timesteps: 13260000
Best mean reward: 5379.41 - Last mean reward per episode: 4916.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4971.43     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 540         |
|    time_elapsed         | 156756      |
|    total_timesteps      | 13271040    |
| train/                  |             |
|    approx_kl            | 0.015086602 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 123         |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.000668   |
|    value_loss           | 474         |
-----------------------------------------
Num timesteps: 13272000
Best mean reward: 5379.41 - Last mean reward per episode: 4971.43
Num timesteps: 13284000
Best mean reward: 5379.41 - Last mean reward per episode: 4997.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.31e+03   |
|    ep_rew_mean          | 5033.99    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 541        |
|    time_elapsed         | 157053     |
|    total_timesteps      | 13295616   |
| train/                  |            |
|    approx_kl            | 0.01280452 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.971      |
|    learning_rate        | 3e-06      |
|    loss                 | 131        |
|    n_updates            | 5400       |
|    policy_gradient_loss | 0.000728   |
|    value_loss           | 537        |
----------------------------------------
Num timesteps: 13296000
Best mean reward: 5379.41 - Last mean reward per episode: 5027.33
Num timesteps: 13308000
Best mean reward: 5379.41 - Last mean reward per episode: 4972.84
Num timesteps: 13320000
Best mean reward: 5379.41 - Last mean reward per episode: 4932.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4932.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 542         |
|    time_elapsed         | 157358      |
|    total_timesteps      | 13320192    |
| train/                  |             |
|    approx_kl            | 0.011027001 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 310         |
|    n_updates            | 5410        |
|    policy_gradient_loss | 0.000311    |
|    value_loss           | 508         |
-----------------------------------------
Num timesteps: 13332000
Best mean reward: 5379.41 - Last mean reward per episode: 4893.41
Num timesteps: 13344000
Best mean reward: 5379.41 - Last mean reward per episode: 4933.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4933.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 543         |
|    time_elapsed         | 157657      |
|    total_timesteps      | 13344768    |
| train/                  |             |
|    approx_kl            | 0.020641731 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 294         |
|    n_updates            | 5420        |
|    policy_gradient_loss | 0.00269     |
|    value_loss           | 552         |
-----------------------------------------
Num timesteps: 13356000
Best mean reward: 5379.41 - Last mean reward per episode: 4888.67
Num timesteps: 13368000
Best mean reward: 5379.41 - Last mean reward per episode: 4930.53
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.27e+03   |
|    ep_rew_mean          | 4943.92    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 544        |
|    time_elapsed         | 157945     |
|    total_timesteps      | 13369344   |
| train/                  |            |
|    approx_kl            | 0.01300257 |
|    clip_fraction        | 0.0953     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.976      |
|    learning_rate        | 3e-06      |
|    loss                 | 191        |
|    n_updates            | 5430       |
|    policy_gradient_loss | -0.000371  |
|    value_loss           | 456        |
----------------------------------------
Num timesteps: 13380000
Best mean reward: 5379.41 - Last mean reward per episode: 4936.91
Num timesteps: 13392000
Best mean reward: 5379.41 - Last mean reward per episode: 4953.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4953.4      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 545         |
|    time_elapsed         | 158239      |
|    total_timesteps      | 13393920    |
| train/                  |             |
|    approx_kl            | 0.011001676 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 87.1        |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 453         |
-----------------------------------------
Num timesteps: 13404000
Best mean reward: 5379.41 - Last mean reward per episode: 4952.98
Num timesteps: 13416000
Best mean reward: 5379.41 - Last mean reward per episode: 5008.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4994.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 546         |
|    time_elapsed         | 158536      |
|    total_timesteps      | 13418496    |
| train/                  |             |
|    approx_kl            | 0.012762499 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 169         |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 484         |
-----------------------------------------
Num timesteps: 13428000
Best mean reward: 5379.41 - Last mean reward per episode: 5037.76
Num timesteps: 13440000
Best mean reward: 5379.41 - Last mean reward per episode: 5066.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 5104.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 547         |
|    time_elapsed         | 158835      |
|    total_timesteps      | 13443072    |
| train/                  |             |
|    approx_kl            | 0.009009006 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 207         |
|    n_updates            | 5460        |
|    policy_gradient_loss | 0.00051     |
|    value_loss           | 390         |
-----------------------------------------
Num timesteps: 13452000
Best mean reward: 5379.41 - Last mean reward per episode: 5132.13
Num timesteps: 13464000
Best mean reward: 5379.41 - Last mean reward per episode: 5176.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 5165.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 548         |
|    time_elapsed         | 159122      |
|    total_timesteps      | 13467648    |
| train/                  |             |
|    approx_kl            | 0.010209612 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 132         |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 439         |
-----------------------------------------
Num timesteps: 13476000
Best mean reward: 5379.41 - Last mean reward per episode: 5128.39
Num timesteps: 13488000
Best mean reward: 5379.41 - Last mean reward per episode: 5107.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 5077.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 549         |
|    time_elapsed         | 159418      |
|    total_timesteps      | 13492224    |
| train/                  |             |
|    approx_kl            | 0.014684115 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 211         |
|    n_updates            | 5480        |
|    policy_gradient_loss | 0.00181     |
|    value_loss           | 575         |
-----------------------------------------
Num timesteps: 13500000
Best mean reward: 5379.41 - Last mean reward per episode: 5044.25
Num timesteps: 13512000
Best mean reward: 5379.41 - Last mean reward per episode: 5028.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 5036.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 550         |
|    time_elapsed         | 159698      |
|    total_timesteps      | 13516800    |
| train/                  |             |
|    approx_kl            | 0.018258726 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.689      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 269         |
|    n_updates            | 5490        |
|    policy_gradient_loss | 0.000439    |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 13524000
Best mean reward: 5379.41 - Last mean reward per episode: 5053.55
Num timesteps: 13536000
Best mean reward: 5379.41 - Last mean reward per episode: 5060.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 5078.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 551         |
|    time_elapsed         | 159981      |
|    total_timesteps      | 13541376    |
| train/                  |             |
|    approx_kl            | 0.010103074 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 141         |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.000604   |
|    value_loss           | 410         |
-----------------------------------------
Num timesteps: 13548000
Best mean reward: 5379.41 - Last mean reward per episode: 5112.70
Num timesteps: 13560000
Best mean reward: 5379.41 - Last mean reward per episode: 5193.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5232.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 552         |
|    time_elapsed         | 160264      |
|    total_timesteps      | 13565952    |
| train/                  |             |
|    approx_kl            | 0.012648597 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 435         |
|    n_updates            | 5510        |
|    policy_gradient_loss | -3.56e-05   |
|    value_loss           | 514         |
-----------------------------------------
Num timesteps: 13572000
Best mean reward: 5379.41 - Last mean reward per episode: 5205.37
Num timesteps: 13584000
Best mean reward: 5379.41 - Last mean reward per episode: 5210.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5252.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 553         |
|    time_elapsed         | 160550      |
|    total_timesteps      | 13590528    |
| train/                  |             |
|    approx_kl            | 0.013147515 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 13596000
Best mean reward: 5379.41 - Last mean reward per episode: 5214.64
Num timesteps: 13608000
Best mean reward: 5379.41 - Last mean reward per episode: 5213.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5211.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 554         |
|    time_elapsed         | 160839      |
|    total_timesteps      | 13615104    |
| train/                  |             |
|    approx_kl            | 0.014088884 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 160         |
|    n_updates            | 5530        |
|    policy_gradient_loss | 4.64e-05    |
|    value_loss           | 536         |
-----------------------------------------
Num timesteps: 13620000
Best mean reward: 5379.41 - Last mean reward per episode: 5140.12
Num timesteps: 13632000
Best mean reward: 5379.41 - Last mean reward per episode: 5137.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 5134.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 555         |
|    time_elapsed         | 161129      |
|    total_timesteps      | 13639680    |
| train/                  |             |
|    approx_kl            | 0.013814132 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.627      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 208         |
|    n_updates            | 5540        |
|    policy_gradient_loss | 0.00299     |
|    value_loss           | 583         |
-----------------------------------------
Num timesteps: 13644000
Best mean reward: 5379.41 - Last mean reward per episode: 5176.74
Num timesteps: 13656000
Best mean reward: 5379.41 - Last mean reward per episode: 5168.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 5120.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 556         |
|    time_elapsed         | 161418      |
|    total_timesteps      | 13664256    |
| train/                  |             |
|    approx_kl            | 0.012423106 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 81.3        |
|    n_updates            | 5550        |
|    policy_gradient_loss | 0.000162    |
|    value_loss           | 480         |
-----------------------------------------
Num timesteps: 13668000
Best mean reward: 5379.41 - Last mean reward per episode: 5088.72
Num timesteps: 13680000
Best mean reward: 5379.41 - Last mean reward per episode: 5102.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 5083.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 557         |
|    time_elapsed         | 161707      |
|    total_timesteps      | 13688832    |
| train/                  |             |
|    approx_kl            | 0.012084581 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 55.5        |
|    n_updates            | 5560        |
|    policy_gradient_loss | -9.6e-06    |
|    value_loss           | 390         |
-----------------------------------------
Num timesteps: 13692000
Best mean reward: 5379.41 - Last mean reward per episode: 5083.17
Num timesteps: 13704000
Best mean reward: 5379.41 - Last mean reward per episode: 5108.20
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.37e+03   |
|    ep_rew_mean          | 5157.71    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 558        |
|    time_elapsed         | 161995     |
|    total_timesteps      | 13713408   |
| train/                  |            |
|    approx_kl            | 0.03425077 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.974      |
|    learning_rate        | 3e-06      |
|    loss                 | 225        |
|    n_updates            | 5570       |
|    policy_gradient_loss | -0.00302   |
|    value_loss           | 449        |
----------------------------------------
Num timesteps: 13716000
Best mean reward: 5379.41 - Last mean reward per episode: 5167.86
Num timesteps: 13728000
Best mean reward: 5379.41 - Last mean reward per episode: 5164.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5255.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 559         |
|    time_elapsed         | 162283      |
|    total_timesteps      | 13737984    |
| train/                  |             |
|    approx_kl            | 0.009626661 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 198         |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.000694   |
|    value_loss           | 469         |
-----------------------------------------
Num timesteps: 13740000
Best mean reward: 5379.41 - Last mean reward per episode: 5263.17
Num timesteps: 13752000
Best mean reward: 5379.41 - Last mean reward per episode: 5199.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 5126.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 560         |
|    time_elapsed         | 162578      |
|    total_timesteps      | 13762560    |
| train/                  |             |
|    approx_kl            | 0.011709449 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 401         |
|    n_updates            | 5590        |
|    policy_gradient_loss | 0.000934    |
|    value_loss           | 428         |
-----------------------------------------
Num timesteps: 13764000
Best mean reward: 5379.41 - Last mean reward per episode: 5126.37
Num timesteps: 13776000
Best mean reward: 5379.41 - Last mean reward per episode: 5149.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.36e+03   |
|    ep_rew_mean          | 5113.16    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 561        |
|    time_elapsed         | 162869     |
|    total_timesteps      | 13787136   |
| train/                  |            |
|    approx_kl            | 0.01718792 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.606     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 401        |
|    n_updates            | 5600       |
|    policy_gradient_loss | 0.00206    |
|    value_loss           | 474        |
----------------------------------------
Num timesteps: 13788000
Best mean reward: 5379.41 - Last mean reward per episode: 5113.16
Num timesteps: 13800000
Best mean reward: 5379.41 - Last mean reward per episode: 5096.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5133.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 562         |
|    time_elapsed         | 163157      |
|    total_timesteps      | 13811712    |
| train/                  |             |
|    approx_kl            | 0.013933629 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 143         |
|    n_updates            | 5610        |
|    policy_gradient_loss | 0.00212     |
|    value_loss           | 553         |
-----------------------------------------
Num timesteps: 13812000
Best mean reward: 5379.41 - Last mean reward per episode: 5133.72
Num timesteps: 13824000
Best mean reward: 5379.41 - Last mean reward per episode: 5134.01
Num timesteps: 13836000
Best mean reward: 5379.41 - Last mean reward per episode: 5140.07
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.37e+03   |
|    ep_rew_mean          | 5140.07    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 563        |
|    time_elapsed         | 163443     |
|    total_timesteps      | 13836288   |
| train/                  |            |
|    approx_kl            | 0.01449418 |
|    clip_fraction        | 0.0892     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.521     |
|    explained_variance   | 0.979      |
|    learning_rate        | 3e-06      |
|    loss                 | 56.8       |
|    n_updates            | 5620       |
|    policy_gradient_loss | 0.000684   |
|    value_loss           | 448        |
----------------------------------------
Num timesteps: 13848000
Best mean reward: 5379.41 - Last mean reward per episode: 5183.57
Num timesteps: 13860000
Best mean reward: 5379.41 - Last mean reward per episode: 5207.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 5255.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 564         |
|    time_elapsed         | 163730      |
|    total_timesteps      | 13860864    |
| train/                  |             |
|    approx_kl            | 0.014410861 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 307         |
|    n_updates            | 5630        |
|    policy_gradient_loss | 0.000455    |
|    value_loss           | 501         |
-----------------------------------------
Num timesteps: 13872000
Best mean reward: 5379.41 - Last mean reward per episode: 5203.03
Num timesteps: 13884000
Best mean reward: 5379.41 - Last mean reward per episode: 5175.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 5185.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 565         |
|    time_elapsed         | 164023      |
|    total_timesteps      | 13885440    |
| train/                  |             |
|    approx_kl            | 0.012667156 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.000468   |
|    value_loss           | 560         |
-----------------------------------------
Num timesteps: 13896000
Best mean reward: 5379.41 - Last mean reward per episode: 5216.21
Num timesteps: 13908000
Best mean reward: 5379.41 - Last mean reward per episode: 5249.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5250.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 566         |
|    time_elapsed         | 164318      |
|    total_timesteps      | 13910016    |
| train/                  |             |
|    approx_kl            | 0.011994102 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 63.9        |
|    n_updates            | 5650        |
|    policy_gradient_loss | 0.000508    |
|    value_loss           | 441         |
-----------------------------------------
Num timesteps: 13920000
Best mean reward: 5379.41 - Last mean reward per episode: 5263.64
Num timesteps: 13932000
Best mean reward: 5379.41 - Last mean reward per episode: 5327.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 5330.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 567         |
|    time_elapsed         | 164609      |
|    total_timesteps      | 13934592    |
| train/                  |             |
|    approx_kl            | 0.010530195 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 85.9        |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.000981   |
|    value_loss           | 457         |
-----------------------------------------
Num timesteps: 13944000
Best mean reward: 5379.41 - Last mean reward per episode: 5338.11
Num timesteps: 13956000
Best mean reward: 5379.41 - Last mean reward per episode: 5334.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 5329.15     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 568         |
|    time_elapsed         | 164900      |
|    total_timesteps      | 13959168    |
| train/                  |             |
|    approx_kl            | 0.020516664 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 153         |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.000821   |
|    value_loss           | 511         |
-----------------------------------------
Num timesteps: 13968000
Best mean reward: 5379.41 - Last mean reward per episode: 5329.78
Num timesteps: 13980000
Best mean reward: 5379.41 - Last mean reward per episode: 5360.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 5359.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 569         |
|    time_elapsed         | 165185      |
|    total_timesteps      | 13983744    |
| train/                  |             |
|    approx_kl            | 0.010203683 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 409         |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 13992000
Best mean reward: 5379.41 - Last mean reward per episode: 5400.54
Saving new best model to tmp/best_model
Num timesteps: 14004000
Best mean reward: 5400.54 - Last mean reward per episode: 5512.11
Saving new best model to tmp/best_model
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.48e+03  |
|    ep_rew_mean          | 5520.04   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 570       |
|    time_elapsed         | 165476    |
|    total_timesteps      | 14008320  |
| train/                  |           |
|    approx_kl            | 0.0119477 |
|    clip_fraction        | 0.0835    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.583    |
|    explained_variance   | 0.977     |
|    learning_rate        | 3e-06     |
|    loss                 | 154       |
|    n_updates            | 5690      |
|    policy_gradient_loss | -0.000407 |
|    value_loss           | 437       |
---------------------------------------
Num timesteps: 14016000
Best mean reward: 5512.11 - Last mean reward per episode: 5520.86
Saving new best model to tmp/best_model
Num timesteps: 14028000
Best mean reward: 5520.86 - Last mean reward per episode: 5535.25
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 5568.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 571         |
|    time_elapsed         | 165764      |
|    total_timesteps      | 14032896    |
| train/                  |             |
|    approx_kl            | 0.011276995 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 128         |
|    n_updates            | 5700        |
|    policy_gradient_loss | 0.000742    |
|    value_loss           | 443         |
-----------------------------------------
Num timesteps: 14040000
Best mean reward: 5535.25 - Last mean reward per episode: 5554.89
Saving new best model to tmp/best_model
Num timesteps: 14052000
Best mean reward: 5554.89 - Last mean reward per episode: 5564.95
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 5560.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 572         |
|    time_elapsed         | 166052      |
|    total_timesteps      | 14057472    |
| train/                  |             |
|    approx_kl            | 0.015146333 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 129         |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 397         |
-----------------------------------------
Num timesteps: 14064000
Best mean reward: 5564.95 - Last mean reward per episode: 5557.43
Num timesteps: 14076000
Best mean reward: 5564.95 - Last mean reward per episode: 5546.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 5588.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 573         |
|    time_elapsed         | 166341      |
|    total_timesteps      | 14082048    |
| train/                  |             |
|    approx_kl            | 0.010249551 |
|    clip_fraction        | 0.0782      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 212         |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.000134   |
|    value_loss           | 438         |
-----------------------------------------
Num timesteps: 14088000
Best mean reward: 5564.95 - Last mean reward per episode: 5498.94
Num timesteps: 14100000
Best mean reward: 5564.95 - Last mean reward per episode: 5487.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5481.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 574         |
|    time_elapsed         | 166636      |
|    total_timesteps      | 14106624    |
| train/                  |             |
|    approx_kl            | 0.009954225 |
|    clip_fraction        | 0.0712      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 294         |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 464         |
-----------------------------------------
Num timesteps: 14112000
Best mean reward: 5564.95 - Last mean reward per episode: 5490.70
Num timesteps: 14124000
Best mean reward: 5564.95 - Last mean reward per episode: 5538.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 5549.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 575         |
|    time_elapsed         | 166927      |
|    total_timesteps      | 14131200    |
| train/                  |             |
|    approx_kl            | 0.014657817 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 181         |
|    n_updates            | 5740        |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 462         |
-----------------------------------------
Num timesteps: 14136000
Best mean reward: 5564.95 - Last mean reward per episode: 5544.68
Num timesteps: 14148000
Best mean reward: 5564.95 - Last mean reward per episode: 5544.71
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.51e+03  |
|    ep_rew_mean          | 5554.15   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 576       |
|    time_elapsed         | 167215    |
|    total_timesteps      | 14155776  |
| train/                  |           |
|    approx_kl            | 0.0153743 |
|    clip_fraction        | 0.0958    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.553    |
|    explained_variance   | 0.976     |
|    learning_rate        | 3e-06     |
|    loss                 | 79        |
|    n_updates            | 5750      |
|    policy_gradient_loss | -7e-05    |
|    value_loss           | 497       |
---------------------------------------
Num timesteps: 14160000
Best mean reward: 5564.95 - Last mean reward per episode: 5550.87
Num timesteps: 14172000
Best mean reward: 5564.95 - Last mean reward per episode: 5546.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 5544.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 577         |
|    time_elapsed         | 167504      |
|    total_timesteps      | 14180352    |
| train/                  |             |
|    approx_kl            | 0.009895329 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 58.2        |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.000365   |
|    value_loss           | 460         |
-----------------------------------------
Num timesteps: 14184000
Best mean reward: 5564.95 - Last mean reward per episode: 5542.34
Num timesteps: 14196000
Best mean reward: 5564.95 - Last mean reward per episode: 5521.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5433.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 578         |
|    time_elapsed         | 167790      |
|    total_timesteps      | 14204928    |
| train/                  |             |
|    approx_kl            | 0.013116191 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 266         |
|    n_updates            | 5770        |
|    policy_gradient_loss | 2.93e-07    |
|    value_loss           | 608         |
-----------------------------------------
Num timesteps: 14208000
Best mean reward: 5564.95 - Last mean reward per episode: 5422.21
Num timesteps: 14220000
Best mean reward: 5564.95 - Last mean reward per episode: 5405.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5400.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 579         |
|    time_elapsed         | 168071      |
|    total_timesteps      | 14229504    |
| train/                  |             |
|    approx_kl            | 0.016744068 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 149         |
|    n_updates            | 5780        |
|    policy_gradient_loss | 0.000919    |
|    value_loss           | 522         |
-----------------------------------------
Num timesteps: 14232000
Best mean reward: 5564.95 - Last mean reward per episode: 5405.80
Num timesteps: 14244000
Best mean reward: 5564.95 - Last mean reward per episode: 5385.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.44e+03    |
|    ep_rew_mean          | 5388.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 580         |
|    time_elapsed         | 168358      |
|    total_timesteps      | 14254080    |
| train/                  |             |
|    approx_kl            | 0.009418822 |
|    clip_fraction        | 0.0782      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.568      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 157         |
|    n_updates            | 5790        |
|    policy_gradient_loss | 0.00069     |
|    value_loss           | 479         |
-----------------------------------------
Num timesteps: 14256000
Best mean reward: 5564.95 - Last mean reward per episode: 5388.66
Num timesteps: 14268000
Best mean reward: 5564.95 - Last mean reward per episode: 5380.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.45e+03     |
|    ep_rew_mean          | 5432.13      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 581          |
|    time_elapsed         | 168647       |
|    total_timesteps      | 14278656     |
| train/                  |              |
|    approx_kl            | 0.0109437695 |
|    clip_fraction        | 0.086        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.977        |
|    learning_rate        | 3e-06        |
|    loss                 | 254          |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.000635    |
|    value_loss           | 516          |
------------------------------------------
Num timesteps: 14280000
Best mean reward: 5564.95 - Last mean reward per episode: 5435.59
Num timesteps: 14292000
Best mean reward: 5564.95 - Last mean reward per episode: 5395.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5359.1      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 582         |
|    time_elapsed         | 168934      |
|    total_timesteps      | 14303232    |
| train/                  |             |
|    approx_kl            | 0.008287207 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 133         |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 399         |
-----------------------------------------
Num timesteps: 14304000
Best mean reward: 5564.95 - Last mean reward per episode: 5359.10
Num timesteps: 14316000
Best mean reward: 5564.95 - Last mean reward per episode: 5195.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 5230.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 583         |
|    time_elapsed         | 169221      |
|    total_timesteps      | 14327808    |
| train/                  |             |
|    approx_kl            | 0.013989534 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.646      |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 466         |
|    n_updates            | 5820        |
|    policy_gradient_loss | 0.00059     |
|    value_loss           | 549         |
-----------------------------------------
Num timesteps: 14328000
Best mean reward: 5564.95 - Last mean reward per episode: 5230.42
Num timesteps: 14340000
Best mean reward: 5564.95 - Last mean reward per episode: 5216.77
Num timesteps: 14352000
Best mean reward: 5564.95 - Last mean reward per episode: 5203.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 5203.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 584         |
|    time_elapsed         | 169517      |
|    total_timesteps      | 14352384    |
| train/                  |             |
|    approx_kl            | 0.019346269 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 661         |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 493         |
-----------------------------------------
Num timesteps: 14364000
Best mean reward: 5564.95 - Last mean reward per episode: 5217.42
Num timesteps: 14376000
Best mean reward: 5564.95 - Last mean reward per episode: 5130.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5130.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 585         |
|    time_elapsed         | 169806      |
|    total_timesteps      | 14376960    |
| train/                  |             |
|    approx_kl            | 0.010294802 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 201         |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 389         |
-----------------------------------------
Num timesteps: 14388000
Best mean reward: 5564.95 - Last mean reward per episode: 5097.15
Num timesteps: 14400000
Best mean reward: 5564.95 - Last mean reward per episode: 5097.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 5097.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 586         |
|    time_elapsed         | 170102      |
|    total_timesteps      | 14401536    |
| train/                  |             |
|    approx_kl            | 0.012733203 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 209         |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 464         |
-----------------------------------------
Num timesteps: 14412000
Best mean reward: 5564.95 - Last mean reward per episode: 5119.24
Num timesteps: 14424000
Best mean reward: 5564.95 - Last mean reward per episode: 5102.68
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.3e+03    |
|    ep_rew_mean          | 5102.68    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 587        |
|    time_elapsed         | 170394     |
|    total_timesteps      | 14426112   |
| train/                  |            |
|    approx_kl            | 0.00850915 |
|    clip_fraction        | 0.0698     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.983      |
|    learning_rate        | 3e-06      |
|    loss                 | 238        |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.00175   |
|    value_loss           | 332        |
----------------------------------------
Num timesteps: 14436000
Best mean reward: 5564.95 - Last mean reward per episode: 5159.75
Num timesteps: 14448000
Best mean reward: 5564.95 - Last mean reward per episode: 5230.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 5225.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 588         |
|    time_elapsed         | 170689      |
|    total_timesteps      | 14450688    |
| train/                  |             |
|    approx_kl            | 0.009544447 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 105         |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 333         |
-----------------------------------------
Num timesteps: 14460000
Best mean reward: 5564.95 - Last mean reward per episode: 5153.89
Num timesteps: 14472000
Best mean reward: 5564.95 - Last mean reward per episode: 5126.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 5133.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 589         |
|    time_elapsed         | 170982      |
|    total_timesteps      | 14475264    |
| train/                  |             |
|    approx_kl            | 0.009599243 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 299         |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00119    |
|    value_loss           | 460         |
-----------------------------------------
Num timesteps: 14484000
Best mean reward: 5564.95 - Last mean reward per episode: 5134.02
Num timesteps: 14496000
Best mean reward: 5564.95 - Last mean reward per episode: 5126.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 5122.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 590         |
|    time_elapsed         | 171269      |
|    total_timesteps      | 14499840    |
| train/                  |             |
|    approx_kl            | 0.012290229 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 129         |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 466         |
-----------------------------------------
Num timesteps: 14508000
Best mean reward: 5564.95 - Last mean reward per episode: 5121.49
Num timesteps: 14520000
Best mean reward: 5564.95 - Last mean reward per episode: 5116.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5171.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 591         |
|    time_elapsed         | 171562      |
|    total_timesteps      | 14524416    |
| train/                  |             |
|    approx_kl            | 0.010098427 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 501         |
|    n_updates            | 5900        |
|    policy_gradient_loss | -4.15e-05   |
|    value_loss           | 439         |
-----------------------------------------
Num timesteps: 14532000
Best mean reward: 5564.95 - Last mean reward per episode: 5211.56
Num timesteps: 14544000
Best mean reward: 5564.95 - Last mean reward per episode: 5253.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5300.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 592         |
|    time_elapsed         | 171848      |
|    total_timesteps      | 14548992    |
| train/                  |             |
|    approx_kl            | 0.012145181 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 160         |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.000834   |
|    value_loss           | 417         |
-----------------------------------------
Num timesteps: 14556000
Best mean reward: 5564.95 - Last mean reward per episode: 5391.87
Num timesteps: 14568000
Best mean reward: 5564.95 - Last mean reward per episode: 5364.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5338.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 593         |
|    time_elapsed         | 172134      |
|    total_timesteps      | 14573568    |
| train/                  |             |
|    approx_kl            | 0.011731689 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 288         |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 439         |
-----------------------------------------
Num timesteps: 14580000
Best mean reward: 5564.95 - Last mean reward per episode: 5356.29
Num timesteps: 14592000
Best mean reward: 5564.95 - Last mean reward per episode: 5350.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 5309.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 594         |
|    time_elapsed         | 172422      |
|    total_timesteps      | 14598144    |
| train/                  |             |
|    approx_kl            | 0.011155973 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 118         |
|    n_updates            | 5930        |
|    policy_gradient_loss | 0.00024     |
|    value_loss           | 482         |
-----------------------------------------
Num timesteps: 14604000
Best mean reward: 5564.95 - Last mean reward per episode: 5349.03
Num timesteps: 14616000
Best mean reward: 5564.95 - Last mean reward per episode: 5360.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5399.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 595         |
|    time_elapsed         | 172710      |
|    total_timesteps      | 14622720    |
| train/                  |             |
|    approx_kl            | 0.011059248 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 73.4        |
|    n_updates            | 5940        |
|    policy_gradient_loss | 0.000451    |
|    value_loss           | 462         |
-----------------------------------------
Num timesteps: 14628000
Best mean reward: 5564.95 - Last mean reward per episode: 5327.38
Num timesteps: 14640000
Best mean reward: 5564.95 - Last mean reward per episode: 5256.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5259.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 596         |
|    time_elapsed         | 172994      |
|    total_timesteps      | 14647296    |
| train/                  |             |
|    approx_kl            | 0.015698966 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 298         |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.000179   |
|    value_loss           | 466         |
-----------------------------------------
Num timesteps: 14652000
Best mean reward: 5564.95 - Last mean reward per episode: 5218.94
Num timesteps: 14664000
Best mean reward: 5564.95 - Last mean reward per episode: 5255.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 5168.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 597         |
|    time_elapsed         | 173284      |
|    total_timesteps      | 14671872    |
| train/                  |             |
|    approx_kl            | 0.012333598 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 145         |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.000764   |
|    value_loss           | 516         |
-----------------------------------------
Num timesteps: 14676000
Best mean reward: 5564.95 - Last mean reward per episode: 5171.32
Num timesteps: 14688000
Best mean reward: 5564.95 - Last mean reward per episode: 5250.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5236.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 598         |
|    time_elapsed         | 173578      |
|    total_timesteps      | 14696448    |
| train/                  |             |
|    approx_kl            | 0.014551406 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 107         |
|    n_updates            | 5970        |
|    policy_gradient_loss | 0.000604    |
|    value_loss           | 567         |
-----------------------------------------
Num timesteps: 14700000
Best mean reward: 5564.95 - Last mean reward per episode: 5273.07
Num timesteps: 14712000
Best mean reward: 5564.95 - Last mean reward per episode: 5258.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 5179.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 599         |
|    time_elapsed         | 173864      |
|    total_timesteps      | 14721024    |
| train/                  |             |
|    approx_kl            | 0.017349342 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 1.08e+03    |
|    n_updates            | 5980        |
|    policy_gradient_loss | 0.00123     |
|    value_loss           | 606         |
-----------------------------------------
Num timesteps: 14724000
Best mean reward: 5564.95 - Last mean reward per episode: 5179.57
Num timesteps: 14736000
Best mean reward: 5564.95 - Last mean reward per episode: 5151.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 5099.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 600         |
|    time_elapsed         | 174154      |
|    total_timesteps      | 14745600    |
| train/                  |             |
|    approx_kl            | 0.012203851 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 267         |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.000304   |
|    value_loss           | 606         |
-----------------------------------------
Num timesteps: 14748000
Best mean reward: 5564.95 - Last mean reward per episode: 5094.22
Num timesteps: 14760000
Best mean reward: 5564.95 - Last mean reward per episode: 5080.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 5088.31     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 601         |
|    time_elapsed         | 174444      |
|    total_timesteps      | 14770176    |
| train/                  |             |
|    approx_kl            | 0.010772902 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 108         |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 14772000
Best mean reward: 5564.95 - Last mean reward per episode: 5088.31
Num timesteps: 14784000
Best mean reward: 5564.95 - Last mean reward per episode: 5136.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 5126.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 602         |
|    time_elapsed         | 174736      |
|    total_timesteps      | 14794752    |
| train/                  |             |
|    approx_kl            | 0.009860211 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 237         |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.000577   |
|    value_loss           | 546         |
-----------------------------------------
Num timesteps: 14796000
Best mean reward: 5564.95 - Last mean reward per episode: 5126.91
Num timesteps: 14808000
Best mean reward: 5564.95 - Last mean reward per episode: 5152.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 5142.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 603         |
|    time_elapsed         | 175024      |
|    total_timesteps      | 14819328    |
| train/                  |             |
|    approx_kl            | 0.011508656 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 302         |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.000111   |
|    value_loss           | 389         |
-----------------------------------------
Num timesteps: 14820000
Best mean reward: 5564.95 - Last mean reward per episode: 5142.35
Num timesteps: 14832000
Best mean reward: 5564.95 - Last mean reward per episode: 5200.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5257.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 604         |
|    time_elapsed         | 175312      |
|    total_timesteps      | 14843904    |
| train/                  |             |
|    approx_kl            | 0.014439176 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 161         |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 383         |
-----------------------------------------
Num timesteps: 14844000
Best mean reward: 5564.95 - Last mean reward per episode: 5257.95
Num timesteps: 14856000
Best mean reward: 5564.95 - Last mean reward per episode: 5248.35
Num timesteps: 14868000
Best mean reward: 5564.95 - Last mean reward per episode: 5301.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.44e+03    |
|    ep_rew_mean          | 5301.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 605         |
|    time_elapsed         | 175603      |
|    total_timesteps      | 14868480    |
| train/                  |             |
|    approx_kl            | 0.011846309 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 212         |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.000262   |
|    value_loss           | 415         |
-----------------------------------------
Num timesteps: 14880000
Best mean reward: 5564.95 - Last mean reward per episode: 5340.48
Num timesteps: 14892000
Best mean reward: 5564.95 - Last mean reward per episode: 5380.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5380.15     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 606         |
|    time_elapsed         | 175896      |
|    total_timesteps      | 14893056    |
| train/                  |             |
|    approx_kl            | 0.013434253 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 266         |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 442         |
-----------------------------------------
Num timesteps: 14904000
Best mean reward: 5564.95 - Last mean reward per episode: 5419.82
Num timesteps: 14916000
Best mean reward: 5564.95 - Last mean reward per episode: 5379.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5429.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 607         |
|    time_elapsed         | 176183      |
|    total_timesteps      | 14917632    |
| train/                  |             |
|    approx_kl            | 0.012636024 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 146         |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.000134   |
|    value_loss           | 448         |
-----------------------------------------
Num timesteps: 14928000
Best mean reward: 5564.95 - Last mean reward per episode: 5423.47
Num timesteps: 14940000
Best mean reward: 5564.95 - Last mean reward per episode: 5384.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5384.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 608         |
|    time_elapsed         | 176473      |
|    total_timesteps      | 14942208    |
| train/                  |             |
|    approx_kl            | 0.013061411 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 41.3        |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.000968   |
|    value_loss           | 474         |
-----------------------------------------
Num timesteps: 14952000
Best mean reward: 5564.95 - Last mean reward per episode: 5387.67
Num timesteps: 14964000
Best mean reward: 5564.95 - Last mean reward per episode: 5390.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 5434.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 609         |
|    time_elapsed         | 176764      |
|    total_timesteps      | 14966784    |
| train/                  |             |
|    approx_kl            | 0.012137626 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 259         |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 495         |
-----------------------------------------
Num timesteps: 14976000
Best mean reward: 5564.95 - Last mean reward per episode: 5429.43
Num timesteps: 14988000
Best mean reward: 5564.95 - Last mean reward per episode: 5393.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 5434.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 610         |
|    time_elapsed         | 177055      |
|    total_timesteps      | 14991360    |
| train/                  |             |
|    approx_kl            | 0.014189324 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 596         |
|    n_updates            | 6090        |
|    policy_gradient_loss | 0.00177     |
|    value_loss           | 641         |
-----------------------------------------
Num timesteps: 15000000
Best mean reward: 5564.95 - Last mean reward per episode: 5442.10
Num timesteps: 15012000
Best mean reward: 5564.95 - Last mean reward per episode: 5420.08
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.47e+03   |
|    ep_rew_mean          | 5420.46    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 611        |
|    time_elapsed         | 177342     |
|    total_timesteps      | 15015936   |
| train/                  |            |
|    approx_kl            | 0.01412383 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.585     |
|    explained_variance   | 0.977      |
|    learning_rate        | 3e-06      |
|    loss                 | 224        |
|    n_updates            | 6100       |
|    policy_gradient_loss | 0.00166    |
|    value_loss           | 484        |
----------------------------------------
Num timesteps: 15024000
Best mean reward: 5564.95 - Last mean reward per episode: 5415.02
Num timesteps: 15036000
Best mean reward: 5564.95 - Last mean reward per episode: 5408.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.45e+03     |
|    ep_rew_mean          | 5408.6       |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 612          |
|    time_elapsed         | 177634       |
|    total_timesteps      | 15040512     |
| train/                  |              |
|    approx_kl            | 0.0078114443 |
|    clip_fraction        | 0.0722       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.983        |
|    learning_rate        | 3e-06        |
|    loss                 | 289          |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.000998    |
|    value_loss           | 392          |
------------------------------------------
Num timesteps: 15048000
Best mean reward: 5564.95 - Last mean reward per episode: 5435.50
Num timesteps: 15060000
Best mean reward: 5564.95 - Last mean reward per episode: 5453.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5444.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 613         |
|    time_elapsed         | 177921      |
|    total_timesteps      | 15065088    |
| train/                  |             |
|    approx_kl            | 0.014392276 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 408         |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 523         |
-----------------------------------------
Num timesteps: 15072000
Best mean reward: 5564.95 - Last mean reward per episode: 5405.26
Num timesteps: 15084000
Best mean reward: 5564.95 - Last mean reward per episode: 5394.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5384.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 614         |
|    time_elapsed         | 178213      |
|    total_timesteps      | 15089664    |
| train/                  |             |
|    approx_kl            | 0.011653166 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 497         |
|    n_updates            | 6130        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 587         |
-----------------------------------------
Num timesteps: 15096000
Best mean reward: 5564.95 - Last mean reward per episode: 5376.66
Num timesteps: 15108000
Best mean reward: 5564.95 - Last mean reward per episode: 5382.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 5337.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 615         |
|    time_elapsed         | 178500      |
|    total_timesteps      | 15114240    |
| train/                  |             |
|    approx_kl            | 0.011748157 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 169         |
|    n_updates            | 6140        |
|    policy_gradient_loss | 0.00108     |
|    value_loss           | 491         |
-----------------------------------------
Num timesteps: 15120000
Best mean reward: 5564.95 - Last mean reward per episode: 5337.94
Num timesteps: 15132000
Best mean reward: 5564.95 - Last mean reward per episode: 5320.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.33e+03   |
|    ep_rew_mean          | 5320.52    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 616        |
|    time_elapsed         | 178802     |
|    total_timesteps      | 15138816   |
| train/                  |            |
|    approx_kl            | 0.00957451 |
|    clip_fraction        | 0.084      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.559     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 99.1       |
|    n_updates            | 6150       |
|    policy_gradient_loss | 0.000779   |
|    value_loss           | 543        |
----------------------------------------
Num timesteps: 15144000
Best mean reward: 5564.95 - Last mean reward per episode: 5274.58
Num timesteps: 15156000
Best mean reward: 5564.95 - Last mean reward per episode: 5314.53
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.34e+03   |
|    ep_rew_mean          | 5277.8     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 617        |
|    time_elapsed         | 179097     |
|    total_timesteps      | 15163392   |
| train/                  |            |
|    approx_kl            | 0.01150884 |
|    clip_fraction        | 0.0848     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.513     |
|    explained_variance   | 0.978      |
|    learning_rate        | 3e-06      |
|    loss                 | 96.9       |
|    n_updates            | 6160       |
|    policy_gradient_loss | 0.00144    |
|    value_loss           | 482        |
----------------------------------------
Num timesteps: 15168000
Best mean reward: 5564.95 - Last mean reward per episode: 5277.80
Num timesteps: 15180000
Best mean reward: 5564.95 - Last mean reward per episode: 5323.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 5334.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 618         |
|    time_elapsed         | 179386      |
|    total_timesteps      | 15187968    |
| train/                  |             |
|    approx_kl            | 0.015861815 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 131         |
|    n_updates            | 6170        |
|    policy_gradient_loss | 0.000112    |
|    value_loss           | 420         |
-----------------------------------------
Num timesteps: 15192000
Best mean reward: 5564.95 - Last mean reward per episode: 5318.26
Num timesteps: 15204000
Best mean reward: 5564.95 - Last mean reward per episode: 5331.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5330.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 619         |
|    time_elapsed         | 179677      |
|    total_timesteps      | 15212544    |
| train/                  |             |
|    approx_kl            | 0.011710311 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 191         |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.00082    |
|    value_loss           | 464         |
-----------------------------------------
Num timesteps: 15216000
Best mean reward: 5564.95 - Last mean reward per episode: 5380.86
Num timesteps: 15228000
Best mean reward: 5564.95 - Last mean reward per episode: 5425.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 5417.4      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 620         |
|    time_elapsed         | 179967      |
|    total_timesteps      | 15237120    |
| train/                  |             |
|    approx_kl            | 0.007889556 |
|    clip_fraction        | 0.0732      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 168         |
|    n_updates            | 6190        |
|    policy_gradient_loss | -8.23e-06   |
|    value_loss           | 410         |
-----------------------------------------
Num timesteps: 15240000
Best mean reward: 5564.95 - Last mean reward per episode: 5417.40
Num timesteps: 15252000
Best mean reward: 5564.95 - Last mean reward per episode: 5428.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5420.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 621         |
|    time_elapsed         | 180256      |
|    total_timesteps      | 15261696    |
| train/                  |             |
|    approx_kl            | 0.007822869 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 162         |
|    n_updates            | 6200        |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 363         |
-----------------------------------------
Num timesteps: 15264000
Best mean reward: 5564.95 - Last mean reward per episode: 5423.25
Num timesteps: 15276000
Best mean reward: 5564.95 - Last mean reward per episode: 5368.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5396.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 622         |
|    time_elapsed         | 180552      |
|    total_timesteps      | 15286272    |
| train/                  |             |
|    approx_kl            | 0.009323587 |
|    clip_fraction        | 0.0685      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 107         |
|    n_updates            | 6210        |
|    policy_gradient_loss | -0.000234   |
|    value_loss           | 479         |
-----------------------------------------
Num timesteps: 15288000
Best mean reward: 5564.95 - Last mean reward per episode: 5396.23
Num timesteps: 15300000
Best mean reward: 5564.95 - Last mean reward per episode: 5372.30
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.4e+03    |
|    ep_rew_mean          | 5375.31    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 623        |
|    time_elapsed         | 180844     |
|    total_timesteps      | 15310848   |
| train/                  |            |
|    approx_kl            | 0.01225213 |
|    clip_fraction        | 0.0904     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.558     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 343        |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.000504  |
|    value_loss           | 570        |
----------------------------------------
Num timesteps: 15312000
Best mean reward: 5564.95 - Last mean reward per episode: 5375.31
Num timesteps: 15324000
Best mean reward: 5564.95 - Last mean reward per episode: 5372.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 5406.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 624         |
|    time_elapsed         | 181135      |
|    total_timesteps      | 15335424    |
| train/                  |             |
|    approx_kl            | 0.011505776 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 6230        |
|    policy_gradient_loss | -0.000186   |
|    value_loss           | 539         |
-----------------------------------------
Num timesteps: 15336000
Best mean reward: 5564.95 - Last mean reward per episode: 5406.73
Num timesteps: 15348000
Best mean reward: 5564.95 - Last mean reward per episode: 5356.96
Num timesteps: 15360000
Best mean reward: 5564.95 - Last mean reward per episode: 5379.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5379.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 625         |
|    time_elapsed         | 181420      |
|    total_timesteps      | 15360000    |
| train/                  |             |
|    approx_kl            | 0.014196816 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 269         |
|    n_updates            | 6240        |
|    policy_gradient_loss | -9.76e-06   |
|    value_loss           | 543         |
-----------------------------------------
Num timesteps: 15372000
Best mean reward: 5564.95 - Last mean reward per episode: 5361.17
Num timesteps: 15384000
Best mean reward: 5564.95 - Last mean reward per episode: 5416.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 5416.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 626         |
|    time_elapsed         | 181718      |
|    total_timesteps      | 15384576    |
| train/                  |             |
|    approx_kl            | 0.010500416 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 154         |
|    n_updates            | 6250        |
|    policy_gradient_loss | -0.000484   |
|    value_loss           | 514         |
-----------------------------------------
Num timesteps: 15396000
Best mean reward: 5564.95 - Last mean reward per episode: 5415.28
Num timesteps: 15408000
Best mean reward: 5564.95 - Last mean reward per episode: 5450.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 5493.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 627         |
|    time_elapsed         | 182009      |
|    total_timesteps      | 15409152    |
| train/                  |             |
|    approx_kl            | 0.012138947 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 266         |
|    n_updates            | 6260        |
|    policy_gradient_loss | -0.000735   |
|    value_loss           | 502         |
-----------------------------------------
Num timesteps: 15420000
Best mean reward: 5564.95 - Last mean reward per episode: 5509.97
Num timesteps: 15432000
Best mean reward: 5564.95 - Last mean reward per episode: 5464.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5473.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 628         |
|    time_elapsed         | 182305      |
|    total_timesteps      | 15433728    |
| train/                  |             |
|    approx_kl            | 0.009334356 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 305         |
|    n_updates            | 6270        |
|    policy_gradient_loss | 0.000379    |
|    value_loss           | 463         |
-----------------------------------------
Num timesteps: 15444000
Best mean reward: 5564.95 - Last mean reward per episode: 5418.05
Num timesteps: 15456000
Best mean reward: 5564.95 - Last mean reward per episode: 5414.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 5414.77     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 629         |
|    time_elapsed         | 182594      |
|    total_timesteps      | 15458304    |
| train/                  |             |
|    approx_kl            | 0.020175798 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 252         |
|    n_updates            | 6280        |
|    policy_gradient_loss | 0.000753    |
|    value_loss           | 509         |
-----------------------------------------
Num timesteps: 15468000
Best mean reward: 5564.95 - Last mean reward per episode: 5390.29
Num timesteps: 15480000
Best mean reward: 5564.95 - Last mean reward per episode: 5427.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 5427.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 630         |
|    time_elapsed         | 182889      |
|    total_timesteps      | 15482880    |
| train/                  |             |
|    approx_kl            | 0.012797377 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 314         |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.000694   |
|    value_loss           | 510         |
-----------------------------------------
Num timesteps: 15492000
Best mean reward: 5564.95 - Last mean reward per episode: 5408.64
Num timesteps: 15504000
Best mean reward: 5564.95 - Last mean reward per episode: 5461.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5455.28     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 631         |
|    time_elapsed         | 183173      |
|    total_timesteps      | 15507456    |
| train/                  |             |
|    approx_kl            | 0.013398967 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 358         |
|    n_updates            | 6300        |
|    policy_gradient_loss | -0.00066    |
|    value_loss           | 479         |
-----------------------------------------
Num timesteps: 15516000
Best mean reward: 5564.95 - Last mean reward per episode: 5424.31
Num timesteps: 15528000
Best mean reward: 5564.95 - Last mean reward per episode: 5408.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5395.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 632         |
|    time_elapsed         | 183458      |
|    total_timesteps      | 15532032    |
| train/                  |             |
|    approx_kl            | 0.011494651 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 202         |
|    n_updates            | 6310        |
|    policy_gradient_loss | -0.000337   |
|    value_loss           | 528         |
-----------------------------------------
Num timesteps: 15540000
Best mean reward: 5564.95 - Last mean reward per episode: 5400.06
Num timesteps: 15552000
Best mean reward: 5564.95 - Last mean reward per episode: 5395.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5400.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 633         |
|    time_elapsed         | 183757      |
|    total_timesteps      | 15556608    |
| train/                  |             |
|    approx_kl            | 0.008814565 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 170         |
|    n_updates            | 6320        |
|    policy_gradient_loss | -0.000515   |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 15564000
Best mean reward: 5564.95 - Last mean reward per episode: 5439.19
Num timesteps: 15576000
Best mean reward: 5564.95 - Last mean reward per episode: 5407.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5453.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 634         |
|    time_elapsed         | 184049      |
|    total_timesteps      | 15581184    |
| train/                  |             |
|    approx_kl            | 0.011921077 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 352         |
|    n_updates            | 6330        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 545         |
-----------------------------------------
Num timesteps: 15588000
Best mean reward: 5564.95 - Last mean reward per episode: 5412.45
Num timesteps: 15600000
Best mean reward: 5564.95 - Last mean reward per episode: 5442.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 5442.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 635         |
|    time_elapsed         | 184339      |
|    total_timesteps      | 15605760    |
| train/                  |             |
|    approx_kl            | 0.011701117 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.975       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 6340        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 579         |
-----------------------------------------
Num timesteps: 15612000
Best mean reward: 5564.95 - Last mean reward per episode: 5381.99
Num timesteps: 15624000
Best mean reward: 5564.95 - Last mean reward per episode: 5387.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5343.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 636         |
|    time_elapsed         | 184627      |
|    total_timesteps      | 15630336    |
| train/                  |             |
|    approx_kl            | 0.012259863 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 179         |
|    n_updates            | 6350        |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 503         |
-----------------------------------------
Num timesteps: 15636000
Best mean reward: 5564.95 - Last mean reward per episode: 5343.57
Num timesteps: 15648000
Best mean reward: 5564.95 - Last mean reward per episode: 5341.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 5315.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 637         |
|    time_elapsed         | 184918      |
|    total_timesteps      | 15654912    |
| train/                  |             |
|    approx_kl            | 0.015816584 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 6360        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 499         |
-----------------------------------------
Num timesteps: 15660000
Best mean reward: 5564.95 - Last mean reward per episode: 5320.10
Num timesteps: 15672000
Best mean reward: 5564.95 - Last mean reward per episode: 5377.50
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.33e+03     |
|    ep_rew_mean          | 5393.18      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 638          |
|    time_elapsed         | 185211       |
|    total_timesteps      | 15679488     |
| train/                  |              |
|    approx_kl            | 0.0086820265 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.981        |
|    learning_rate        | 3e-06        |
|    loss                 | 352          |
|    n_updates            | 6370         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 442          |
------------------------------------------
Num timesteps: 15684000
Best mean reward: 5564.95 - Last mean reward per episode: 5405.09
Num timesteps: 15696000
Best mean reward: 5564.95 - Last mean reward per episode: 5419.90
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.34e+03     |
|    ep_rew_mean          | 5419.84      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 639          |
|    time_elapsed         | 185503       |
|    total_timesteps      | 15704064     |
| train/                  |              |
|    approx_kl            | 0.0073196217 |
|    clip_fraction        | 0.0619       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.982        |
|    learning_rate        | 3e-06        |
|    loss                 | 166          |
|    n_updates            | 6380         |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 405          |
------------------------------------------
Num timesteps: 15708000
Best mean reward: 5564.95 - Last mean reward per episode: 5413.08
Num timesteps: 15720000
Best mean reward: 5564.95 - Last mean reward per episode: 5388.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5380.49     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 640         |
|    time_elapsed         | 185794      |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.009878694 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 117         |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.000669   |
|    value_loss           | 494         |
-----------------------------------------
Num timesteps: 15732000
Best mean reward: 5564.95 - Last mean reward per episode: 5374.42
Num timesteps: 15744000
Best mean reward: 5564.95 - Last mean reward per episode: 5420.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 5374.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 641         |
|    time_elapsed         | 186086      |
|    total_timesteps      | 15753216    |
| train/                  |             |
|    approx_kl            | 0.012648657 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 110         |
|    n_updates            | 6400        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 434         |
-----------------------------------------
Num timesteps: 15756000
Best mean reward: 5564.95 - Last mean reward per episode: 5388.37
Num timesteps: 15768000
Best mean reward: 5564.95 - Last mean reward per episode: 5409.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 5420.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 642         |
|    time_elapsed         | 186378      |
|    total_timesteps      | 15777792    |
| train/                  |             |
|    approx_kl            | 0.016662026 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 129         |
|    n_updates            | 6410        |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 423         |
-----------------------------------------
Num timesteps: 15780000
Best mean reward: 5564.95 - Last mean reward per episode: 5420.45
Num timesteps: 15792000
Best mean reward: 5564.95 - Last mean reward per episode: 5439.10
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.34e+03   |
|    ep_rew_mean          | 5426.73    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 643        |
|    time_elapsed         | 186672     |
|    total_timesteps      | 15802368   |
| train/                  |            |
|    approx_kl            | 0.07126459 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.529     |
|    explained_variance   | 0.978      |
|    learning_rate        | 3e-06      |
|    loss                 | 260        |
|    n_updates            | 6420       |
|    policy_gradient_loss | -0.0049    |
|    value_loss           | 374        |
----------------------------------------
Num timesteps: 15804000
Best mean reward: 5564.95 - Last mean reward per episode: 5426.73
Num timesteps: 15816000
Best mean reward: 5564.95 - Last mean reward per episode: 5469.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 5503.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 644         |
|    time_elapsed         | 186959      |
|    total_timesteps      | 15826944    |
| train/                  |             |
|    approx_kl            | 0.007962504 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 110         |
|    n_updates            | 6430        |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 388         |
-----------------------------------------
Num timesteps: 15828000
Best mean reward: 5564.95 - Last mean reward per episode: 5503.19
Num timesteps: 15840000
Best mean reward: 5564.95 - Last mean reward per episode: 5526.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5544.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 645         |
|    time_elapsed         | 187247      |
|    total_timesteps      | 15851520    |
| train/                  |             |
|    approx_kl            | 0.012105215 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 175         |
|    n_updates            | 6440        |
|    policy_gradient_loss | 0.000136    |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 15852000
Best mean reward: 5564.95 - Last mean reward per episode: 5544.68
Num timesteps: 15864000
Best mean reward: 5564.95 - Last mean reward per episode: 5510.88
Num timesteps: 15876000
Best mean reward: 5564.95 - Last mean reward per episode: 5557.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 5557.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 646         |
|    time_elapsed         | 187538      |
|    total_timesteps      | 15876096    |
| train/                  |             |
|    approx_kl            | 0.010675912 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 79.7        |
|    n_updates            | 6450        |
|    policy_gradient_loss | 0.000629    |
|    value_loss           | 476         |
-----------------------------------------
Num timesteps: 15888000
Best mean reward: 5564.95 - Last mean reward per episode: 5539.38
Num timesteps: 15900000
Best mean reward: 5564.95 - Last mean reward per episode: 5597.83
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 5597.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 647         |
|    time_elapsed         | 187831      |
|    total_timesteps      | 15900672    |
| train/                  |             |
|    approx_kl            | 0.013376622 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 358         |
|    n_updates            | 6460        |
|    policy_gradient_loss | 0.000383    |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 15912000
Best mean reward: 5597.83 - Last mean reward per episode: 5603.65
Saving new best model to tmp/best_model
Num timesteps: 15924000
Best mean reward: 5603.65 - Last mean reward per episode: 5600.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5606.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 648         |
|    time_elapsed         | 188121      |
|    total_timesteps      | 15925248    |
| train/                  |             |
|    approx_kl            | 0.019474408 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.5        |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 94.6        |
|    n_updates            | 6470        |
|    policy_gradient_loss | -0.000663   |
|    value_loss           | 471         |
-----------------------------------------
Num timesteps: 15936000
Best mean reward: 5603.65 - Last mean reward per episode: 5604.38
Saving new best model to tmp/best_model
Num timesteps: 15948000
Best mean reward: 5604.38 - Last mean reward per episode: 5605.45
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5605.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 649         |
|    time_elapsed         | 188412      |
|    total_timesteps      | 15949824    |
| train/                  |             |
|    approx_kl            | 0.012838423 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 6480        |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 417         |
-----------------------------------------
Num timesteps: 15960000
Best mean reward: 5605.45 - Last mean reward per episode: 5617.72
Saving new best model to tmp/best_model
Num timesteps: 15972000
Best mean reward: 5617.72 - Last mean reward per episode: 5610.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5617.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 650         |
|    time_elapsed         | 188702      |
|    total_timesteps      | 15974400    |
| train/                  |             |
|    approx_kl            | 0.009854891 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 211         |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 416         |
-----------------------------------------
Num timesteps: 15984000
Best mean reward: 5617.72 - Last mean reward per episode: 5646.98
Saving new best model to tmp/best_model
Num timesteps: 15996000
Best mean reward: 5646.98 - Last mean reward per episode: 5573.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5619.75     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 651         |
|    time_elapsed         | 188989      |
|    total_timesteps      | 15998976    |
| train/                  |             |
|    approx_kl            | 0.017923974 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 6500        |
|    policy_gradient_loss | -0.000144   |
|    value_loss           | 438         |
-----------------------------------------
Num timesteps: 16008000
Best mean reward: 5646.98 - Last mean reward per episode: 5570.84
Num timesteps: 16020000
Best mean reward: 5646.98 - Last mean reward per episode: 5578.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5577.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 652         |
|    time_elapsed         | 189276      |
|    total_timesteps      | 16023552    |
| train/                  |             |
|    approx_kl            | 0.015753873 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 545         |
|    n_updates            | 6510        |
|    policy_gradient_loss | 0.00238     |
|    value_loss           | 488         |
-----------------------------------------
Num timesteps: 16032000
Best mean reward: 5646.98 - Last mean reward per episode: 5570.63
Num timesteps: 16044000
Best mean reward: 5646.98 - Last mean reward per episode: 5556.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5519.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 653         |
|    time_elapsed         | 189563      |
|    total_timesteps      | 16048128    |
| train/                  |             |
|    approx_kl            | 0.013675877 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 82.1        |
|    n_updates            | 6520        |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 352         |
-----------------------------------------
Num timesteps: 16056000
Best mean reward: 5646.98 - Last mean reward per episode: 5520.64
Num timesteps: 16068000
Best mean reward: 5646.98 - Last mean reward per episode: 5515.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5535.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 654         |
|    time_elapsed         | 189850      |
|    total_timesteps      | 16072704    |
| train/                  |             |
|    approx_kl            | 0.013882729 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 243         |
|    n_updates            | 6530        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 438         |
-----------------------------------------
Num timesteps: 16080000
Best mean reward: 5646.98 - Last mean reward per episode: 5526.88
Num timesteps: 16092000
Best mean reward: 5646.98 - Last mean reward per episode: 5458.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5453.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 655         |
|    time_elapsed         | 190140      |
|    total_timesteps      | 16097280    |
| train/                  |             |
|    approx_kl            | 0.010449591 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 463         |
|    n_updates            | 6540        |
|    policy_gradient_loss | 0.000228    |
|    value_loss           | 439         |
-----------------------------------------
Num timesteps: 16104000
Best mean reward: 5646.98 - Last mean reward per episode: 5489.23
Num timesteps: 16116000
Best mean reward: 5646.98 - Last mean reward per episode: 5447.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5419.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 656         |
|    time_elapsed         | 190433      |
|    total_timesteps      | 16121856    |
| train/                  |             |
|    approx_kl            | 0.015391957 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.976       |
|    learning_rate        | 3e-06       |
|    loss                 | 212         |
|    n_updates            | 6550        |
|    policy_gradient_loss | 0.00309     |
|    value_loss           | 513         |
-----------------------------------------
Num timesteps: 16128000
Best mean reward: 5646.98 - Last mean reward per episode: 5391.25
Num timesteps: 16140000
Best mean reward: 5646.98 - Last mean reward per episode: 5391.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.44e+03    |
|    ep_rew_mean          | 5398.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 657         |
|    time_elapsed         | 190727      |
|    total_timesteps      | 16146432    |
| train/                  |             |
|    approx_kl            | 0.022558898 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 629         |
|    n_updates            | 6560        |
|    policy_gradient_loss | 0.00791     |
|    value_loss           | 545         |
-----------------------------------------
Num timesteps: 16152000
Best mean reward: 5646.98 - Last mean reward per episode: 5407.01
Num timesteps: 16164000
Best mean reward: 5646.98 - Last mean reward per episode: 5407.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.44e+03    |
|    ep_rew_mean          | 5407.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 658         |
|    time_elapsed         | 191016      |
|    total_timesteps      | 16171008    |
| train/                  |             |
|    approx_kl            | 0.014668182 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 196         |
|    n_updates            | 6570        |
|    policy_gradient_loss | 0.00298     |
|    value_loss           | 221         |
-----------------------------------------
Num timesteps: 16176000
Best mean reward: 5646.98 - Last mean reward per episode: 5402.36
Num timesteps: 16188000
Best mean reward: 5646.98 - Last mean reward per episode: 5368.55
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.61e+03  |
|    ep_rew_mean          | 5353.09   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 659       |
|    time_elapsed         | 191309    |
|    total_timesteps      | 16195584  |
| train/                  |           |
|    approx_kl            | 0.0126343 |
|    clip_fraction        | 0.0986    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.897    |
|    explained_variance   | 0.984     |
|    learning_rate        | 3e-06     |
|    loss                 | 69.2      |
|    n_updates            | 6580      |
|    policy_gradient_loss | -0.000332 |
|    value_loss           | 195       |
---------------------------------------
Num timesteps: 16200000
Best mean reward: 5646.98 - Last mean reward per episode: 5353.09
Num timesteps: 16212000
Best mean reward: 5646.98 - Last mean reward per episode: 5350.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.79e+03    |
|    ep_rew_mean          | 5323.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 660         |
|    time_elapsed         | 191598      |
|    total_timesteps      | 16220160    |
| train/                  |             |
|    approx_kl            | 0.015479923 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 88.7        |
|    n_updates            | 6590        |
|    policy_gradient_loss | 0.00267     |
|    value_loss           | 333         |
-----------------------------------------
Num timesteps: 16224000
Best mean reward: 5646.98 - Last mean reward per episode: 5329.27
Num timesteps: 16236000
Best mean reward: 5646.98 - Last mean reward per episode: 5332.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.98e+03    |
|    ep_rew_mean          | 5351.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 661         |
|    time_elapsed         | 191893      |
|    total_timesteps      | 16244736    |
| train/                  |             |
|    approx_kl            | 0.009994734 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 133         |
|    n_updates            | 6600        |
|    policy_gradient_loss | 0.000499    |
|    value_loss           | 368         |
-----------------------------------------
Num timesteps: 16248000
Best mean reward: 5646.98 - Last mean reward per episode: 5344.10
Num timesteps: 16260000
Best mean reward: 5646.98 - Last mean reward per episode: 5344.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.02e+03    |
|    ep_rew_mean          | 5339.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 662         |
|    time_elapsed         | 192183      |
|    total_timesteps      | 16269312    |
| train/                  |             |
|    approx_kl            | 0.009837458 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 92.4        |
|    n_updates            | 6610        |
|    policy_gradient_loss | -0.000929   |
|    value_loss           | 338         |
-----------------------------------------
Num timesteps: 16272000
Best mean reward: 5646.98 - Last mean reward per episode: 5339.12
Num timesteps: 16284000
Best mean reward: 5646.98 - Last mean reward per episode: 5386.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.09e+03    |
|    ep_rew_mean          | 5391.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 663         |
|    time_elapsed         | 192472      |
|    total_timesteps      | 16293888    |
| train/                  |             |
|    approx_kl            | 0.008992442 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 117         |
|    n_updates            | 6620        |
|    policy_gradient_loss | -0.000206   |
|    value_loss           | 354         |
-----------------------------------------
Num timesteps: 16296000
Best mean reward: 5646.98 - Last mean reward per episode: 5389.23
Num timesteps: 16308000
Best mean reward: 5646.98 - Last mean reward per episode: 5430.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.16e+03    |
|    ep_rew_mean          | 5468.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 664         |
|    time_elapsed         | 192758      |
|    total_timesteps      | 16318464    |
| train/                  |             |
|    approx_kl            | 0.012963566 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 125         |
|    n_updates            | 6630        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 348         |
-----------------------------------------
Num timesteps: 16320000
Best mean reward: 5646.98 - Last mean reward per episode: 5468.51
Num timesteps: 16332000
Best mean reward: 5646.98 - Last mean reward per episode: 5509.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.17e+03    |
|    ep_rew_mean          | 5495.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 665         |
|    time_elapsed         | 193046      |
|    total_timesteps      | 16343040    |
| train/                  |             |
|    approx_kl            | 0.011775908 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 123         |
|    n_updates            | 6640        |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 438         |
-----------------------------------------
Num timesteps: 16344000
Best mean reward: 5646.98 - Last mean reward per episode: 5495.79
Num timesteps: 16356000
Best mean reward: 5646.98 - Last mean reward per episode: 5496.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.17e+03    |
|    ep_rew_mean          | 5501.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 666         |
|    time_elapsed         | 193332      |
|    total_timesteps      | 16367616    |
| train/                  |             |
|    approx_kl            | 0.013088648 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 334         |
|    n_updates            | 6650        |
|    policy_gradient_loss | -2.2e-05    |
|    value_loss           | 372         |
-----------------------------------------
Num timesteps: 16368000
Best mean reward: 5646.98 - Last mean reward per episode: 5532.87
Num timesteps: 16380000
Best mean reward: 5646.98 - Last mean reward per episode: 5509.78
Num timesteps: 16392000
Best mean reward: 5646.98 - Last mean reward per episode: 5468.85
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.2e+03    |
|    ep_rew_mean          | 5468.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 667        |
|    time_elapsed         | 193621     |
|    total_timesteps      | 16392192   |
| train/                  |            |
|    approx_kl            | 0.00902428 |
|    clip_fraction        | 0.077      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.548     |
|    explained_variance   | 0.983      |
|    learning_rate        | 3e-06      |
|    loss                 | 85.9       |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.000119  |
|    value_loss           | 385        |
----------------------------------------
Num timesteps: 16404000
Best mean reward: 5646.98 - Last mean reward per episode: 5459.98
Num timesteps: 16416000
Best mean reward: 5646.98 - Last mean reward per episode: 5485.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.22e+03   |
|    ep_rew_mean          | 5485.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 668        |
|    time_elapsed         | 193912     |
|    total_timesteps      | 16416768   |
| train/                  |            |
|    approx_kl            | 0.01584437 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.841     |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 619        |
|    n_updates            | 6670       |
|    policy_gradient_loss | 0.000265   |
|    value_loss           | 592        |
----------------------------------------
Num timesteps: 16428000
Best mean reward: 5646.98 - Last mean reward per episode: 5507.19
Num timesteps: 16440000
Best mean reward: 5646.98 - Last mean reward per episode: 5507.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.23e+03   |
|    ep_rew_mean          | 5507.19    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 669        |
|    time_elapsed         | 194209     |
|    total_timesteps      | 16441344   |
| train/                  |            |
|    approx_kl            | 0.01829361 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.986     |
|    explained_variance   | 0.981      |
|    learning_rate        | 3e-06      |
|    loss                 | 101        |
|    n_updates            | 6680       |
|    policy_gradient_loss | -0.00153   |
|    value_loss           | 333        |
----------------------------------------
Num timesteps: 16452000
Best mean reward: 5646.98 - Last mean reward per episode: 5560.85
Num timesteps: 16464000
Best mean reward: 5646.98 - Last mean reward per episode: 5619.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.36e+03   |
|    ep_rew_mean          | 5619.43    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 670        |
|    time_elapsed         | 194499     |
|    total_timesteps      | 16465920   |
| train/                  |            |
|    approx_kl            | 0.01256796 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.89      |
|    explained_variance   | 0.982      |
|    learning_rate        | 3e-06      |
|    loss                 | 163        |
|    n_updates            | 6690       |
|    policy_gradient_loss | 0.000422   |
|    value_loss           | 381        |
----------------------------------------
Num timesteps: 16476000
Best mean reward: 5646.98 - Last mean reward per episode: 5617.42
Num timesteps: 16488000
Best mean reward: 5646.98 - Last mean reward per episode: 5664.81
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.32e+03    |
|    ep_rew_mean          | 5662.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 671         |
|    time_elapsed         | 194791      |
|    total_timesteps      | 16490496    |
| train/                  |             |
|    approx_kl            | 0.009933303 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 106         |
|    n_updates            | 6700        |
|    policy_gradient_loss | -0.000183   |
|    value_loss           | 376         |
-----------------------------------------
Num timesteps: 16500000
Best mean reward: 5664.81 - Last mean reward per episode: 5647.27
Num timesteps: 16512000
Best mean reward: 5664.81 - Last mean reward per episode: 5670.01
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.09e+03    |
|    ep_rew_mean          | 5667.28     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 672         |
|    time_elapsed         | 195089      |
|    total_timesteps      | 16515072    |
| train/                  |             |
|    approx_kl            | 0.010361589 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.566      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 187         |
|    n_updates            | 6710        |
|    policy_gradient_loss | -0.000863   |
|    value_loss           | 441         |
-----------------------------------------
Num timesteps: 16524000
Best mean reward: 5670.01 - Last mean reward per episode: 5629.40
Num timesteps: 16536000
Best mean reward: 5670.01 - Last mean reward per episode: 5622.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.94e+03    |
|    ep_rew_mean          | 5624.49     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 673         |
|    time_elapsed         | 195381      |
|    total_timesteps      | 16539648    |
| train/                  |             |
|    approx_kl            | 0.009819898 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 408         |
|    n_updates            | 6720        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 16548000
Best mean reward: 5670.01 - Last mean reward per episode: 5626.17
Num timesteps: 16560000
Best mean reward: 5670.01 - Last mean reward per episode: 5612.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.85e+03    |
|    ep_rew_mean          | 5608.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 674         |
|    time_elapsed         | 195666      |
|    total_timesteps      | 16564224    |
| train/                  |             |
|    approx_kl            | 0.011690737 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 82.8        |
|    n_updates            | 6730        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 16572000
Best mean reward: 5670.01 - Last mean reward per episode: 5599.85
Num timesteps: 16584000
Best mean reward: 5670.01 - Last mean reward per episode: 5595.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8e+03     |
|    ep_rew_mean          | 5595.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 675         |
|    time_elapsed         | 195955      |
|    total_timesteps      | 16588800    |
| train/                  |             |
|    approx_kl            | 0.010166747 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 55          |
|    n_updates            | 6740        |
|    policy_gradient_loss | 0.000944    |
|    value_loss           | 336         |
-----------------------------------------
Num timesteps: 16596000
Best mean reward: 5670.01 - Last mean reward per episode: 5592.83
Num timesteps: 16608000
Best mean reward: 5670.01 - Last mean reward per episode: 5573.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.84e+03    |
|    ep_rew_mean          | 5574.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 676         |
|    time_elapsed         | 196242      |
|    total_timesteps      | 16613376    |
| train/                  |             |
|    approx_kl            | 0.007896037 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 65          |
|    n_updates            | 6750        |
|    policy_gradient_loss | -0.00155    |
|    value_loss           | 325         |
-----------------------------------------
Num timesteps: 16620000
Best mean reward: 5670.01 - Last mean reward per episode: 5559.29
Num timesteps: 16632000
Best mean reward: 5670.01 - Last mean reward per episode: 5557.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.93e+03     |
|    ep_rew_mean          | 5560.59      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 677          |
|    time_elapsed         | 196532       |
|    total_timesteps      | 16637952     |
| train/                  |              |
|    approx_kl            | 0.0152437985 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.762       |
|    explained_variance   | 0.982        |
|    learning_rate        | 3e-06        |
|    loss                 | 242          |
|    n_updates            | 6760         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 369          |
------------------------------------------
Num timesteps: 16644000
Best mean reward: 5670.01 - Last mean reward per episode: 5572.24
Num timesteps: 16656000
Best mean reward: 5670.01 - Last mean reward per episode: 5502.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.01e+03    |
|    ep_rew_mean          | 5501.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 678         |
|    time_elapsed         | 196815      |
|    total_timesteps      | 16662528    |
| train/                  |             |
|    approx_kl            | 0.009094778 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 65.9        |
|    n_updates            | 6770        |
|    policy_gradient_loss | 0.000248    |
|    value_loss           | 299         |
-----------------------------------------
Num timesteps: 16668000
Best mean reward: 5670.01 - Last mean reward per episode: 5511.15
Num timesteps: 16680000
Best mean reward: 5670.01 - Last mean reward per episode: 5505.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.98e+03    |
|    ep_rew_mean          | 5513.92     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 679         |
|    time_elapsed         | 197106      |
|    total_timesteps      | 16687104    |
| train/                  |             |
|    approx_kl            | 0.012240679 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 101         |
|    n_updates            | 6780        |
|    policy_gradient_loss | -0.000327   |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 16692000
Best mean reward: 5670.01 - Last mean reward per episode: 5513.92
Num timesteps: 16704000
Best mean reward: 5670.01 - Last mean reward per episode: 5535.82
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.99e+03   |
|    ep_rew_mean          | 5540.07    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 680        |
|    time_elapsed         | 197393     |
|    total_timesteps      | 16711680   |
| train/                  |            |
|    approx_kl            | 0.01815938 |
|    clip_fraction        | 0.0921     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.587     |
|    explained_variance   | 0.977      |
|    learning_rate        | 3e-06      |
|    loss                 | 135        |
|    n_updates            | 6790       |
|    policy_gradient_loss | 0.00119    |
|    value_loss           | 492        |
----------------------------------------
Num timesteps: 16716000
Best mean reward: 5670.01 - Last mean reward per episode: 5537.81
Num timesteps: 16728000
Best mean reward: 5670.01 - Last mean reward per episode: 5551.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.99e+03    |
|    ep_rew_mean          | 5554.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 681         |
|    time_elapsed         | 197687      |
|    total_timesteps      | 16736256    |
| train/                  |             |
|    approx_kl            | 0.012036532 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.543      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 68.3        |
|    n_updates            | 6800        |
|    policy_gradient_loss | -0.000606   |
|    value_loss           | 438         |
-----------------------------------------
Num timesteps: 16740000
Best mean reward: 5670.01 - Last mean reward per episode: 5532.84
Num timesteps: 16752000
Best mean reward: 5670.01 - Last mean reward per episode: 5542.39
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.91e+03     |
|    ep_rew_mean          | 5578.36      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 682          |
|    time_elapsed         | 197973       |
|    total_timesteps      | 16760832     |
| train/                  |              |
|    approx_kl            | 0.0067891106 |
|    clip_fraction        | 0.0678       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.508       |
|    explained_variance   | 0.985        |
|    learning_rate        | 3e-06        |
|    loss                 | 440          |
|    n_updates            | 6810         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 376          |
------------------------------------------
Num timesteps: 16764000
Best mean reward: 5670.01 - Last mean reward per episode: 5540.33
Num timesteps: 16776000
Best mean reward: 5670.01 - Last mean reward per episode: 5547.01
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.84e+03   |
|    ep_rew_mean          | 5550.32    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 683        |
|    time_elapsed         | 198267     |
|    total_timesteps      | 16785408   |
| train/                  |            |
|    approx_kl            | 0.01010191 |
|    clip_fraction        | 0.0738     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.509     |
|    explained_variance   | 0.985      |
|    learning_rate        | 3e-06      |
|    loss                 | 201        |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.00127   |
|    value_loss           | 349        |
----------------------------------------
Num timesteps: 16788000
Best mean reward: 5670.01 - Last mean reward per episode: 5550.32
Num timesteps: 16800000
Best mean reward: 5670.01 - Last mean reward per episode: 5552.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.84e+03    |
|    ep_rew_mean          | 5537.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 684         |
|    time_elapsed         | 198555      |
|    total_timesteps      | 16809984    |
| train/                  |             |
|    approx_kl            | 0.012344386 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 173         |
|    n_updates            | 6830        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 16812000
Best mean reward: 5670.01 - Last mean reward per episode: 5543.52
Num timesteps: 16824000
Best mean reward: 5670.01 - Last mean reward per episode: 5580.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.85e+03    |
|    ep_rew_mean          | 5582.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 685         |
|    time_elapsed         | 198850      |
|    total_timesteps      | 16834560    |
| train/                  |             |
|    approx_kl            | 0.009274942 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 97.5        |
|    n_updates            | 6840        |
|    policy_gradient_loss | -9.51e-05   |
|    value_loss           | 382         |
-----------------------------------------
Num timesteps: 16836000
Best mean reward: 5670.01 - Last mean reward per episode: 5583.14
Num timesteps: 16848000
Best mean reward: 5670.01 - Last mean reward per episode: 5555.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.97e+03    |
|    ep_rew_mean          | 5567.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 686         |
|    time_elapsed         | 199142      |
|    total_timesteps      | 16859136    |
| train/                  |             |
|    approx_kl            | 0.008342107 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.444      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 71.3        |
|    n_updates            | 6850        |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 365         |
-----------------------------------------
Num timesteps: 16860000
Best mean reward: 5670.01 - Last mean reward per episode: 5567.38
Num timesteps: 16872000
Best mean reward: 5670.01 - Last mean reward per episode: 5582.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.93e+03    |
|    ep_rew_mean          | 5582.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 687         |
|    time_elapsed         | 199432      |
|    total_timesteps      | 16883712    |
| train/                  |             |
|    approx_kl            | 0.009243036 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 166         |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.000185   |
|    value_loss           | 354         |
-----------------------------------------
Num timesteps: 16884000
Best mean reward: 5670.01 - Last mean reward per episode: 5582.24
Num timesteps: 16896000
Best mean reward: 5670.01 - Last mean reward per episode: 5608.30
Num timesteps: 16908000
Best mean reward: 5670.01 - Last mean reward per episode: 5602.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8e+03     |
|    ep_rew_mean          | 5602.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 688         |
|    time_elapsed         | 199722      |
|    total_timesteps      | 16908288    |
| train/                  |             |
|    approx_kl            | 0.008617258 |
|    clip_fraction        | 0.0736      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 235         |
|    n_updates            | 6870        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 317         |
-----------------------------------------
Num timesteps: 16920000
Best mean reward: 5670.01 - Last mean reward per episode: 5630.95
Num timesteps: 16932000
Best mean reward: 5670.01 - Last mean reward per episode: 5685.31
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.71e+03    |
|    ep_rew_mean          | 5681.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 689         |
|    time_elapsed         | 200016      |
|    total_timesteps      | 16932864    |
| train/                  |             |
|    approx_kl            | 0.010374378 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.677      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 291         |
|    n_updates            | 6880        |
|    policy_gradient_loss | 4.37e-05    |
|    value_loss           | 393         |
-----------------------------------------
Num timesteps: 16944000
Best mean reward: 5685.31 - Last mean reward per episode: 5716.58
Saving new best model to tmp/best_model
Num timesteps: 16956000
Best mean reward: 5716.58 - Last mean reward per episode: 5689.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.76e+03     |
|    ep_rew_mean          | 5689.95      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 690          |
|    time_elapsed         | 200305       |
|    total_timesteps      | 16957440     |
| train/                  |              |
|    approx_kl            | 0.0065951124 |
|    clip_fraction        | 0.0631       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.983        |
|    learning_rate        | 3e-06        |
|    loss                 | 163          |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 414          |
------------------------------------------
Num timesteps: 16968000
Best mean reward: 5716.58 - Last mean reward per episode: 5708.70
Num timesteps: 16980000
Best mean reward: 5716.58 - Last mean reward per episode: 5720.91
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.71e+03    |
|    ep_rew_mean          | 5720.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 691         |
|    time_elapsed         | 200598      |
|    total_timesteps      | 16982016    |
| train/                  |             |
|    approx_kl            | 0.010393458 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 131         |
|    n_updates            | 6900        |
|    policy_gradient_loss | -0.000305   |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 16992000
Best mean reward: 5720.91 - Last mean reward per episode: 5648.68
Num timesteps: 17004000
Best mean reward: 5720.91 - Last mean reward per episode: 5657.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.68e+03    |
|    ep_rew_mean          | 5657.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 692         |
|    time_elapsed         | 200887      |
|    total_timesteps      | 17006592    |
| train/                  |             |
|    approx_kl            | 0.014046844 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 562         |
|    n_updates            | 6910        |
|    policy_gradient_loss | 0.000484    |
|    value_loss           | 466         |
-----------------------------------------
Num timesteps: 17016000
Best mean reward: 5720.91 - Last mean reward per episode: 5615.55
Num timesteps: 17028000
Best mean reward: 5720.91 - Last mean reward per episode: 5647.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.67e+03    |
|    ep_rew_mean          | 5620.85     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 693         |
|    time_elapsed         | 201172      |
|    total_timesteps      | 17031168    |
| train/                  |             |
|    approx_kl            | 0.017796054 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.619      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 128         |
|    n_updates            | 6920        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 17040000
Best mean reward: 5720.91 - Last mean reward per episode: 5644.94
Num timesteps: 17052000
Best mean reward: 5720.91 - Last mean reward per episode: 5658.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.63e+03    |
|    ep_rew_mean          | 5658.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 694         |
|    time_elapsed         | 201461      |
|    total_timesteps      | 17055744    |
| train/                  |             |
|    approx_kl            | 0.019601906 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 811         |
|    n_updates            | 6930        |
|    policy_gradient_loss | 0.00222     |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 17064000
Best mean reward: 5720.91 - Last mean reward per episode: 5651.04
Num timesteps: 17076000
Best mean reward: 5720.91 - Last mean reward per episode: 5639.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.64e+03    |
|    ep_rew_mean          | 5630.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 695         |
|    time_elapsed         | 201756      |
|    total_timesteps      | 17080320    |
| train/                  |             |
|    approx_kl            | 0.013804016 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 193         |
|    n_updates            | 6940        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 417         |
-----------------------------------------
Num timesteps: 17088000
Best mean reward: 5720.91 - Last mean reward per episode: 5634.12
Num timesteps: 17100000
Best mean reward: 5720.91 - Last mean reward per episode: 5644.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 5651.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 696         |
|    time_elapsed         | 202044      |
|    total_timesteps      | 17104896    |
| train/                  |             |
|    approx_kl            | 0.009730113 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 76.6        |
|    n_updates            | 6950        |
|    policy_gradient_loss | -0.000207   |
|    value_loss           | 406         |
-----------------------------------------
Num timesteps: 17112000
Best mean reward: 5720.91 - Last mean reward per episode: 5655.20
Num timesteps: 17124000
Best mean reward: 5720.91 - Last mean reward per episode: 5662.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5665.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 697         |
|    time_elapsed         | 202334      |
|    total_timesteps      | 17129472    |
| train/                  |             |
|    approx_kl            | 0.008656793 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 65.7        |
|    n_updates            | 6960        |
|    policy_gradient_loss | 2.43e-05    |
|    value_loss           | 469         |
-----------------------------------------
Num timesteps: 17136000
Best mean reward: 5720.91 - Last mean reward per episode: 5663.20
Num timesteps: 17148000
Best mean reward: 5720.91 - Last mean reward per episode: 5668.54
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.56e+03     |
|    ep_rew_mean          | 5670.75      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 698          |
|    time_elapsed         | 202622       |
|    total_timesteps      | 17154048     |
| train/                  |              |
|    approx_kl            | 0.0073684617 |
|    clip_fraction        | 0.062        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.983        |
|    learning_rate        | 3e-06        |
|    loss                 | 274          |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 404          |
------------------------------------------
Num timesteps: 17160000
Best mean reward: 5720.91 - Last mean reward per episode: 5676.87
Num timesteps: 17172000
Best mean reward: 5720.91 - Last mean reward per episode: 5671.82
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.55e+03     |
|    ep_rew_mean          | 5669.43      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 699          |
|    time_elapsed         | 202906       |
|    total_timesteps      | 17178624     |
| train/                  |              |
|    approx_kl            | 0.0108687235 |
|    clip_fraction        | 0.0832       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.983        |
|    learning_rate        | 3e-06        |
|    loss                 | 247          |
|    n_updates            | 6980         |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 397          |
------------------------------------------
Num timesteps: 17184000
Best mean reward: 5720.91 - Last mean reward per episode: 5674.40
Num timesteps: 17196000
Best mean reward: 5720.91 - Last mean reward per episode: 5680.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 5713.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 700         |
|    time_elapsed         | 203193      |
|    total_timesteps      | 17203200    |
| train/                  |             |
|    approx_kl            | 0.013844502 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 289         |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 385         |
-----------------------------------------
Num timesteps: 17208000
Best mean reward: 5720.91 - Last mean reward per episode: 5713.70
Num timesteps: 17220000
Best mean reward: 5720.91 - Last mean reward per episode: 5706.74
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.55e+03   |
|    ep_rew_mean          | 5711.42    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 701        |
|    time_elapsed         | 203478     |
|    total_timesteps      | 17227776   |
| train/                  |            |
|    approx_kl            | 0.00952297 |
|    clip_fraction        | 0.0778     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.545     |
|    explained_variance   | 0.981      |
|    learning_rate        | 3e-06      |
|    loss                 | 143        |
|    n_updates            | 7000       |
|    policy_gradient_loss | 0.000251   |
|    value_loss           | 361        |
----------------------------------------
Num timesteps: 17232000
Best mean reward: 5720.91 - Last mean reward per episode: 5714.24
Num timesteps: 17244000
Best mean reward: 5720.91 - Last mean reward per episode: 5680.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.54e+03    |
|    ep_rew_mean          | 5680.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 702         |
|    time_elapsed         | 203768      |
|    total_timesteps      | 17252352    |
| train/                  |             |
|    approx_kl            | 0.008051612 |
|    clip_fraction        | 0.0601      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 74.2        |
|    n_updates            | 7010        |
|    policy_gradient_loss | -0.000637   |
|    value_loss           | 394         |
-----------------------------------------
Num timesteps: 17256000
Best mean reward: 5720.91 - Last mean reward per episode: 5680.39
Num timesteps: 17268000
Best mean reward: 5720.91 - Last mean reward per episode: 5726.49
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5731.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 703         |
|    time_elapsed         | 204062      |
|    total_timesteps      | 17276928    |
| train/                  |             |
|    approx_kl            | 0.024294203 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 557         |
|    n_updates            | 7020        |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 17280000
Best mean reward: 5726.49 - Last mean reward per episode: 5738.78
Saving new best model to tmp/best_model
Num timesteps: 17292000
Best mean reward: 5738.78 - Last mean reward per episode: 5764.52
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.57e+03    |
|    ep_rew_mean          | 5766.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 704         |
|    time_elapsed         | 204354      |
|    total_timesteps      | 17301504    |
| train/                  |             |
|    approx_kl            | 0.010391281 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 196         |
|    n_updates            | 7030        |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 338         |
-----------------------------------------
Num timesteps: 17304000
Best mean reward: 5764.52 - Last mean reward per episode: 5766.82
Saving new best model to tmp/best_model
Num timesteps: 17316000
Best mean reward: 5766.82 - Last mean reward per episode: 5751.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5756.76     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 705         |
|    time_elapsed         | 204645      |
|    total_timesteps      | 17326080    |
| train/                  |             |
|    approx_kl            | 0.022688411 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 7040        |
|    policy_gradient_loss | 0.000101    |
|    value_loss           | 336         |
-----------------------------------------
Num timesteps: 17328000
Best mean reward: 5766.82 - Last mean reward per episode: 5756.76
Num timesteps: 17340000
Best mean reward: 5766.82 - Last mean reward per episode: 5765.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5773.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 706         |
|    time_elapsed         | 204939      |
|    total_timesteps      | 17350656    |
| train/                  |             |
|    approx_kl            | 0.011564682 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 106         |
|    n_updates            | 7050        |
|    policy_gradient_loss | -0.00063    |
|    value_loss           | 267         |
-----------------------------------------
Num timesteps: 17352000
Best mean reward: 5766.82 - Last mean reward per episode: 5773.29
Saving new best model to tmp/best_model
Num timesteps: 17364000
Best mean reward: 5773.29 - Last mean reward per episode: 5783.04
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.59e+03   |
|    ep_rew_mean          | 5791.44    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 707        |
|    time_elapsed         | 205228     |
|    total_timesteps      | 17375232   |
| train/                  |            |
|    approx_kl            | 0.08612486 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.982      |
|    learning_rate        | 3e-06      |
|    loss                 | 86.1       |
|    n_updates            | 7060       |
|    policy_gradient_loss | -0.00477   |
|    value_loss           | 211        |
----------------------------------------
Num timesteps: 17376000
Best mean reward: 5783.04 - Last mean reward per episode: 5791.44
Saving new best model to tmp/best_model
Num timesteps: 17388000
Best mean reward: 5791.44 - Last mean reward per episode: 5797.00
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.68e+03    |
|    ep_rew_mean          | 5769.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 708         |
|    time_elapsed         | 205517      |
|    total_timesteps      | 17399808    |
| train/                  |             |
|    approx_kl            | 0.022878787 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 64.7        |
|    n_updates            | 7070        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 170         |
-----------------------------------------
Num timesteps: 17400000
Best mean reward: 5797.00 - Last mean reward per episode: 5769.08
Num timesteps: 17412000
Best mean reward: 5797.00 - Last mean reward per episode: 5745.56
Num timesteps: 17424000
Best mean reward: 5797.00 - Last mean reward per episode: 5708.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.93e+03    |
|    ep_rew_mean          | 5708.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 709         |
|    time_elapsed         | 205812      |
|    total_timesteps      | 17424384    |
| train/                  |             |
|    approx_kl            | 0.010479095 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 39.4        |
|    n_updates            | 7080        |
|    policy_gradient_loss | -0.000239   |
|    value_loss           | 230         |
-----------------------------------------
Num timesteps: 17436000
Best mean reward: 5797.00 - Last mean reward per episode: 5651.78
Num timesteps: 17448000
Best mean reward: 5797.00 - Last mean reward per episode: 5608.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.05e+03    |
|    ep_rew_mean          | 5608.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 710         |
|    time_elapsed         | 206102      |
|    total_timesteps      | 17448960    |
| train/                  |             |
|    approx_kl            | 0.015980216 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 13.6        |
|    n_updates            | 7090        |
|    policy_gradient_loss | 0.00022     |
|    value_loss           | 219         |
-----------------------------------------
Num timesteps: 17460000
Best mean reward: 5797.00 - Last mean reward per episode: 5613.36
Num timesteps: 17472000
Best mean reward: 5797.00 - Last mean reward per episode: 5619.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.05e+03    |
|    ep_rew_mean          | 5616.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 711         |
|    time_elapsed         | 206395      |
|    total_timesteps      | 17473536    |
| train/                  |             |
|    approx_kl            | 0.009925262 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.991       |
|    learning_rate        | 3e-06       |
|    loss                 | 304         |
|    n_updates            | 7100        |
|    policy_gradient_loss | -0.000897   |
|    value_loss           | 220         |
-----------------------------------------
Num timesteps: 17484000
Best mean reward: 5797.00 - Last mean reward per episode: 5614.71
Num timesteps: 17496000
Best mean reward: 5797.00 - Last mean reward per episode: 5622.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.1e+03     |
|    ep_rew_mean          | 5622.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 712         |
|    time_elapsed         | 206683      |
|    total_timesteps      | 17498112    |
| train/                  |             |
|    approx_kl            | 0.015494176 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 24          |
|    n_updates            | 7110        |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 255         |
-----------------------------------------
Num timesteps: 17508000
Best mean reward: 5797.00 - Last mean reward per episode: 5585.87
Num timesteps: 17520000
Best mean reward: 5797.00 - Last mean reward per episode: 5551.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.22e+03    |
|    ep_rew_mean          | 5551.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 713         |
|    time_elapsed         | 206973      |
|    total_timesteps      | 17522688    |
| train/                  |             |
|    approx_kl            | 0.009177352 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.989       |
|    learning_rate        | 3e-06       |
|    loss                 | 59.7        |
|    n_updates            | 7120        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 294         |
-----------------------------------------
Num timesteps: 17532000
Best mean reward: 5797.00 - Last mean reward per episode: 5508.05
Num timesteps: 17544000
Best mean reward: 5797.00 - Last mean reward per episode: 5503.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.3e+03     |
|    ep_rew_mean          | 5503.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 714         |
|    time_elapsed         | 207269      |
|    total_timesteps      | 17547264    |
| train/                  |             |
|    approx_kl            | 0.011539939 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 111         |
|    n_updates            | 7130        |
|    policy_gradient_loss | -0.00192    |
|    value_loss           | 330         |
-----------------------------------------
Num timesteps: 17556000
Best mean reward: 5797.00 - Last mean reward per episode: 5511.52
Num timesteps: 17568000
Best mean reward: 5797.00 - Last mean reward per episode: 5519.96
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.28e+03   |
|    ep_rew_mean          | 5519.96    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 715        |
|    time_elapsed         | 207556     |
|    total_timesteps      | 17571840   |
| train/                  |            |
|    approx_kl            | 0.00865689 |
|    clip_fraction        | 0.0798     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.688     |
|    explained_variance   | 0.986      |
|    learning_rate        | 3e-06      |
|    loss                 | 189        |
|    n_updates            | 7140       |
|    policy_gradient_loss | -8.46e-05  |
|    value_loss           | 345        |
----------------------------------------
Num timesteps: 17580000
Best mean reward: 5797.00 - Last mean reward per episode: 5585.14
Num timesteps: 17592000
Best mean reward: 5797.00 - Last mean reward per episode: 5623.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.31e+03    |
|    ep_rew_mean          | 5625.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 716         |
|    time_elapsed         | 207842      |
|    total_timesteps      | 17596416    |
| train/                  |             |
|    approx_kl            | 0.009632862 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.987       |
|    learning_rate        | 3e-06       |
|    loss                 | 131         |
|    n_updates            | 7150        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 318         |
-----------------------------------------
Num timesteps: 17604000
Best mean reward: 5797.00 - Last mean reward per episode: 5587.28
Num timesteps: 17616000
Best mean reward: 5797.00 - Last mean reward per episode: 5579.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.4e+03     |
|    ep_rew_mean          | 5577.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 717         |
|    time_elapsed         | 208138      |
|    total_timesteps      | 17620992    |
| train/                  |             |
|    approx_kl            | 0.009924381 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 235         |
|    n_updates            | 7160        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 347         |
-----------------------------------------
Num timesteps: 17628000
Best mean reward: 5797.00 - Last mean reward per episode: 5578.66
Num timesteps: 17640000
Best mean reward: 5797.00 - Last mean reward per episode: 5576.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.4e+03     |
|    ep_rew_mean          | 5587.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 718         |
|    time_elapsed         | 208424      |
|    total_timesteps      | 17645568    |
| train/                  |             |
|    approx_kl            | 0.010765026 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 90          |
|    n_updates            | 7170        |
|    policy_gradient_loss | 0.000311    |
|    value_loss           | 307         |
-----------------------------------------
Num timesteps: 17652000
Best mean reward: 5797.00 - Last mean reward per episode: 5587.17
Num timesteps: 17664000
Best mean reward: 5797.00 - Last mean reward per episode: 5514.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.48e+03    |
|    ep_rew_mean          | 5521.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 719         |
|    time_elapsed         | 208706      |
|    total_timesteps      | 17670144    |
| train/                  |             |
|    approx_kl            | 0.013076395 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.989       |
|    learning_rate        | 3e-06       |
|    loss                 | 303         |
|    n_updates            | 7180        |
|    policy_gradient_loss | 0.000364    |
|    value_loss           | 267         |
-----------------------------------------
Num timesteps: 17676000
Best mean reward: 5797.00 - Last mean reward per episode: 5525.81
Num timesteps: 17688000
Best mean reward: 5797.00 - Last mean reward per episode: 5455.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.57e+03    |
|    ep_rew_mean          | 5451.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 720         |
|    time_elapsed         | 208991      |
|    total_timesteps      | 17694720    |
| train/                  |             |
|    approx_kl            | 0.012235214 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 130         |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 283         |
-----------------------------------------
Num timesteps: 17700000
Best mean reward: 5797.00 - Last mean reward per episode: 5449.74
Num timesteps: 17712000
Best mean reward: 5797.00 - Last mean reward per episode: 5422.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.62e+03    |
|    ep_rew_mean          | 5379.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 721         |
|    time_elapsed         | 209280      |
|    total_timesteps      | 17719296    |
| train/                  |             |
|    approx_kl            | 0.009677264 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.992       |
|    learning_rate        | 3e-06       |
|    loss                 | 61.5        |
|    n_updates            | 7200        |
|    policy_gradient_loss | -0.000838   |
|    value_loss           | 221         |
-----------------------------------------
Num timesteps: 17724000
Best mean reward: 5797.00 - Last mean reward per episode: 5380.24
Num timesteps: 17736000
Best mean reward: 5797.00 - Last mean reward per episode: 5383.91
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.49e+03   |
|    ep_rew_mean          | 5391.98    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 722        |
|    time_elapsed         | 209573     |
|    total_timesteps      | 17743872   |
| train/                  |            |
|    approx_kl            | 0.02323333 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.986      |
|    learning_rate        | 3e-06      |
|    loss                 | 109        |
|    n_updates            | 7210       |
|    policy_gradient_loss | -0.00588   |
|    value_loss           | 287        |
----------------------------------------
Num timesteps: 17748000
Best mean reward: 5797.00 - Last mean reward per episode: 5474.89
Num timesteps: 17760000
Best mean reward: 5797.00 - Last mean reward per episode: 5482.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.4e+03     |
|    ep_rew_mean          | 5446.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 723         |
|    time_elapsed         | 209861      |
|    total_timesteps      | 17768448    |
| train/                  |             |
|    approx_kl            | 0.009111475 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 107         |
|    n_updates            | 7220        |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 284         |
-----------------------------------------
Num timesteps: 17772000
Best mean reward: 5797.00 - Last mean reward per episode: 5449.89
Num timesteps: 17784000
Best mean reward: 5797.00 - Last mean reward per episode: 5442.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.31e+03    |
|    ep_rew_mean          | 5403.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 724         |
|    time_elapsed         | 210146      |
|    total_timesteps      | 17793024    |
| train/                  |             |
|    approx_kl            | 0.009679625 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.948      |
|    explained_variance   | 0.989       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 7230        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 290         |
-----------------------------------------
Num timesteps: 17796000
Best mean reward: 5797.00 - Last mean reward per episode: 5401.34
Num timesteps: 17808000
Best mean reward: 5797.00 - Last mean reward per episode: 5391.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.15e+03    |
|    ep_rew_mean          | 5463.47     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 725         |
|    time_elapsed         | 210431      |
|    total_timesteps      | 17817600    |
| train/                  |             |
|    approx_kl            | 0.008735463 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 179         |
|    n_updates            | 7240        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 360         |
-----------------------------------------
Num timesteps: 17820000
Best mean reward: 5797.00 - Last mean reward per episode: 5463.47
Num timesteps: 17832000
Best mean reward: 5797.00 - Last mean reward per episode: 5463.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.06e+03    |
|    ep_rew_mean          | 5515.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 726         |
|    time_elapsed         | 210715      |
|    total_timesteps      | 17842176    |
| train/                  |             |
|    approx_kl            | 0.011122539 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 220         |
|    n_updates            | 7250        |
|    policy_gradient_loss | 0.000567    |
|    value_loss           | 384         |
-----------------------------------------
Num timesteps: 17844000
Best mean reward: 5797.00 - Last mean reward per episode: 5519.20
Num timesteps: 17856000
Best mean reward: 5797.00 - Last mean reward per episode: 5518.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.06e+03    |
|    ep_rew_mean          | 5513.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 727         |
|    time_elapsed         | 211000      |
|    total_timesteps      | 17866752    |
| train/                  |             |
|    approx_kl            | 0.008538182 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 193         |
|    n_updates            | 7260        |
|    policy_gradient_loss | -0.000129   |
|    value_loss           | 366         |
-----------------------------------------
Num timesteps: 17868000
Best mean reward: 5797.00 - Last mean reward per episode: 5506.06
Num timesteps: 17880000
Best mean reward: 5797.00 - Last mean reward per episode: 5475.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.05e+03    |
|    ep_rew_mean          | 5443.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 728         |
|    time_elapsed         | 211290      |
|    total_timesteps      | 17891328    |
| train/                  |             |
|    approx_kl            | 0.009211001 |
|    clip_fraction        | 0.07        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 176         |
|    n_updates            | 7270        |
|    policy_gradient_loss | 0.000219    |
|    value_loss           | 370         |
-----------------------------------------
Num timesteps: 17892000
Best mean reward: 5797.00 - Last mean reward per episode: 5443.50
Num timesteps: 17904000
Best mean reward: 5797.00 - Last mean reward per episode: 5452.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.97e+03    |
|    ep_rew_mean          | 5451.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 729         |
|    time_elapsed         | 211574      |
|    total_timesteps      | 17915904    |
| train/                  |             |
|    approx_kl            | 0.011041533 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.978       |
|    learning_rate        | 3e-06       |
|    loss                 | 422         |
|    n_updates            | 7280        |
|    policy_gradient_loss | 0.00204     |
|    value_loss           | 430         |
-----------------------------------------
Num timesteps: 17916000
Best mean reward: 5797.00 - Last mean reward per episode: 5451.03
Num timesteps: 17928000
Best mean reward: 5797.00 - Last mean reward per episode: 5364.18
Num timesteps: 17940000
Best mean reward: 5797.00 - Last mean reward per episode: 5443.28
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.86e+03     |
|    ep_rew_mean          | 5443.28      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 730          |
|    time_elapsed         | 211857       |
|    total_timesteps      | 17940480     |
| train/                  |              |
|    approx_kl            | 0.0150446715 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.567       |
|    explained_variance   | 0.973        |
|    learning_rate        | 3e-06        |
|    loss                 | 172          |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.000182    |
|    value_loss           | 485          |
------------------------------------------
Num timesteps: 17952000
Best mean reward: 5797.00 - Last mean reward per episode: 5520.55
Num timesteps: 17964000
Best mean reward: 5797.00 - Last mean reward per episode: 5552.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.74e+03    |
|    ep_rew_mean          | 5552.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 731         |
|    time_elapsed         | 212146      |
|    total_timesteps      | 17965056    |
| train/                  |             |
|    approx_kl            | 0.008118222 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.466      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 115         |
|    n_updates            | 7300        |
|    policy_gradient_loss | 0.000143    |
|    value_loss           | 388         |
-----------------------------------------
Num timesteps: 17976000
Best mean reward: 5797.00 - Last mean reward per episode: 5543.79
Num timesteps: 17988000
Best mean reward: 5797.00 - Last mean reward per episode: 5531.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.6e+03    |
|    ep_rew_mean          | 5534.08    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 732        |
|    time_elapsed         | 212431     |
|    total_timesteps      | 17989632   |
| train/                  |            |
|    approx_kl            | 0.01734954 |
|    clip_fraction        | 0.0881     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.49      |
|    explained_variance   | 0.981      |
|    learning_rate        | 3e-06      |
|    loss                 | 166        |
|    n_updates            | 7310       |
|    policy_gradient_loss | -0.00305   |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 18000000
Best mean reward: 5797.00 - Last mean reward per episode: 5523.83
Num timesteps: 18012000
Best mean reward: 5797.00 - Last mean reward per episode: 5590.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5590.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 733         |
|    time_elapsed         | 212731      |
|    total_timesteps      | 18014208    |
| train/                  |             |
|    approx_kl            | 0.015618796 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 345         |
|    n_updates            | 7320        |
|    policy_gradient_loss | -0.000492   |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 18024000
Best mean reward: 5797.00 - Last mean reward per episode: 5580.77
Num timesteps: 18036000
Best mean reward: 5797.00 - Last mean reward per episode: 5619.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5623.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 734         |
|    time_elapsed         | 213025      |
|    total_timesteps      | 18038784    |
| train/                  |             |
|    approx_kl            | 0.008132681 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.453      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 270         |
|    n_updates            | 7330        |
|    policy_gradient_loss | -0.00057    |
|    value_loss           | 374         |
-----------------------------------------
Num timesteps: 18048000
Best mean reward: 5797.00 - Last mean reward per episode: 5627.81
Num timesteps: 18060000
Best mean reward: 5797.00 - Last mean reward per episode: 5622.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5622.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 735         |
|    time_elapsed         | 213323      |
|    total_timesteps      | 18063360    |
| train/                  |             |
|    approx_kl            | 0.005765777 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 164         |
|    n_updates            | 7340        |
|    policy_gradient_loss | -0.000701   |
|    value_loss           | 377         |
-----------------------------------------
Num timesteps: 18072000
Best mean reward: 5797.00 - Last mean reward per episode: 5620.69
Num timesteps: 18084000
Best mean reward: 5797.00 - Last mean reward per episode: 5605.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.42e+03   |
|    ep_rew_mean          | 5604.89    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 736        |
|    time_elapsed         | 213624     |
|    total_timesteps      | 18087936   |
| train/                  |            |
|    approx_kl            | 0.00714927 |
|    clip_fraction        | 0.0622     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.983      |
|    learning_rate        | 3e-06      |
|    loss                 | 635        |
|    n_updates            | 7350       |
|    policy_gradient_loss | -0.00147   |
|    value_loss           | 411        |
----------------------------------------
Num timesteps: 18096000
Best mean reward: 5797.00 - Last mean reward per episode: 5607.12
Num timesteps: 18108000
Best mean reward: 5797.00 - Last mean reward per episode: 5609.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 5611.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 737         |
|    time_elapsed         | 213917      |
|    total_timesteps      | 18112512    |
| train/                  |             |
|    approx_kl            | 0.009872932 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 243         |
|    n_updates            | 7360        |
|    policy_gradient_loss | -0.000796   |
|    value_loss           | 451         |
-----------------------------------------
Num timesteps: 18120000
Best mean reward: 5797.00 - Last mean reward per episode: 5618.24
Num timesteps: 18132000
Best mean reward: 5797.00 - Last mean reward per episode: 5659.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.44e+03    |
|    ep_rew_mean          | 5686.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 738         |
|    time_elapsed         | 214205      |
|    total_timesteps      | 18137088    |
| train/                  |             |
|    approx_kl            | 0.011471182 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 7370        |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 333         |
-----------------------------------------
Num timesteps: 18144000
Best mean reward: 5797.00 - Last mean reward per episode: 5686.97
Num timesteps: 18156000
Best mean reward: 5797.00 - Last mean reward per episode: 5670.46
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.45e+03     |
|    ep_rew_mean          | 5638.59      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 739          |
|    time_elapsed         | 214495       |
|    total_timesteps      | 18161664     |
| train/                  |              |
|    approx_kl            | 0.0064062155 |
|    clip_fraction        | 0.0637       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.985        |
|    learning_rate        | 3e-06        |
|    loss                 | 225          |
|    n_updates            | 7380         |
|    policy_gradient_loss | -0.000929    |
|    value_loss           | 387          |
------------------------------------------
Num timesteps: 18168000
Best mean reward: 5797.00 - Last mean reward per episode: 5639.64
Num timesteps: 18180000
Best mean reward: 5797.00 - Last mean reward per episode: 5659.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5668.13     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 740         |
|    time_elapsed         | 214787      |
|    total_timesteps      | 18186240    |
| train/                  |             |
|    approx_kl            | 0.008968524 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 387         |
|    n_updates            | 7390        |
|    policy_gradient_loss | -0.000959   |
|    value_loss           | 465         |
-----------------------------------------
Num timesteps: 18192000
Best mean reward: 5797.00 - Last mean reward per episode: 5621.73
Num timesteps: 18204000
Best mean reward: 5797.00 - Last mean reward per episode: 5623.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5665.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 741         |
|    time_elapsed         | 215077      |
|    total_timesteps      | 18210816    |
| train/                  |             |
|    approx_kl            | 0.012929986 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 56.7        |
|    n_updates            | 7400        |
|    policy_gradient_loss | -0.000962   |
|    value_loss           | 384         |
-----------------------------------------
Num timesteps: 18216000
Best mean reward: 5797.00 - Last mean reward per episode: 5660.41
Num timesteps: 18228000
Best mean reward: 5797.00 - Last mean reward per episode: 5674.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5676.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 742         |
|    time_elapsed         | 215366      |
|    total_timesteps      | 18235392    |
| train/                  |             |
|    approx_kl            | 0.020878376 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.465      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 7410        |
|    policy_gradient_loss | -0.00272    |
|    value_loss           | 383         |
-----------------------------------------
Num timesteps: 18240000
Best mean reward: 5797.00 - Last mean reward per episode: 5717.77
Num timesteps: 18252000
Best mean reward: 5797.00 - Last mean reward per episode: 5771.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 5780.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 743         |
|    time_elapsed         | 215652      |
|    total_timesteps      | 18259968    |
| train/                  |             |
|    approx_kl            | 0.007271638 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 242         |
|    n_updates            | 7420        |
|    policy_gradient_loss | -0.00094    |
|    value_loss           | 342         |
-----------------------------------------
Num timesteps: 18264000
Best mean reward: 5797.00 - Last mean reward per episode: 5784.63
Num timesteps: 18276000
Best mean reward: 5797.00 - Last mean reward per episode: 5798.18
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.5e+03      |
|    ep_rew_mean          | 5783.61      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 744          |
|    time_elapsed         | 215937       |
|    total_timesteps      | 18284544     |
| train/                  |              |
|    approx_kl            | 0.0070651867 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.986        |
|    learning_rate        | 3e-06        |
|    loss                 | 288          |
|    n_updates            | 7430         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 374          |
------------------------------------------
Num timesteps: 18288000
Best mean reward: 5798.18 - Last mean reward per episode: 5783.61
Num timesteps: 18300000
Best mean reward: 5798.18 - Last mean reward per episode: 5788.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 5796.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 745         |
|    time_elapsed         | 216220      |
|    total_timesteps      | 18309120    |
| train/                  |             |
|    approx_kl            | 0.007628693 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 46.9        |
|    n_updates            | 7440        |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 372         |
-----------------------------------------
Num timesteps: 18312000
Best mean reward: 5798.18 - Last mean reward per episode: 5769.56
Num timesteps: 18324000
Best mean reward: 5798.18 - Last mean reward per episode: 5755.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 5764.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 746         |
|    time_elapsed         | 216507      |
|    total_timesteps      | 18333696    |
| train/                  |             |
|    approx_kl            | 0.010836472 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 143         |
|    n_updates            | 7450        |
|    policy_gradient_loss | -0.00012    |
|    value_loss           | 446         |
-----------------------------------------
Num timesteps: 18336000
Best mean reward: 5798.18 - Last mean reward per episode: 5760.49
Num timesteps: 18348000
Best mean reward: 5798.18 - Last mean reward per episode: 5762.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 5758.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 747         |
|    time_elapsed         | 216794      |
|    total_timesteps      | 18358272    |
| train/                  |             |
|    approx_kl            | 0.013321403 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 84.3        |
|    n_updates            | 7460        |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 430         |
-----------------------------------------
Num timesteps: 18360000
Best mean reward: 5798.18 - Last mean reward per episode: 5758.45
Num timesteps: 18372000
Best mean reward: 5798.18 - Last mean reward per episode: 5760.10
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.5e+03      |
|    ep_rew_mean          | 5775.69      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 748          |
|    time_elapsed         | 217081       |
|    total_timesteps      | 18382848     |
| train/                  |              |
|    approx_kl            | 0.0054722372 |
|    clip_fraction        | 0.0537       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.395       |
|    explained_variance   | 0.987        |
|    learning_rate        | 3e-06        |
|    loss                 | 38.6         |
|    n_updates            | 7470         |
|    policy_gradient_loss | 0.000142     |
|    value_loss           | 326          |
------------------------------------------
Num timesteps: 18384000
Best mean reward: 5798.18 - Last mean reward per episode: 5775.69
Num timesteps: 18396000
Best mean reward: 5798.18 - Last mean reward per episode: 5815.67
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.47e+03     |
|    ep_rew_mean          | 5827.69      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 749          |
|    time_elapsed         | 217372       |
|    total_timesteps      | 18407424     |
| train/                  |              |
|    approx_kl            | 0.0055437223 |
|    clip_fraction        | 0.0522       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.411       |
|    explained_variance   | 0.987        |
|    learning_rate        | 3e-06        |
|    loss                 | 247          |
|    n_updates            | 7480         |
|    policy_gradient_loss | -0.000677    |
|    value_loss           | 328          |
------------------------------------------
Num timesteps: 18408000
Best mean reward: 5815.67 - Last mean reward per episode: 5827.69
Saving new best model to tmp/best_model
Num timesteps: 18420000
Best mean reward: 5827.69 - Last mean reward per episode: 5835.17
Saving new best model to tmp/best_model
Num timesteps: 18432000
Best mean reward: 5835.17 - Last mean reward per episode: 5833.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 5833.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 750         |
|    time_elapsed         | 217674      |
|    total_timesteps      | 18432000    |
| train/                  |             |
|    approx_kl            | 0.009217725 |
|    clip_fraction        | 0.0738      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.446      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 398         |
-----------------------------------------
Num timesteps: 18444000
Best mean reward: 5835.17 - Last mean reward per episode: 5907.26
Saving new best model to tmp/best_model
Num timesteps: 18456000
Best mean reward: 5907.26 - Last mean reward per episode: 5780.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5780.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 751         |
|    time_elapsed         | 217972      |
|    total_timesteps      | 18456576    |
| train/                  |             |
|    approx_kl            | 0.015670693 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 58.1        |
|    n_updates            | 7500        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 391         |
-----------------------------------------
Num timesteps: 18468000
Best mean reward: 5907.26 - Last mean reward per episode: 5730.14
Num timesteps: 18480000
Best mean reward: 5907.26 - Last mean reward per episode: 5676.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5676.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 752         |
|    time_elapsed         | 218261      |
|    total_timesteps      | 18481152    |
| train/                  |             |
|    approx_kl            | 0.010285325 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 329         |
|    n_updates            | 7510        |
|    policy_gradient_loss | -0.000567   |
|    value_loss           | 452         |
-----------------------------------------
Num timesteps: 18492000
Best mean reward: 5907.26 - Last mean reward per episode: 5680.15
Num timesteps: 18504000
Best mean reward: 5907.26 - Last mean reward per episode: 5676.40
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.43e+03     |
|    ep_rew_mean          | 5678.87      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 753          |
|    time_elapsed         | 218561       |
|    total_timesteps      | 18505728     |
| train/                  |              |
|    approx_kl            | 0.0090480475 |
|    clip_fraction        | 0.0741       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.463       |
|    explained_variance   | 0.979        |
|    learning_rate        | 3e-06        |
|    loss                 | 110          |
|    n_updates            | 7520         |
|    policy_gradient_loss | -0.000595    |
|    value_loss           | 503          |
------------------------------------------
Num timesteps: 18516000
Best mean reward: 5907.26 - Last mean reward per episode: 5685.38
Num timesteps: 18528000
Best mean reward: 5907.26 - Last mean reward per episode: 5660.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5626.43     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 754         |
|    time_elapsed         | 218852      |
|    total_timesteps      | 18530304    |
| train/                  |             |
|    approx_kl            | 0.009805092 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 151         |
|    n_updates            | 7530        |
|    policy_gradient_loss | -0.000428   |
|    value_loss           | 429         |
-----------------------------------------
Num timesteps: 18540000
Best mean reward: 5907.26 - Last mean reward per episode: 5586.25
Num timesteps: 18552000
Best mean reward: 5907.26 - Last mean reward per episode: 5621.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 5621.63     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 755         |
|    time_elapsed         | 219146      |
|    total_timesteps      | 18554880    |
| train/                  |             |
|    approx_kl            | 0.013394099 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 152         |
|    n_updates            | 7540        |
|    policy_gradient_loss | 0.000959    |
|    value_loss           | 461         |
-----------------------------------------
Num timesteps: 18564000
Best mean reward: 5907.26 - Last mean reward per episode: 5643.52
Num timesteps: 18576000
Best mean reward: 5907.26 - Last mean reward per episode: 5646.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.4e+03      |
|    ep_rew_mean          | 5646.63      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 756          |
|    time_elapsed         | 219442       |
|    total_timesteps      | 18579456     |
| train/                  |              |
|    approx_kl            | 0.0063560866 |
|    clip_fraction        | 0.0598       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.421       |
|    explained_variance   | 0.987        |
|    learning_rate        | 3e-06        |
|    loss                 | 101          |
|    n_updates            | 7550         |
|    policy_gradient_loss | -0.000564    |
|    value_loss           | 327          |
------------------------------------------
Num timesteps: 18588000
Best mean reward: 5907.26 - Last mean reward per episode: 5642.02
Num timesteps: 18600000
Best mean reward: 5907.26 - Last mean reward per episode: 5617.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 5617.47     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 757         |
|    time_elapsed         | 219733      |
|    total_timesteps      | 18604032    |
| train/                  |             |
|    approx_kl            | 0.008851046 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.462      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 214         |
|    n_updates            | 7560        |
|    policy_gradient_loss | -0.000364   |
|    value_loss           | 394         |
-----------------------------------------
Num timesteps: 18612000
Best mean reward: 5907.26 - Last mean reward per episode: 5527.32
Num timesteps: 18624000
Best mean reward: 5907.26 - Last mean reward per episode: 5522.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 5519.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 758         |
|    time_elapsed         | 220031      |
|    total_timesteps      | 18628608    |
| train/                  |             |
|    approx_kl            | 0.010274432 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 37.3        |
|    n_updates            | 7570        |
|    policy_gradient_loss | -0.000245   |
|    value_loss           | 432         |
-----------------------------------------
Num timesteps: 18636000
Best mean reward: 5907.26 - Last mean reward per episode: 5522.01
Num timesteps: 18648000
Best mean reward: 5907.26 - Last mean reward per episode: 5507.59
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.41e+03   |
|    ep_rew_mean          | 5553.36    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 759        |
|    time_elapsed         | 220322     |
|    total_timesteps      | 18653184   |
| train/                  |            |
|    approx_kl            | 0.00997547 |
|    clip_fraction        | 0.081      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.616     |
|    explained_variance   | 0.985      |
|    learning_rate        | 3e-06      |
|    loss                 | 599        |
|    n_updates            | 7580       |
|    policy_gradient_loss | -0.00137   |
|    value_loss           | 420        |
----------------------------------------
Num timesteps: 18660000
Best mean reward: 5907.26 - Last mean reward per episode: 5554.63
Num timesteps: 18672000
Best mean reward: 5907.26 - Last mean reward per episode: 5588.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 5545.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 760         |
|    time_elapsed         | 220607      |
|    total_timesteps      | 18677760    |
| train/                  |             |
|    approx_kl            | 0.013633954 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 217         |
|    n_updates            | 7590        |
|    policy_gradient_loss | -1.74e-05   |
|    value_loss           | 423         |
-----------------------------------------
Num timesteps: 18684000
Best mean reward: 5907.26 - Last mean reward per episode: 5532.81
Num timesteps: 18696000
Best mean reward: 5907.26 - Last mean reward per episode: 5497.07
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.44e+03     |
|    ep_rew_mean          | 5497.02      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 761          |
|    time_elapsed         | 220900       |
|    total_timesteps      | 18702336     |
| train/                  |              |
|    approx_kl            | 0.0134274475 |
|    clip_fraction        | 0.0941       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.653       |
|    explained_variance   | 0.966        |
|    learning_rate        | 3e-06        |
|    loss                 | 115          |
|    n_updates            | 7600         |
|    policy_gradient_loss | 0.00102      |
|    value_loss           | 430          |
------------------------------------------
Num timesteps: 18708000
Best mean reward: 5907.26 - Last mean reward per episode: 5572.87
Num timesteps: 18720000
Best mean reward: 5907.26 - Last mean reward per episode: 5615.49
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.54e+03   |
|    ep_rew_mean          | 5574.61    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 762        |
|    time_elapsed         | 221189     |
|    total_timesteps      | 18726912   |
| train/                  |            |
|    approx_kl            | 0.01044028 |
|    clip_fraction        | 0.0875     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 229        |
|    n_updates            | 7610       |
|    policy_gradient_loss | 0.00109    |
|    value_loss           | 330        |
----------------------------------------
Num timesteps: 18732000
Best mean reward: 5907.26 - Last mean reward per episode: 5548.04
Num timesteps: 18744000
Best mean reward: 5907.26 - Last mean reward per episode: 5546.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.57e+03    |
|    ep_rew_mean          | 5549.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 763         |
|    time_elapsed         | 221480      |
|    total_timesteps      | 18751488    |
| train/                  |             |
|    approx_kl            | 0.011452511 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 218         |
|    n_updates            | 7620        |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 391         |
-----------------------------------------
Num timesteps: 18756000
Best mean reward: 5907.26 - Last mean reward per episode: 5549.73
Num timesteps: 18768000
Best mean reward: 5907.26 - Last mean reward per episode: 5546.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.57e+03    |
|    ep_rew_mean          | 5548.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 764         |
|    time_elapsed         | 221770      |
|    total_timesteps      | 18776064    |
| train/                  |             |
|    approx_kl            | 0.007580606 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 7630        |
|    policy_gradient_loss | -0.000368   |
|    value_loss           | 373         |
-----------------------------------------
Num timesteps: 18780000
Best mean reward: 5907.26 - Last mean reward per episode: 5587.38
Num timesteps: 18792000
Best mean reward: 5907.26 - Last mean reward per episode: 5551.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.6e+03      |
|    ep_rew_mean          | 5644.76      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 765          |
|    time_elapsed         | 222058       |
|    total_timesteps      | 18800640     |
| train/                  |              |
|    approx_kl            | 0.0069224522 |
|    clip_fraction        | 0.065        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.458       |
|    explained_variance   | 0.981        |
|    learning_rate        | 3e-06        |
|    loss                 | 65           |
|    n_updates            | 7640         |
|    policy_gradient_loss | -0.000838    |
|    value_loss           | 373          |
------------------------------------------
Num timesteps: 18804000
Best mean reward: 5907.26 - Last mean reward per episode: 5644.76
Num timesteps: 18816000
Best mean reward: 5907.26 - Last mean reward per episode: 5627.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.63e+03    |
|    ep_rew_mean          | 5585.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 766         |
|    time_elapsed         | 222348      |
|    total_timesteps      | 18825216    |
| train/                  |             |
|    approx_kl            | 0.010299512 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.443      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 76.8        |
|    n_updates            | 7650        |
|    policy_gradient_loss | -0.000407   |
|    value_loss           | 342         |
-----------------------------------------
Num timesteps: 18828000
Best mean reward: 5907.26 - Last mean reward per episode: 5572.90
Num timesteps: 18840000
Best mean reward: 5907.26 - Last mean reward per episode: 5543.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61e+03    |
|    ep_rew_mean          | 5543.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 767         |
|    time_elapsed         | 222643      |
|    total_timesteps      | 18849792    |
| train/                  |             |
|    approx_kl            | 0.008557371 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 152         |
|    n_updates            | 7660        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 440         |
-----------------------------------------
Num timesteps: 18852000
Best mean reward: 5907.26 - Last mean reward per episode: 5509.42
Num timesteps: 18864000
Best mean reward: 5907.26 - Last mean reward per episode: 5505.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61e+03    |
|    ep_rew_mean          | 5547.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 768         |
|    time_elapsed         | 222935      |
|    total_timesteps      | 18874368    |
| train/                  |             |
|    approx_kl            | 0.009871844 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 328         |
|    n_updates            | 7670        |
|    policy_gradient_loss | -0.000128   |
|    value_loss           | 362         |
-----------------------------------------
Num timesteps: 18876000
Best mean reward: 5907.26 - Last mean reward per episode: 5546.71
Num timesteps: 18888000
Best mean reward: 5907.26 - Last mean reward per episode: 5535.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.6e+03     |
|    ep_rew_mean          | 5533.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 769         |
|    time_elapsed         | 223218      |
|    total_timesteps      | 18898944    |
| train/                  |             |
|    approx_kl            | 0.008875909 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.445      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 93.2        |
|    n_updates            | 7680        |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 393         |
-----------------------------------------
Num timesteps: 18900000
Best mean reward: 5907.26 - Last mean reward per episode: 5556.69
Num timesteps: 18912000
Best mean reward: 5907.26 - Last mean reward per episode: 5561.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.56e+03     |
|    ep_rew_mean          | 5561.46      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 770          |
|    time_elapsed         | 223515       |
|    total_timesteps      | 18923520     |
| train/                  |              |
|    approx_kl            | 0.0057347356 |
|    clip_fraction        | 0.0652       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.415       |
|    explained_variance   | 0.986        |
|    learning_rate        | 3e-06        |
|    loss                 | 151          |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.000835    |
|    value_loss           | 302          |
------------------------------------------
Num timesteps: 18924000
Best mean reward: 5907.26 - Last mean reward per episode: 5561.46
Num timesteps: 18936000
Best mean reward: 5907.26 - Last mean reward per episode: 5578.37
Num timesteps: 18948000
Best mean reward: 5907.26 - Last mean reward per episode: 5619.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 5619.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 771         |
|    time_elapsed         | 223808      |
|    total_timesteps      | 18948096    |
| train/                  |             |
|    approx_kl            | 0.015522763 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 224         |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 393         |
-----------------------------------------
Num timesteps: 18960000
Best mean reward: 5907.26 - Last mean reward per episode: 5596.89
Num timesteps: 18972000
Best mean reward: 5907.26 - Last mean reward per episode: 5673.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5673.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 772         |
|    time_elapsed         | 224108      |
|    total_timesteps      | 18972672    |
| train/                  |             |
|    approx_kl            | 0.009376076 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 7710        |
|    policy_gradient_loss | -0.000389   |
|    value_loss           | 329         |
-----------------------------------------
Num timesteps: 18984000
Best mean reward: 5907.26 - Last mean reward per episode: 5712.63
Num timesteps: 18996000
Best mean reward: 5907.26 - Last mean reward per episode: 5681.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5681.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 773         |
|    time_elapsed         | 224404      |
|    total_timesteps      | 18997248    |
| train/                  |             |
|    approx_kl            | 0.008292376 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 58.3        |
|    n_updates            | 7720        |
|    policy_gradient_loss | -0.000565   |
|    value_loss           | 392         |
-----------------------------------------
Num timesteps: 19008000
Best mean reward: 5907.26 - Last mean reward per episode: 5638.27
Num timesteps: 19020000
Best mean reward: 5907.26 - Last mean reward per episode: 5644.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5644.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 774         |
|    time_elapsed         | 224693      |
|    total_timesteps      | 19021824    |
| train/                  |             |
|    approx_kl            | 0.008280676 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 563         |
|    n_updates            | 7730        |
|    policy_gradient_loss | -0.00156    |
|    value_loss           | 374         |
-----------------------------------------
Num timesteps: 19032000
Best mean reward: 5907.26 - Last mean reward per episode: 5671.26
Num timesteps: 19044000
Best mean reward: 5907.26 - Last mean reward per episode: 5635.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5635.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 775         |
|    time_elapsed         | 224986      |
|    total_timesteps      | 19046400    |
| train/                  |             |
|    approx_kl            | 0.008108194 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 405         |
|    n_updates            | 7740        |
|    policy_gradient_loss | -0.000992   |
|    value_loss           | 400         |
-----------------------------------------
Num timesteps: 19056000
Best mean reward: 5907.26 - Last mean reward per episode: 5645.36
Num timesteps: 19068000
Best mean reward: 5907.26 - Last mean reward per episode: 5689.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 5689.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 776         |
|    time_elapsed         | 225274      |
|    total_timesteps      | 19070976    |
| train/                  |             |
|    approx_kl            | 0.009276231 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 343         |
|    n_updates            | 7750        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 445         |
-----------------------------------------
Num timesteps: 19080000
Best mean reward: 5907.26 - Last mean reward per episode: 5693.27
Num timesteps: 19092000
Best mean reward: 5907.26 - Last mean reward per episode: 5774.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.46e+03     |
|    ep_rew_mean          | 5809.63      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 777          |
|    time_elapsed         | 225566       |
|    total_timesteps      | 19095552     |
| train/                  |              |
|    approx_kl            | 0.0056497827 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.986        |
|    learning_rate        | 3e-06        |
|    loss                 | 287          |
|    n_updates            | 7760         |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 356          |
------------------------------------------
Num timesteps: 19104000
Best mean reward: 5907.26 - Last mean reward per episode: 5803.54
Num timesteps: 19116000
Best mean reward: 5907.26 - Last mean reward per episode: 5854.06
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.47e+03   |
|    ep_rew_mean          | 5864.49    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 778        |
|    time_elapsed         | 225864     |
|    total_timesteps      | 19120128   |
| train/                  |            |
|    approx_kl            | 0.00731561 |
|    clip_fraction        | 0.0628     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.391     |
|    explained_variance   | 0.986      |
|    learning_rate        | 3e-06      |
|    loss                 | 89.9       |
|    n_updates            | 7770       |
|    policy_gradient_loss | -0.000792  |
|    value_loss           | 339        |
----------------------------------------
Num timesteps: 19128000
Best mean reward: 5907.26 - Last mean reward per episode: 5865.43
Num timesteps: 19140000
Best mean reward: 5907.26 - Last mean reward per episode: 5826.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5826.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 779         |
|    time_elapsed         | 226156      |
|    total_timesteps      | 19144704    |
| train/                  |             |
|    approx_kl            | 0.008792827 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 297         |
|    n_updates            | 7780        |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 411         |
-----------------------------------------
Num timesteps: 19152000
Best mean reward: 5907.26 - Last mean reward per episode: 5822.67
Num timesteps: 19164000
Best mean reward: 5907.26 - Last mean reward per episode: 5796.66
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.45e+03   |
|    ep_rew_mean          | 5764.77    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 780        |
|    time_elapsed         | 226447     |
|    total_timesteps      | 19169280   |
| train/                  |            |
|    approx_kl            | 0.01549919 |
|    clip_fraction        | 0.0887     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.567     |
|    explained_variance   | 0.987      |
|    learning_rate        | 3e-06      |
|    loss                 | 101        |
|    n_updates            | 7790       |
|    policy_gradient_loss | -0.000611  |
|    value_loss           | 331        |
----------------------------------------
Num timesteps: 19176000
Best mean reward: 5907.26 - Last mean reward per episode: 5728.34
Num timesteps: 19188000
Best mean reward: 5907.26 - Last mean reward per episode: 5743.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 5741.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 781         |
|    time_elapsed         | 226732      |
|    total_timesteps      | 19193856    |
| train/                  |             |
|    approx_kl            | 0.011041292 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 7800        |
|    policy_gradient_loss | -0.00061    |
|    value_loss           | 401         |
-----------------------------------------
Num timesteps: 19200000
Best mean reward: 5907.26 - Last mean reward per episode: 5742.48
Num timesteps: 19212000
Best mean reward: 5907.26 - Last mean reward per episode: 5784.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 5787.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 782         |
|    time_elapsed         | 227026      |
|    total_timesteps      | 19218432    |
| train/                  |             |
|    approx_kl            | 0.028923303 |
|    clip_fraction        | 0.0961      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 110         |
|    n_updates            | 7810        |
|    policy_gradient_loss | -0.00297    |
|    value_loss           | 348         |
-----------------------------------------
Num timesteps: 19224000
Best mean reward: 5907.26 - Last mean reward per episode: 5777.45
Num timesteps: 19236000
Best mean reward: 5907.26 - Last mean reward per episode: 5767.99
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.47e+03   |
|    ep_rew_mean          | 5774.12    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 783        |
|    time_elapsed         | 227315     |
|    total_timesteps      | 19243008   |
| train/                  |            |
|    approx_kl            | 0.01091145 |
|    clip_fraction        | 0.0814     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.457     |
|    explained_variance   | 0.983      |
|    learning_rate        | 3e-06      |
|    loss                 | 146        |
|    n_updates            | 7820       |
|    policy_gradient_loss | -2.27e-05  |
|    value_loss           | 402        |
----------------------------------------
Num timesteps: 19248000
Best mean reward: 5907.26 - Last mean reward per episode: 5776.99
Num timesteps: 19260000
Best mean reward: 5907.26 - Last mean reward per episode: 5807.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 5816.1      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 784         |
|    time_elapsed         | 227610      |
|    total_timesteps      | 19267584    |
| train/                  |             |
|    approx_kl            | 0.010436763 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 160         |
|    n_updates            | 7830        |
|    policy_gradient_loss | -0.000416   |
|    value_loss           | 491         |
-----------------------------------------
Num timesteps: 19272000
Best mean reward: 5907.26 - Last mean reward per episode: 5817.87
Num timesteps: 19284000
Best mean reward: 5907.26 - Last mean reward per episode: 5814.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 5795.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 785         |
|    time_elapsed         | 227900      |
|    total_timesteps      | 19292160    |
| train/                  |             |
|    approx_kl            | 0.009045295 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 236         |
|    n_updates            | 7840        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 401         |
-----------------------------------------
Num timesteps: 19296000
Best mean reward: 5907.26 - Last mean reward per episode: 5798.29
Num timesteps: 19308000
Best mean reward: 5907.26 - Last mean reward per episode: 5789.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 5798.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 786         |
|    time_elapsed         | 228190      |
|    total_timesteps      | 19316736    |
| train/                  |             |
|    approx_kl            | 0.007957976 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 109         |
|    n_updates            | 7850        |
|    policy_gradient_loss | -0.000285   |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 19320000
Best mean reward: 5907.26 - Last mean reward per episode: 5801.30
Num timesteps: 19332000
Best mean reward: 5907.26 - Last mean reward per episode: 5749.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 5750.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 787         |
|    time_elapsed         | 228484      |
|    total_timesteps      | 19341312    |
| train/                  |             |
|    approx_kl            | 0.007374251 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 73.9        |
|    n_updates            | 7860        |
|    policy_gradient_loss | -0.000482   |
|    value_loss           | 418         |
-----------------------------------------
Num timesteps: 19344000
Best mean reward: 5907.26 - Last mean reward per episode: 5727.89
Num timesteps: 19356000
Best mean reward: 5907.26 - Last mean reward per episode: 5725.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 5713.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 788         |
|    time_elapsed         | 228780      |
|    total_timesteps      | 19365888    |
| train/                  |             |
|    approx_kl            | 0.010913408 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 254         |
|    n_updates            | 7870        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 445         |
-----------------------------------------
Num timesteps: 19368000
Best mean reward: 5907.26 - Last mean reward per episode: 5711.42
Num timesteps: 19380000
Best mean reward: 5907.26 - Last mean reward per episode: 5718.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 5764.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 789         |
|    time_elapsed         | 229076      |
|    total_timesteps      | 19390464    |
| train/                  |             |
|    approx_kl            | 0.007557254 |
|    clip_fraction        | 0.0746      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.979       |
|    learning_rate        | 3e-06       |
|    loss                 | 85.4        |
|    n_updates            | 7880        |
|    policy_gradient_loss | -0.000407   |
|    value_loss           | 374         |
-----------------------------------------
Num timesteps: 19392000
Best mean reward: 5907.26 - Last mean reward per episode: 5764.95
Num timesteps: 19404000
Best mean reward: 5907.26 - Last mean reward per episode: 5721.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 5746.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 790         |
|    time_elapsed         | 229371      |
|    total_timesteps      | 19415040    |
| train/                  |             |
|    approx_kl            | 0.014995846 |
|    clip_fraction        | 0.0669      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 518         |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 324         |
-----------------------------------------
Num timesteps: 19416000
Best mean reward: 5907.26 - Last mean reward per episode: 5780.54
Num timesteps: 19428000
Best mean reward: 5907.26 - Last mean reward per episode: 5853.97
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.54e+03     |
|    ep_rew_mean          | 5795.02      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 791          |
|    time_elapsed         | 229662       |
|    total_timesteps      | 19439616     |
| train/                  |              |
|    approx_kl            | 0.0072317463 |
|    clip_fraction        | 0.0613       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.417       |
|    explained_variance   | 0.985        |
|    learning_rate        | 3e-06        |
|    loss                 | 91.9         |
|    n_updates            | 7900         |
|    policy_gradient_loss | -0.000713    |
|    value_loss           | 358          |
------------------------------------------
Num timesteps: 19440000
Best mean reward: 5907.26 - Last mean reward per episode: 5795.02
Num timesteps: 19452000
Best mean reward: 5907.26 - Last mean reward per episode: 5834.65
Num timesteps: 19464000
Best mean reward: 5907.26 - Last mean reward per episode: 5799.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 5799.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 792         |
|    time_elapsed         | 229951      |
|    total_timesteps      | 19464192    |
| train/                  |             |
|    approx_kl            | 0.012178185 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 357         |
|    n_updates            | 7910        |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 426         |
-----------------------------------------
Num timesteps: 19476000
Best mean reward: 5907.26 - Last mean reward per episode: 5815.02
Num timesteps: 19488000
Best mean reward: 5907.26 - Last mean reward per episode: 5815.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 5815.75     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 793         |
|    time_elapsed         | 230255      |
|    total_timesteps      | 19488768    |
| train/                  |             |
|    approx_kl            | 0.008175284 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 99.3        |
|    n_updates            | 7920        |
|    policy_gradient_loss | -0.000487   |
|    value_loss           | 359         |
-----------------------------------------
Num timesteps: 19500000
Best mean reward: 5907.26 - Last mean reward per episode: 5785.15
Num timesteps: 19512000
Best mean reward: 5907.26 - Last mean reward per episode: 5732.38
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.43e+03     |
|    ep_rew_mean          | 5730.11      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 794          |
|    time_elapsed         | 230551       |
|    total_timesteps      | 19513344     |
| train/                  |              |
|    approx_kl            | 0.0071286596 |
|    clip_fraction        | 0.0648       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.445       |
|    explained_variance   | 0.985        |
|    learning_rate        | 3e-06        |
|    loss                 | 109          |
|    n_updates            | 7930         |
|    policy_gradient_loss | -0.000534    |
|    value_loss           | 374          |
------------------------------------------
Num timesteps: 19524000
Best mean reward: 5907.26 - Last mean reward per episode: 5742.30
Num timesteps: 19536000
Best mean reward: 5907.26 - Last mean reward per episode: 5796.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5794.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 795         |
|    time_elapsed         | 230849      |
|    total_timesteps      | 19537920    |
| train/                  |             |
|    approx_kl            | 0.010270682 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.444      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 210         |
|    n_updates            | 7940        |
|    policy_gradient_loss | -0.000959   |
|    value_loss           | 421         |
-----------------------------------------
Num timesteps: 19548000
Best mean reward: 5907.26 - Last mean reward per episode: 5806.82
Num timesteps: 19560000
Best mean reward: 5907.26 - Last mean reward per episode: 5802.57
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.43e+03     |
|    ep_rew_mean          | 5797.3       |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 796          |
|    time_elapsed         | 231140       |
|    total_timesteps      | 19562496     |
| train/                  |              |
|    approx_kl            | 0.0057041137 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | 0.986        |
|    learning_rate        | 3e-06        |
|    loss                 | 135          |
|    n_updates            | 7950         |
|    policy_gradient_loss | -0.000731    |
|    value_loss           | 341          |
------------------------------------------
Num timesteps: 19572000
Best mean reward: 5907.26 - Last mean reward per episode: 5845.44
Num timesteps: 19584000
Best mean reward: 5907.26 - Last mean reward per episode: 5822.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.44e+03     |
|    ep_rew_mean          | 5822.27      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 797          |
|    time_elapsed         | 231434       |
|    total_timesteps      | 19587072     |
| train/                  |              |
|    approx_kl            | 0.0050884136 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | 0.988        |
|    learning_rate        | 3e-06        |
|    loss                 | 289          |
|    n_updates            | 7960         |
|    policy_gradient_loss | -0.000651    |
|    value_loss           | 307          |
------------------------------------------
Num timesteps: 19596000
Best mean reward: 5907.26 - Last mean reward per episode: 5783.12
Num timesteps: 19608000
Best mean reward: 5907.26 - Last mean reward per episode: 5799.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 5795.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 798         |
|    time_elapsed         | 231726      |
|    total_timesteps      | 19611648    |
| train/                  |             |
|    approx_kl            | 0.008746565 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 7970        |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 442         |
-----------------------------------------
Num timesteps: 19620000
Best mean reward: 5907.26 - Last mean reward per episode: 5797.12
Num timesteps: 19632000
Best mean reward: 5907.26 - Last mean reward per episode: 5736.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5730.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 799         |
|    time_elapsed         | 232019      |
|    total_timesteps      | 19636224    |
| train/                  |             |
|    approx_kl            | 0.006021746 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0.989       |
|    learning_rate        | 3e-06       |
|    loss                 | 28.7        |
|    n_updates            | 7980        |
|    policy_gradient_loss | -0.000696   |
|    value_loss           | 299         |
-----------------------------------------
Num timesteps: 19644000
Best mean reward: 5907.26 - Last mean reward per episode: 5727.95
Num timesteps: 19656000
Best mean reward: 5907.26 - Last mean reward per episode: 5727.32
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5732.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 800         |
|    time_elapsed         | 232308      |
|    total_timesteps      | 19660800    |
| train/                  |             |
|    approx_kl            | 0.022169573 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 75.9        |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 416         |
-----------------------------------------
Num timesteps: 19668000
Best mean reward: 5907.26 - Last mean reward per episode: 5772.52
Num timesteps: 19680000
Best mean reward: 5907.26 - Last mean reward per episode: 5732.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 5734.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 801         |
|    time_elapsed         | 232599      |
|    total_timesteps      | 19685376    |
| train/                  |             |
|    approx_kl            | 0.009401931 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 294         |
|    n_updates            | 8000        |
|    policy_gradient_loss | -9.5e-05    |
|    value_loss           | 374         |
-----------------------------------------
Num timesteps: 19692000
Best mean reward: 5907.26 - Last mean reward per episode: 5733.99
Num timesteps: 19704000
Best mean reward: 5907.26 - Last mean reward per episode: 5770.03
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.38e+03   |
|    ep_rew_mean          | 5770.03    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 802        |
|    time_elapsed         | 232892     |
|    total_timesteps      | 19709952   |
| train/                  |            |
|    approx_kl            | 0.01027274 |
|    clip_fraction        | 0.078      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.554     |
|    explained_variance   | 0.986      |
|    learning_rate        | 3e-06      |
|    loss                 | 141        |
|    n_updates            | 8010       |
|    policy_gradient_loss | -0.00091   |
|    value_loss           | 327        |
----------------------------------------
Num timesteps: 19716000
Best mean reward: 5907.26 - Last mean reward per episode: 5771.36
Num timesteps: 19728000
Best mean reward: 5907.26 - Last mean reward per episode: 5768.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | 5765.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 803         |
|    time_elapsed         | 233183      |
|    total_timesteps      | 19734528    |
| train/                  |             |
|    approx_kl            | 0.015252746 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.984       |
|    learning_rate        | 3e-06       |
|    loss                 | 235         |
|    n_updates            | 8020        |
|    policy_gradient_loss | 0.00179     |
|    value_loss           | 362         |
-----------------------------------------
Num timesteps: 19740000
Best mean reward: 5907.26 - Last mean reward per episode: 5765.01
Num timesteps: 19752000
Best mean reward: 5907.26 - Last mean reward per episode: 5791.06
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.46e+03   |
|    ep_rew_mean          | 5791.51    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 804        |
|    time_elapsed         | 233474     |
|    total_timesteps      | 19759104   |
| train/                  |            |
|    approx_kl            | 0.01419389 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.989      |
|    learning_rate        | 3e-06      |
|    loss                 | 129        |
|    n_updates            | 8030       |
|    policy_gradient_loss | -0.00451   |
|    value_loss           | 269        |
----------------------------------------
Num timesteps: 19764000
Best mean reward: 5907.26 - Last mean reward per episode: 5831.76
Num timesteps: 19776000
Best mean reward: 5907.26 - Last mean reward per episode: 5796.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5786.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 805         |
|    time_elapsed         | 233764      |
|    total_timesteps      | 19783680    |
| train/                  |             |
|    approx_kl            | 0.008706135 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.987       |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 8040        |
|    policy_gradient_loss | -0.000131   |
|    value_loss           | 346         |
-----------------------------------------
Num timesteps: 19788000
Best mean reward: 5907.26 - Last mean reward per episode: 5786.69
Num timesteps: 19800000
Best mean reward: 5907.26 - Last mean reward per episode: 5742.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.54e+03    |
|    ep_rew_mean          | 5738.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 806         |
|    time_elapsed         | 234058      |
|    total_timesteps      | 19808256    |
| train/                  |             |
|    approx_kl            | 0.007473782 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 256         |
|    n_updates            | 8050        |
|    policy_gradient_loss | 0.000154    |
|    value_loss           | 427         |
-----------------------------------------
Num timesteps: 19812000
Best mean reward: 5907.26 - Last mean reward per episode: 5739.33
Num timesteps: 19824000
Best mean reward: 5907.26 - Last mean reward per episode: 5694.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.54e+03     |
|    ep_rew_mean          | 5741.38      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 807          |
|    time_elapsed         | 234352       |
|    total_timesteps      | 19832832     |
| train/                  |              |
|    approx_kl            | 0.0071705473 |
|    clip_fraction        | 0.0572       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.393       |
|    explained_variance   | 0.983        |
|    learning_rate        | 3e-06        |
|    loss                 | 145          |
|    n_updates            | 8060         |
|    policy_gradient_loss | -0.000568    |
|    value_loss           | 414          |
------------------------------------------
Num timesteps: 19836000
Best mean reward: 5907.26 - Last mean reward per episode: 5698.36
Num timesteps: 19848000
Best mean reward: 5907.26 - Last mean reward per episode: 5746.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.54e+03    |
|    ep_rew_mean          | 5745.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 808         |
|    time_elapsed         | 234644      |
|    total_timesteps      | 19857408    |
| train/                  |             |
|    approx_kl            | 0.021454213 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 170         |
|    n_updates            | 8070        |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 367         |
-----------------------------------------
Num timesteps: 19860000
Best mean reward: 5907.26 - Last mean reward per episode: 5742.96
Num timesteps: 19872000
Best mean reward: 5907.26 - Last mean reward per episode: 5693.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 5665.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 809         |
|    time_elapsed         | 234936      |
|    total_timesteps      | 19881984    |
| train/                  |             |
|    approx_kl            | 0.013576974 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 73.1        |
|    n_updates            | 8080        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 271         |
-----------------------------------------
Num timesteps: 19884000
Best mean reward: 5907.26 - Last mean reward per episode: 5667.76
Num timesteps: 19896000
Best mean reward: 5907.26 - Last mean reward per episode: 5756.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.58e+03    |
|    ep_rew_mean          | 5720.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 810         |
|    time_elapsed         | 235226      |
|    total_timesteps      | 19906560    |
| train/                  |             |
|    approx_kl            | 0.012876323 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 61.2        |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 414         |
-----------------------------------------
Num timesteps: 19908000
Best mean reward: 5907.26 - Last mean reward per episode: 5720.52
Num timesteps: 19920000
Best mean reward: 5907.26 - Last mean reward per episode: 5687.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | 5696.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 811         |
|    time_elapsed         | 235523      |
|    total_timesteps      | 19931136    |
| train/                  |             |
|    approx_kl            | 0.007884476 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.987       |
|    learning_rate        | 3e-06       |
|    loss                 | 102         |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 337         |
-----------------------------------------
Num timesteps: 19932000
Best mean reward: 5907.26 - Last mean reward per episode: 5696.97
Num timesteps: 19944000
Best mean reward: 5907.26 - Last mean reward per episode: 5694.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.57e+03    |
|    ep_rew_mean          | 5701.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 812         |
|    time_elapsed         | 235813      |
|    total_timesteps      | 19955712    |
| train/                  |             |
|    approx_kl            | 0.008386811 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.403      |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 80.1        |
|    n_updates            | 8110        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 356         |
-----------------------------------------
Num timesteps: 19956000
Best mean reward: 5907.26 - Last mean reward per episode: 5701.30
Num timesteps: 19968000
Best mean reward: 5907.26 - Last mean reward per episode: 5729.57
Num timesteps: 19980000
Best mean reward: 5907.26 - Last mean reward per episode: 5741.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 5738.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 813         |
|    time_elapsed         | 236109      |
|    total_timesteps      | 19980288    |
| train/                  |             |
|    approx_kl            | 0.006992269 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.981       |
|    learning_rate        | 3e-06       |
|    loss                 | 219         |
|    n_updates            | 8120        |
|    policy_gradient_loss | -0.000515   |
|    value_loss           | 422         |
-----------------------------------------
Num timesteps: 19992000
Best mean reward: 5907.26 - Last mean reward per episode: 5743.04
Num timesteps: 20004000
Best mean reward: 5907.26 - Last mean reward per episode: 5748.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 5748.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 814         |
|    time_elapsed         | 236407      |
|    total_timesteps      | 20004864    |
| train/                  |             |
|    approx_kl            | 0.004902936 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.988       |
|    learning_rate        | 3e-06       |
|    loss                 | 130         |
|    n_updates            | 8130        |
|    policy_gradient_loss | -0.000877   |
|    value_loss           | 312         |
-----------------------------------------
<<<<< Stop learning >>>>>
