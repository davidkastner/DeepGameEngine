Python 3.8.17
2023-06-20 10:50:24.049548: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:24.931075: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:35.941275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.941344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.948447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.966063: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.973458: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.988563: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.991215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.993638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.995714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.996945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:35.997444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:36.029850: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.029850: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.030859: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.042893: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.044967: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-20 10:50:36.047519: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066513: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066519: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066517: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066526: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066528: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.066532: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-20 10:50:36.129595: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Using cpu device
Wrapping the env in a VecTransposeImage.
<<<<< Start learning >>>>>
Logging to ./tensorboard/PPO-00003_1
------------------------------
| time/              |       |
|    fps             | 1044  |
|    iterations      | 1     |
|    time_elapsed    | 23    |
|    total_timesteps | 24576 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 2           |
|    time_elapsed         | 319         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014710789 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | -5.94e-05   |
|    learning_rate        | 3e-06       |
|    loss                 | 0.195       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 3           |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -inf - Last mean reward per episode: 337.00
Saving new best model to tmp/best_model
Num timesteps: 72000
Best mean reward: 337.00 - Last mean reward per episode: 337.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.85e+03    |
|    ep_rew_mean          | 337.0       |
| time/                   |             |
|    fps                  | 119         |
|    iterations           | 3           |
|    time_elapsed         | 617         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.009660824 |
|    clip_fraction        | 0.0963      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.125       |
|    learning_rate        | 3e-06       |
|    loss                 | 16.4        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 31.4        |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 337.00 - Last mean reward per episode: 610.75
Saving new best model to tmp/best_model
Num timesteps: 96000
Best mean reward: 610.75 - Last mean reward per episode: 610.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.77e+03    |
|    ep_rew_mean          | 610.75      |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 4           |
|    time_elapsed         | 920         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.008571111 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 0.506       |
|    learning_rate        | 3e-06       |
|    loss                 | 54.2        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 65.6        |
-----------------------------------------
Num timesteps: 108000
Best mean reward: 610.75 - Last mean reward per episode: 610.75
Num timesteps: 120000
Best mean reward: 610.75 - Last mean reward per episode: 663.80
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 5.67e+03     |
|    ep_rew_mean          | 663.8        |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 5            |
|    time_elapsed         | 1217         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0072161946 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.666        |
|    learning_rate        | 3e-06        |
|    loss                 | 20.4         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 41.1         |
------------------------------------------
Num timesteps: 132000
Best mean reward: 663.80 - Last mean reward per episode: 989.89
Saving new best model to tmp/best_model
Num timesteps: 144000
Best mean reward: 989.89 - Last mean reward per episode: 1063.50
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.15e+03    |
|    ep_rew_mean          | 1063.5      |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 6           |
|    time_elapsed         | 1518        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.006069818 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.748       |
|    learning_rate        | 3e-06       |
|    loss                 | 1.96        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 13.3        |
-----------------------------------------
Num timesteps: 156000
Best mean reward: 1063.50 - Last mean reward per episode: 1063.50
Num timesteps: 168000
Best mean reward: 1063.50 - Last mean reward per episode: 1063.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.15e+03    |
|    ep_rew_mean          | 1063.5      |
| time/                   |             |
|    fps                  | 94          |
|    iterations           | 7           |
|    time_elapsed         | 1825        |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.006016215 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.817       |
|    learning_rate        | 3e-06       |
|    loss                 | 16          |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 71.1        |
-----------------------------------------
Num timesteps: 180000
Best mean reward: 1063.50 - Last mean reward per episode: 1063.50
Num timesteps: 192000
Best mean reward: 1063.50 - Last mean reward per episode: 1231.85
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 9.75e+03     |
|    ep_rew_mean          | 1231.8462    |
| time/                   |              |
|    fps                  | 92           |
|    iterations           | 8            |
|    time_elapsed         | 2125         |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0057142586 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.784        |
|    learning_rate        | 3e-06        |
|    loss                 | 9.11         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000994    |
|    value_loss           | 5.71         |
------------------------------------------
Num timesteps: 204000
Best mean reward: 1231.85 - Last mean reward per episode: 1265.33
Saving new best model to tmp/best_model
Num timesteps: 216000
Best mean reward: 1265.33 - Last mean reward per episode: 1265.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.85e+03    |
|    ep_rew_mean          | 1265.3334   |
| time/                   |             |
|    fps                  | 91          |
|    iterations           | 9           |
|    time_elapsed         | 2424        |
|    total_timesteps      | 221184      |
| train/                  |             |
|    approx_kl            | 0.007565147 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.875       |
|    learning_rate        | 3e-06       |
|    loss                 | 14.5        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 45.4        |
-----------------------------------------
Num timesteps: 228000
Best mean reward: 1265.33 - Last mean reward per episode: 1271.44
Saving new best model to tmp/best_model
Num timesteps: 240000
Best mean reward: 1271.44 - Last mean reward per episode: 1282.41
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.41e+03    |
|    ep_rew_mean          | 1282.4117   |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 10          |
|    time_elapsed         | 2723        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.007445943 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 9.93        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.000155   |
|    value_loss           | 33.7        |
-----------------------------------------
Num timesteps: 252000
Best mean reward: 1282.41 - Last mean reward per episode: 1313.61
Saving new best model to tmp/best_model
Num timesteps: 264000
Best mean reward: 1313.61 - Last mean reward per episode: 1435.30
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.57e+03    |
|    ep_rew_mean          | 1417.9166   |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 11          |
|    time_elapsed         | 3021        |
|    total_timesteps      | 270336      |
| train/                  |             |
|    approx_kl            | 0.008166317 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 3.45        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.000958   |
|    value_loss           | 16.6        |
-----------------------------------------
Num timesteps: 276000
Best mean reward: 1435.30 - Last mean reward per episode: 1417.92
Num timesteps: 288000
Best mean reward: 1435.30 - Last mean reward per episode: 1450.44
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 9.06e+03     |
|    ep_rew_mean          | 1473.7778    |
| time/                   |              |
|    fps                  | 88           |
|    iterations           | 12           |
|    time_elapsed         | 3328         |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0061202296 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 0.938        |
|    learning_rate        | 3e-06        |
|    loss                 | 17.1         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000856    |
|    value_loss           | 95.2         |
------------------------------------------
Num timesteps: 300000
Best mean reward: 1450.44 - Last mean reward per episode: 1476.96
Saving new best model to tmp/best_model
Num timesteps: 312000
Best mean reward: 1476.96 - Last mean reward per episode: 1482.93
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.83e+03    |
|    ep_rew_mean          | 1503.1936   |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 13          |
|    time_elapsed         | 3632        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.007489993 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.891       |
|    learning_rate        | 3e-06       |
|    loss                 | 69.5        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 115         |
-----------------------------------------
Num timesteps: 324000
Best mean reward: 1482.93 - Last mean reward per episode: 1503.19
Saving new best model to tmp/best_model
Num timesteps: 336000
Best mean reward: 1503.19 - Last mean reward per episode: 1569.06
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.33e+03    |
|    ep_rew_mean          | 1570.9166   |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 14          |
|    time_elapsed         | 3936        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.007523173 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.87       |
|    explained_variance   | 0.911       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000614   |
|    value_loss           | 166         |
-----------------------------------------
Num timesteps: 348000
Best mean reward: 1569.06 - Last mean reward per episode: 1586.32
Saving new best model to tmp/best_model
Num timesteps: 360000
Best mean reward: 1586.32 - Last mean reward per episode: 1630.55
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.55e+03     |
|    ep_rew_mean          | 1644.9783    |
| time/                   |              |
|    fps                  | 86           |
|    iterations           | 15           |
|    time_elapsed         | 4240         |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0075204372 |
|    clip_fraction        | 0.0892       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 0.923        |
|    learning_rate        | 3e-06        |
|    loss                 | 80.4         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 164          |
------------------------------------------
Num timesteps: 372000
Best mean reward: 1630.55 - Last mean reward per episode: 1664.60
Saving new best model to tmp/best_model
Num timesteps: 384000
Best mean reward: 1664.60 - Last mean reward per episode: 1685.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.3e+03     |
|    ep_rew_mean          | 1669.7457   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 16          |
|    time_elapsed         | 4535        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.006384143 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.82       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 143         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00098    |
|    value_loss           | 234         |
-----------------------------------------
Num timesteps: 396000
Best mean reward: 1685.59 - Last mean reward per episode: 1676.79
Num timesteps: 408000
Best mean reward: 1685.59 - Last mean reward per episode: 1685.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.62e+03    |
|    ep_rew_mean          | 1713.9143   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 17          |
|    time_elapsed         | 4829        |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.007551912 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.69       |
|    explained_variance   | 0.899       |
|    learning_rate        | 3e-06       |
|    loss                 | 168         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 385         |
-----------------------------------------
Num timesteps: 420000
Best mean reward: 1685.59 - Last mean reward per episode: 1745.69
Saving new best model to tmp/best_model
Num timesteps: 432000
Best mean reward: 1745.69 - Last mean reward per episode: 1751.91
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.99e+03    |
|    ep_rew_mean          | 1786.1548   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 18          |
|    time_elapsed         | 5126        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.007898296 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.878       |
|    learning_rate        | 3e-06       |
|    loss                 | 205         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.000917   |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 444000
Best mean reward: 1751.91 - Last mean reward per episode: 1780.74
Saving new best model to tmp/best_model
Num timesteps: 456000
Best mean reward: 1780.74 - Last mean reward per episode: 1780.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.66e+03    |
|    ep_rew_mean          | 1802.5773   |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 19          |
|    time_elapsed         | 5422        |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.007152064 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.907       |
|    learning_rate        | 3e-06       |
|    loss                 | 315         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000511   |
|    value_loss           | 409         |
-----------------------------------------
Num timesteps: 468000
Best mean reward: 1780.74 - Last mean reward per episode: 1802.58
Saving new best model to tmp/best_model
Num timesteps: 480000
Best mean reward: 1802.58 - Last mean reward per episode: 1831.46
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.26e+03     |
|    ep_rew_mean          | 1883.87      |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 20           |
|    time_elapsed         | 5723         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0073837326 |
|    clip_fraction        | 0.0774       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.936        |
|    learning_rate        | 3e-06        |
|    loss                 | 98.7         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.000312     |
|    value_loss           | 283          |
------------------------------------------
Num timesteps: 492000
Best mean reward: 1831.46 - Last mean reward per episode: 1883.87
Saving new best model to tmp/best_model
Num timesteps: 504000
Best mean reward: 1883.87 - Last mean reward per episode: 1897.77
Saving new best model to tmp/best_model
Num timesteps: 516000
Best mean reward: 1897.77 - Last mean reward per episode: 1893.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.01e+03     |
|    ep_rew_mean          | 1893.8       |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 21           |
|    time_elapsed         | 6025         |
|    total_timesteps      | 516096       |
| train/                  |              |
|    approx_kl            | 0.0067628263 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.62        |
|    explained_variance   | 0.922        |
|    learning_rate        | 3e-06        |
|    loss                 | 112          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00037     |
|    value_loss           | 362          |
------------------------------------------
Num timesteps: 528000
Best mean reward: 1897.77 - Last mean reward per episode: 1896.88
Num timesteps: 540000
Best mean reward: 1897.77 - Last mean reward per episode: 1888.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.3e+03      |
|    ep_rew_mean          | 1878.28      |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 22           |
|    time_elapsed         | 6320         |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0072628316 |
|    clip_fraction        | 0.0735       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.63        |
|    explained_variance   | 0.925        |
|    learning_rate        | 3e-06        |
|    loss                 | 86.3         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0004      |
|    value_loss           | 346          |
------------------------------------------
Num timesteps: 552000
Best mean reward: 1897.77 - Last mean reward per episode: 1895.70
Num timesteps: 564000
Best mean reward: 1897.77 - Last mean reward per episode: 1935.01
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 1927.81     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 23          |
|    time_elapsed         | 6615        |
|    total_timesteps      | 565248      |
| train/                  |             |
|    approx_kl            | 0.006257016 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.66       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 120         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.000212   |
|    value_loss           | 348         |
-----------------------------------------
Num timesteps: 576000
Best mean reward: 1935.01 - Last mean reward per episode: 1922.51
Num timesteps: 588000
Best mean reward: 1935.01 - Last mean reward per episode: 1938.45
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02e+03    |
|    ep_rew_mean          | 1937.94     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 24          |
|    time_elapsed         | 6919        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.006880377 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 58          |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 382         |
-----------------------------------------
Num timesteps: 600000
Best mean reward: 1938.45 - Last mean reward per episode: 1948.65
Saving new best model to tmp/best_model
Num timesteps: 612000
Best mean reward: 1948.65 - Last mean reward per episode: 1976.83
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.95e+03     |
|    ep_rew_mean          | 1976.83      |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 25           |
|    time_elapsed         | 7211         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0071697645 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.5         |
|    explained_variance   | 0.907        |
|    learning_rate        | 3e-06        |
|    loss                 | 270          |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 478          |
------------------------------------------
Num timesteps: 624000
Best mean reward: 1976.83 - Last mean reward per episode: 1979.80
Saving new best model to tmp/best_model
Num timesteps: 636000
Best mean reward: 1979.80 - Last mean reward per episode: 1998.57
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 1990.76     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 26          |
|    time_elapsed         | 7506        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.007851045 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.38       |
|    explained_variance   | 0.894       |
|    learning_rate        | 3e-06       |
|    loss                 | 189         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.000531   |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 648000
Best mean reward: 1998.57 - Last mean reward per episode: 1974.43
Num timesteps: 660000
Best mean reward: 1998.57 - Last mean reward per episode: 2016.88
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 2028.22     |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 27          |
|    time_elapsed         | 7802        |
|    total_timesteps      | 663552      |
| train/                  |             |
|    approx_kl            | 0.007755907 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.907       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.000111   |
|    value_loss           | 447         |
-----------------------------------------
Num timesteps: 672000
Best mean reward: 2016.88 - Last mean reward per episode: 2024.95
Saving new best model to tmp/best_model
Num timesteps: 684000
Best mean reward: 2024.95 - Last mean reward per episode: 2032.02
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.81e+03     |
|    ep_rew_mean          | 2036.75      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 28           |
|    time_elapsed         | 8098         |
|    total_timesteps      | 688128       |
| train/                  |              |
|    approx_kl            | 0.0072596795 |
|    clip_fraction        | 0.0884       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.927        |
|    learning_rate        | 3e-06        |
|    loss                 | 111          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000542    |
|    value_loss           | 388          |
------------------------------------------
Num timesteps: 696000
Best mean reward: 2032.02 - Last mean reward per episode: 2040.27
Saving new best model to tmp/best_model
Num timesteps: 708000
Best mean reward: 2040.27 - Last mean reward per episode: 2054.33
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 2100.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 29          |
|    time_elapsed         | 8396        |
|    total_timesteps      | 712704      |
| train/                  |             |
|    approx_kl            | 0.008104875 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 91.4        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.000452   |
|    value_loss           | 435         |
-----------------------------------------
Num timesteps: 720000
Best mean reward: 2054.33 - Last mean reward per episode: 2104.39
Saving new best model to tmp/best_model
Num timesteps: 732000
Best mean reward: 2104.39 - Last mean reward per episode: 2067.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.75e+03    |
|    ep_rew_mean          | 2063.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 30          |
|    time_elapsed         | 8696        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.007309798 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 298         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000925   |
|    value_loss           | 475         |
-----------------------------------------
Num timesteps: 744000
Best mean reward: 2104.39 - Last mean reward per episode: 2048.30
Num timesteps: 756000
Best mean reward: 2104.39 - Last mean reward per episode: 2072.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | 2077.47      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 31           |
|    time_elapsed         | 8994         |
|    total_timesteps      | 761856       |
| train/                  |              |
|    approx_kl            | 0.0077880025 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.6         |
|    explained_variance   | 0.927        |
|    learning_rate        | 3e-06        |
|    loss                 | 127          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 393          |
------------------------------------------
Num timesteps: 768000
Best mean reward: 2104.39 - Last mean reward per episode: 2077.60
Num timesteps: 780000
Best mean reward: 2104.39 - Last mean reward per episode: 2046.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 2030.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 32          |
|    time_elapsed         | 9291        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.008641992 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 104         |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 360         |
-----------------------------------------
Num timesteps: 792000
Best mean reward: 2104.39 - Last mean reward per episode: 2006.47
Num timesteps: 804000
Best mean reward: 2104.39 - Last mean reward per episode: 2006.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.7e+03     |
|    ep_rew_mean          | 1996.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 33          |
|    time_elapsed         | 9587        |
|    total_timesteps      | 811008      |
| train/                  |             |
|    approx_kl            | 0.007984597 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.921       |
|    learning_rate        | 3e-06       |
|    loss                 | 161         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.000583   |
|    value_loss           | 431         |
-----------------------------------------
Num timesteps: 816000
Best mean reward: 2104.39 - Last mean reward per episode: 1991.10
Num timesteps: 828000
Best mean reward: 2104.39 - Last mean reward per episode: 2039.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.76e+03    |
|    ep_rew_mean          | 2018.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 34          |
|    time_elapsed         | 9885        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.008569358 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.52       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 251         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.000628   |
|    value_loss           | 462         |
-----------------------------------------
Num timesteps: 840000
Best mean reward: 2104.39 - Last mean reward per episode: 2017.40
Num timesteps: 852000
Best mean reward: 2104.39 - Last mean reward per episode: 1989.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.75e+03    |
|    ep_rew_mean          | 1998.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 35          |
|    time_elapsed         | 10181       |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.008056257 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.47       |
|    explained_variance   | 0.921       |
|    learning_rate        | 3e-06       |
|    loss                 | 312         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.000972   |
|    value_loss           | 445         |
-----------------------------------------
Num timesteps: 864000
Best mean reward: 2104.39 - Last mean reward per episode: 2000.15
Num timesteps: 876000
Best mean reward: 2104.39 - Last mean reward per episode: 2009.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.73e+03    |
|    ep_rew_mean          | 1990.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 36          |
|    time_elapsed         | 10478       |
|    total_timesteps      | 884736      |
| train/                  |             |
|    approx_kl            | 0.008040972 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 559         |
-----------------------------------------
Num timesteps: 888000
Best mean reward: 2104.39 - Last mean reward per episode: 1978.15
Num timesteps: 900000
Best mean reward: 2104.39 - Last mean reward per episode: 1990.68
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.66e+03   |
|    ep_rew_mean          | 1975.29    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 37         |
|    time_elapsed         | 10777      |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.00820512 |
|    clip_fraction        | 0.0992     |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.32      |
|    explained_variance   | 0.913      |
|    learning_rate        | 3e-06      |
|    loss                 | 215        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.00051   |
|    value_loss           | 532        |
----------------------------------------
Num timesteps: 912000
Best mean reward: 2104.39 - Last mean reward per episode: 1976.79
Num timesteps: 924000
Best mean reward: 2104.39 - Last mean reward per episode: 1956.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 1965.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 38          |
|    time_elapsed         | 11073       |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.008466725 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.909       |
|    learning_rate        | 3e-06       |
|    loss                 | 224         |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.000359   |
|    value_loss           | 532         |
-----------------------------------------
Num timesteps: 936000
Best mean reward: 2104.39 - Last mean reward per episode: 1965.71
Num timesteps: 948000
Best mean reward: 2104.39 - Last mean reward per episode: 1982.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.56e+03     |
|    ep_rew_mean          | 1978.14      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 39           |
|    time_elapsed         | 11364        |
|    total_timesteps      | 958464       |
| train/                  |              |
|    approx_kl            | 0.0074082282 |
|    clip_fraction        | 0.0924       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.946        |
|    learning_rate        | 3e-06        |
|    loss                 | 220          |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000219    |
|    value_loss           | 394          |
------------------------------------------
Num timesteps: 960000
Best mean reward: 2104.39 - Last mean reward per episode: 1978.53
Num timesteps: 972000
Best mean reward: 2104.39 - Last mean reward per episode: 1890.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.56e+03     |
|    ep_rew_mean          | 1905.79      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 40           |
|    time_elapsed         | 11658        |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0076002404 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.944        |
|    learning_rate        | 3e-06        |
|    loss                 | 72.5         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00058     |
|    value_loss           | 411          |
------------------------------------------
Num timesteps: 984000
Best mean reward: 2104.39 - Last mean reward per episode: 1902.94
Num timesteps: 996000
Best mean reward: 2104.39 - Last mean reward per episode: 1898.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+03   |
|    ep_rew_mean          | 1904.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 41         |
|    time_elapsed         | 11951      |
|    total_timesteps      | 1007616    |
| train/                  |            |
|    approx_kl            | 0.00850828 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.41      |
|    explained_variance   | 0.934      |
|    learning_rate        | 3e-06      |
|    loss                 | 388        |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.00161   |
|    value_loss           | 494        |
----------------------------------------
Num timesteps: 1008000
Best mean reward: 2104.39 - Last mean reward per episode: 1906.92
Num timesteps: 1020000
Best mean reward: 2104.39 - Last mean reward per episode: 1874.77
Num timesteps: 1032000
Best mean reward: 2104.39 - Last mean reward per episode: 1858.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.47e+03    |
|    ep_rew_mean          | 1858.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 42          |
|    time_elapsed         | 12247       |
|    total_timesteps      | 1032192     |
| train/                  |             |
|    approx_kl            | 0.008319271 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.29       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000982   |
|    value_loss           | 557         |
-----------------------------------------
Num timesteps: 1044000
Best mean reward: 2104.39 - Last mean reward per episode: 1873.38
Num timesteps: 1056000
Best mean reward: 2104.39 - Last mean reward per episode: 1866.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 1866.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 43          |
|    time_elapsed         | 12539       |
|    total_timesteps      | 1056768     |
| train/                  |             |
|    approx_kl            | 0.008626626 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.14       |
|    explained_variance   | 0.912       |
|    learning_rate        | 3e-06       |
|    loss                 | 424         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 648         |
-----------------------------------------
Num timesteps: 1068000
Best mean reward: 2104.39 - Last mean reward per episode: 1889.04
Num timesteps: 1080000
Best mean reward: 2104.39 - Last mean reward per episode: 1870.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.46e+03     |
|    ep_rew_mean          | 1864.79      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 44           |
|    time_elapsed         | 12836        |
|    total_timesteps      | 1081344      |
| train/                  |              |
|    approx_kl            | 0.0087124435 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.14        |
|    explained_variance   | 0.914        |
|    learning_rate        | 3e-06        |
|    loss                 | 407          |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 634          |
------------------------------------------
Num timesteps: 1092000
Best mean reward: 2104.39 - Last mean reward per episode: 1894.92
Num timesteps: 1104000
Best mean reward: 2104.39 - Last mean reward per episode: 1893.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.39e+03    |
|    ep_rew_mean          | 1921.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 45          |
|    time_elapsed         | 13130       |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.009648025 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.895       |
|    learning_rate        | 3e-06       |
|    loss                 | 276         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 640         |
-----------------------------------------
Num timesteps: 1116000
Best mean reward: 2104.39 - Last mean reward per episode: 1927.14
Num timesteps: 1128000
Best mean reward: 2104.39 - Last mean reward per episode: 1952.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 1973.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 46          |
|    time_elapsed         | 13421       |
|    total_timesteps      | 1130496     |
| train/                  |             |
|    approx_kl            | 0.009506631 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.906       |
|    learning_rate        | 3e-06       |
|    loss                 | 318         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 1140000
Best mean reward: 2104.39 - Last mean reward per episode: 1921.87
Num timesteps: 1152000
Best mean reward: 2104.39 - Last mean reward per episode: 1952.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 1948.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 47          |
|    time_elapsed         | 13714       |
|    total_timesteps      | 1155072     |
| train/                  |             |
|    approx_kl            | 0.009655134 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 280         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 590         |
-----------------------------------------
Num timesteps: 1164000
Best mean reward: 2104.39 - Last mean reward per episode: 1936.42
Num timesteps: 1176000
Best mean reward: 2104.39 - Last mean reward per episode: 1942.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 1948.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 48          |
|    time_elapsed         | 14009       |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.009487769 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.01       |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 214         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 628         |
-----------------------------------------
Num timesteps: 1188000
Best mean reward: 2104.39 - Last mean reward per episode: 1965.34
Num timesteps: 1200000
Best mean reward: 2104.39 - Last mean reward per episode: 1983.92
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.33e+03     |
|    ep_rew_mean          | 1990.46      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 49           |
|    time_elapsed         | 14300        |
|    total_timesteps      | 1204224      |
| train/                  |              |
|    approx_kl            | 0.0089955395 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.02        |
|    explained_variance   | 0.909        |
|    learning_rate        | 3e-06        |
|    loss                 | 234          |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 624          |
------------------------------------------
Num timesteps: 1212000
Best mean reward: 2104.39 - Last mean reward per episode: 1986.88
Num timesteps: 1224000
Best mean reward: 2104.39 - Last mean reward per episode: 1941.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 1955.75     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 50          |
|    time_elapsed         | 14591       |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.010048386 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 303         |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 1236000
Best mean reward: 2104.39 - Last mean reward per episode: 1962.69
Num timesteps: 1248000
Best mean reward: 2104.39 - Last mean reward per episode: 1942.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.24e+03   |
|    ep_rew_mean          | 1890.42    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 51         |
|    time_elapsed         | 14886      |
|    total_timesteps      | 1253376    |
| train/                  |            |
|    approx_kl            | 0.00980165 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.94      |
|    explained_variance   | 0.915      |
|    learning_rate        | 3e-06      |
|    loss                 | 559        |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00163   |
|    value_loss           | 608        |
----------------------------------------
Num timesteps: 1260000
Best mean reward: 2104.39 - Last mean reward per episode: 1912.09
Num timesteps: 1272000
Best mean reward: 2104.39 - Last mean reward per episode: 1912.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 1893.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 52          |
|    time_elapsed         | 15179       |
|    total_timesteps      | 1277952     |
| train/                  |             |
|    approx_kl            | 0.010225041 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.9        |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 422         |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 661         |
-----------------------------------------
Num timesteps: 1284000
Best mean reward: 2104.39 - Last mean reward per episode: 1892.32
Num timesteps: 1296000
Best mean reward: 2104.39 - Last mean reward per episode: 1917.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 1897.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 53          |
|    time_elapsed         | 15474       |
|    total_timesteps      | 1302528     |
| train/                  |             |
|    approx_kl            | 0.009864786 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.89       |
|    explained_variance   | 0.913       |
|    learning_rate        | 3e-06       |
|    loss                 | 194         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 718         |
-----------------------------------------
Num timesteps: 1308000
Best mean reward: 2104.39 - Last mean reward per episode: 1900.97
Num timesteps: 1320000
Best mean reward: 2104.39 - Last mean reward per episode: 1888.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.21e+03    |
|    ep_rew_mean          | 1899.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 54          |
|    time_elapsed         | 15770       |
|    total_timesteps      | 1327104     |
| train/                  |             |
|    approx_kl            | 0.009636044 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 313         |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 623         |
-----------------------------------------
Num timesteps: 1332000
Best mean reward: 2104.39 - Last mean reward per episode: 1900.02
Num timesteps: 1344000
Best mean reward: 2104.39 - Last mean reward per episode: 1928.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.21e+03    |
|    ep_rew_mean          | 1929.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 55          |
|    time_elapsed         | 16064       |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.009385503 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.913       |
|    learning_rate        | 3e-06       |
|    loss                 | 311         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.000778   |
|    value_loss           | 753         |
-----------------------------------------
Num timesteps: 1356000
Best mean reward: 2104.39 - Last mean reward per episode: 1943.71
Num timesteps: 1368000
Best mean reward: 2104.39 - Last mean reward per episode: 1968.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.23e+03     |
|    ep_rew_mean          | 1999.73      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 56           |
|    time_elapsed         | 16357        |
|    total_timesteps      | 1376256      |
| train/                  |              |
|    approx_kl            | 0.0097677875 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.74        |
|    explained_variance   | 0.911        |
|    learning_rate        | 3e-06        |
|    loss                 | 701          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 765          |
------------------------------------------
Num timesteps: 1380000
Best mean reward: 2104.39 - Last mean reward per episode: 1990.63
Num timesteps: 1392000
Best mean reward: 2104.39 - Last mean reward per episode: 2031.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.25e+03     |
|    ep_rew_mean          | 2046.05      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 57           |
|    time_elapsed         | 16653        |
|    total_timesteps      | 1400832      |
| train/                  |              |
|    approx_kl            | 0.0091545405 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.73        |
|    explained_variance   | 0.917        |
|    learning_rate        | 3e-06        |
|    loss                 | 204          |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 738          |
------------------------------------------
Num timesteps: 1404000
Best mean reward: 2104.39 - Last mean reward per episode: 2060.72
Num timesteps: 1416000
Best mean reward: 2104.39 - Last mean reward per episode: 2061.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 2088.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 58          |
|    time_elapsed         | 16949       |
|    total_timesteps      | 1425408     |
| train/                  |             |
|    approx_kl            | 0.009358622 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 151         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 687         |
-----------------------------------------
Num timesteps: 1428000
Best mean reward: 2104.39 - Last mean reward per episode: 2077.52
Num timesteps: 1440000
Best mean reward: 2104.39 - Last mean reward per episode: 2079.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2073.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 59          |
|    time_elapsed         | 17244       |
|    total_timesteps      | 1449984     |
| train/                  |             |
|    approx_kl            | 0.010389641 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 211         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 703         |
-----------------------------------------
Num timesteps: 1452000
Best mean reward: 2104.39 - Last mean reward per episode: 2083.12
Num timesteps: 1464000
Best mean reward: 2104.39 - Last mean reward per episode: 2080.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2073.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 60          |
|    time_elapsed         | 17537       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.011292889 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.925       |
|    learning_rate        | 3e-06       |
|    loss                 | 188         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 767         |
-----------------------------------------
Num timesteps: 1476000
Best mean reward: 2104.39 - Last mean reward per episode: 2073.38
Num timesteps: 1488000
Best mean reward: 2104.39 - Last mean reward per episode: 2096.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.32e+03    |
|    ep_rew_mean          | 2123.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 61          |
|    time_elapsed         | 17830       |
|    total_timesteps      | 1499136     |
| train/                  |             |
|    approx_kl            | 0.009676444 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 455         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 676         |
-----------------------------------------
Num timesteps: 1500000
Best mean reward: 2104.39 - Last mean reward per episode: 2123.02
Saving new best model to tmp/best_model
Num timesteps: 1512000
Best mean reward: 2123.02 - Last mean reward per episode: 2137.96
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.38e+03    |
|    ep_rew_mean          | 2116.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 62          |
|    time_elapsed         | 18125       |
|    total_timesteps      | 1523712     |
| train/                  |             |
|    approx_kl            | 0.010773924 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 232         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 659         |
-----------------------------------------
Num timesteps: 1524000
Best mean reward: 2137.96 - Last mean reward per episode: 2116.67
Num timesteps: 1536000
Best mean reward: 2137.96 - Last mean reward per episode: 2105.02
Num timesteps: 1548000
Best mean reward: 2137.96 - Last mean reward per episode: 2116.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 2116.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 63          |
|    time_elapsed         | 18418       |
|    total_timesteps      | 1548288     |
| train/                  |             |
|    approx_kl            | 0.009775658 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.93        |
|    learning_rate        | 3e-06       |
|    loss                 | 440         |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 717         |
-----------------------------------------
Num timesteps: 1560000
Best mean reward: 2137.96 - Last mean reward per episode: 2071.37
Num timesteps: 1572000
Best mean reward: 2137.96 - Last mean reward per episode: 2059.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.33e+03     |
|    ep_rew_mean          | 2059.77      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 64           |
|    time_elapsed         | 18712        |
|    total_timesteps      | 1572864      |
| train/                  |              |
|    approx_kl            | 0.0099542625 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | 0.911        |
|    learning_rate        | 3e-06        |
|    loss                 | 440          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 896          |
------------------------------------------
Num timesteps: 1584000
Best mean reward: 2137.96 - Last mean reward per episode: 2071.69
Num timesteps: 1596000
Best mean reward: 2137.96 - Last mean reward per episode: 2077.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2077.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 65          |
|    time_elapsed         | 19003       |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.011037189 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 461         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 846         |
-----------------------------------------
Num timesteps: 1608000
Best mean reward: 2137.96 - Last mean reward per episode: 2042.67
Num timesteps: 1620000
Best mean reward: 2137.96 - Last mean reward per episode: 1983.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 1988.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 66          |
|    time_elapsed         | 19294       |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 0.010591355 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.64       |
|    explained_variance   | 0.91        |
|    learning_rate        | 3e-06       |
|    loss                 | 498         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 855         |
-----------------------------------------
Num timesteps: 1632000
Best mean reward: 2137.96 - Last mean reward per episode: 1961.67
Num timesteps: 1644000
Best mean reward: 2137.96 - Last mean reward per episode: 1908.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.16e+03    |
|    ep_rew_mean          | 1909.1      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 67          |
|    time_elapsed         | 19592       |
|    total_timesteps      | 1646592     |
| train/                  |             |
|    approx_kl            | 0.011177283 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.912       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 827         |
-----------------------------------------
Num timesteps: 1656000
Best mean reward: 2137.96 - Last mean reward per episode: 1933.86
Num timesteps: 1668000
Best mean reward: 2137.96 - Last mean reward per episode: 1944.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.18e+03    |
|    ep_rew_mean          | 1947.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 68          |
|    time_elapsed         | 19881       |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.010736622 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-06       |
|    loss                 | 450         |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 798         |
-----------------------------------------
Num timesteps: 1680000
Best mean reward: 2137.96 - Last mean reward per episode: 1956.88
Num timesteps: 1692000
Best mean reward: 2137.96 - Last mean reward per episode: 1975.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.19e+03    |
|    ep_rew_mean          | 1975.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 69          |
|    time_elapsed         | 20171       |
|    total_timesteps      | 1695744     |
| train/                  |             |
|    approx_kl            | 0.010727134 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.924       |
|    learning_rate        | 3e-06       |
|    loss                 | 615         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00193    |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 1704000
Best mean reward: 2137.96 - Last mean reward per episode: 1962.67
Num timesteps: 1716000
Best mean reward: 2137.96 - Last mean reward per episode: 1971.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.17e+03    |
|    ep_rew_mean          | 1971.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 70          |
|    time_elapsed         | 20467       |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.010479946 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 570         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 841         |
-----------------------------------------
Num timesteps: 1728000
Best mean reward: 2137.96 - Last mean reward per episode: 1960.04
Num timesteps: 1740000
Best mean reward: 2137.96 - Last mean reward per episode: 2057.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 2044.76     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 71          |
|    time_elapsed         | 20753       |
|    total_timesteps      | 1744896     |
| train/                  |             |
|    approx_kl            | 0.010274403 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.923       |
|    learning_rate        | 3e-06       |
|    loss                 | 291         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 820         |
-----------------------------------------
Num timesteps: 1752000
Best mean reward: 2137.96 - Last mean reward per episode: 2063.51
Num timesteps: 1764000
Best mean reward: 2137.96 - Last mean reward per episode: 2129.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2136.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 72          |
|    time_elapsed         | 21043       |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.012156661 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 233         |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 779         |
-----------------------------------------
Num timesteps: 1776000
Best mean reward: 2137.96 - Last mean reward per episode: 2127.64
Num timesteps: 1788000
Best mean reward: 2137.96 - Last mean reward per episode: 2124.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 2156.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 73          |
|    time_elapsed         | 21332       |
|    total_timesteps      | 1794048     |
| train/                  |             |
|    approx_kl            | 0.012877218 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 248         |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 738         |
-----------------------------------------
Num timesteps: 1800000
Best mean reward: 2137.96 - Last mean reward per episode: 2170.79
Saving new best model to tmp/best_model
Num timesteps: 1812000
Best mean reward: 2170.79 - Last mean reward per episode: 2169.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2179.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 74          |
|    time_elapsed         | 21619       |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.011774891 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.926       |
|    learning_rate        | 3e-06       |
|    loss                 | 164         |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 775         |
-----------------------------------------
Num timesteps: 1824000
Best mean reward: 2170.79 - Last mean reward per episode: 2188.38
Saving new best model to tmp/best_model
Num timesteps: 1836000
Best mean reward: 2188.38 - Last mean reward per episode: 2178.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.31e+03    |
|    ep_rew_mean          | 2187.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 75          |
|    time_elapsed         | 21915       |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.010482428 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 705         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 911         |
-----------------------------------------
Num timesteps: 1848000
Best mean reward: 2188.38 - Last mean reward per episode: 2218.00
Saving new best model to tmp/best_model
Num timesteps: 1860000
Best mean reward: 2218.00 - Last mean reward per episode: 2229.37
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 2186.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 76          |
|    time_elapsed         | 22214       |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.011233815 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 212         |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.000645   |
|    value_loss           | 810         |
-----------------------------------------
Num timesteps: 1872000
Best mean reward: 2229.37 - Last mean reward per episode: 2192.06
Num timesteps: 1884000
Best mean reward: 2229.37 - Last mean reward per episode: 2200.49
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.27e+03   |
|    ep_rew_mean          | 2188.27    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 77         |
|    time_elapsed         | 22509      |
|    total_timesteps      | 1892352    |
| train/                  |            |
|    approx_kl            | 0.01177654 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.926      |
|    learning_rate        | 3e-06      |
|    loss                 | 569        |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00153   |
|    value_loss           | 847        |
----------------------------------------
Num timesteps: 1896000
Best mean reward: 2229.37 - Last mean reward per episode: 2205.92
Num timesteps: 1908000
Best mean reward: 2229.37 - Last mean reward per episode: 2207.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 2189.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 78          |
|    time_elapsed         | 22800       |
|    total_timesteps      | 1916928     |
| train/                  |             |
|    approx_kl            | 0.009708225 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.918       |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 886         |
-----------------------------------------
Num timesteps: 1920000
Best mean reward: 2229.37 - Last mean reward per episode: 2194.05
Num timesteps: 1932000
Best mean reward: 2229.37 - Last mean reward per episode: 2241.18
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.28e+03    |
|    ep_rew_mean          | 2246.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 79          |
|    time_elapsed         | 23090       |
|    total_timesteps      | 1941504     |
| train/                  |             |
|    approx_kl            | 0.012463663 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-06       |
|    loss                 | 231         |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 859         |
-----------------------------------------
Num timesteps: 1944000
Best mean reward: 2241.18 - Last mean reward per episode: 2269.58
Saving new best model to tmp/best_model
Num timesteps: 1956000
Best mean reward: 2269.58 - Last mean reward per episode: 2273.03
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 2267.7      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 80          |
|    time_elapsed         | 23382       |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.011663042 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 338         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 671         |
-----------------------------------------
Num timesteps: 1968000
Best mean reward: 2273.03 - Last mean reward per episode: 2264.86
Num timesteps: 1980000
Best mean reward: 2273.03 - Last mean reward per episode: 2204.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.28e+03    |
|    ep_rew_mean          | 2214.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 81          |
|    time_elapsed         | 23670       |
|    total_timesteps      | 1990656     |
| train/                  |             |
|    approx_kl            | 0.011269095 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 537         |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00162    |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 1992000
Best mean reward: 2273.03 - Last mean reward per episode: 2214.95
Num timesteps: 2004000
Best mean reward: 2273.03 - Last mean reward per episode: 2226.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.29e+03   |
|    ep_rew_mean          | 2238.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 82         |
|    time_elapsed         | 23964      |
|    total_timesteps      | 2015232    |
| train/                  |            |
|    approx_kl            | 0.01317851 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.71      |
|    explained_variance   | 0.915      |
|    learning_rate        | 3e-06      |
|    loss                 | 340        |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.000859  |
|    value_loss           | 896        |
----------------------------------------
Num timesteps: 2016000
Best mean reward: 2273.03 - Last mean reward per episode: 2238.94
Num timesteps: 2028000
Best mean reward: 2273.03 - Last mean reward per episode: 2207.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2173.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 83          |
|    time_elapsed         | 24250       |
|    total_timesteps      | 2039808     |
| train/                  |             |
|    approx_kl            | 0.013038245 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-06       |
|    loss                 | 736         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 988         |
-----------------------------------------
Num timesteps: 2040000
Best mean reward: 2273.03 - Last mean reward per episode: 2173.83
Num timesteps: 2052000
Best mean reward: 2273.03 - Last mean reward per episode: 2163.80
Num timesteps: 2064000
Best mean reward: 2273.03 - Last mean reward per episode: 2118.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 2118.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 84          |
|    time_elapsed         | 24542       |
|    total_timesteps      | 2064384     |
| train/                  |             |
|    approx_kl            | 0.012702306 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 413         |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 883         |
-----------------------------------------
Num timesteps: 2076000
Best mean reward: 2273.03 - Last mean reward per episode: 2141.88
Num timesteps: 2088000
Best mean reward: 2273.03 - Last mean reward per episode: 2134.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.24e+03    |
|    ep_rew_mean          | 2118.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 85          |
|    time_elapsed         | 24833       |
|    total_timesteps      | 2088960     |
| train/                  |             |
|    approx_kl            | 0.011585715 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.916       |
|    learning_rate        | 3e-06       |
|    loss                 | 397         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00068    |
|    value_loss           | 879         |
-----------------------------------------
Num timesteps: 2100000
Best mean reward: 2273.03 - Last mean reward per episode: 2169.57
Num timesteps: 2112000
Best mean reward: 2273.03 - Last mean reward per episode: 2150.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.24e+03    |
|    ep_rew_mean          | 2150.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 86          |
|    time_elapsed         | 25121       |
|    total_timesteps      | 2113536     |
| train/                  |             |
|    approx_kl            | 0.010577741 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 285         |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.000786   |
|    value_loss           | 817         |
-----------------------------------------
Num timesteps: 2124000
Best mean reward: 2273.03 - Last mean reward per episode: 2186.41
Num timesteps: 2136000
Best mean reward: 2273.03 - Last mean reward per episode: 2164.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.25e+03    |
|    ep_rew_mean          | 2189.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 87          |
|    time_elapsed         | 25409       |
|    total_timesteps      | 2138112     |
| train/                  |             |
|    approx_kl            | 0.012394653 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.919       |
|    learning_rate        | 3e-06       |
|    loss                 | 330         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 830         |
-----------------------------------------
Num timesteps: 2148000
Best mean reward: 2273.03 - Last mean reward per episode: 2168.34
Num timesteps: 2160000
Best mean reward: 2273.03 - Last mean reward per episode: 2208.36
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.26e+03     |
|    ep_rew_mean          | 2229.55      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 88           |
|    time_elapsed         | 25702        |
|    total_timesteps      | 2162688      |
| train/                  |              |
|    approx_kl            | 0.0124074435 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.93         |
|    learning_rate        | 3e-06        |
|    loss                 | 246          |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 802          |
------------------------------------------
Num timesteps: 2172000
Best mean reward: 2273.03 - Last mean reward per episode: 2237.45
Num timesteps: 2184000
Best mean reward: 2273.03 - Last mean reward per episode: 2252.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2287.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 89          |
|    time_elapsed         | 25990       |
|    total_timesteps      | 2187264     |
| train/                  |             |
|    approx_kl            | 0.011671756 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-06       |
|    loss                 | 198         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 878         |
-----------------------------------------
Num timesteps: 2196000
Best mean reward: 2273.03 - Last mean reward per episode: 2263.88
Num timesteps: 2208000
Best mean reward: 2273.03 - Last mean reward per episode: 2219.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2236.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 90          |
|    time_elapsed         | 26285       |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.011866022 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.928       |
|    learning_rate        | 3e-06       |
|    loss                 | 298         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.000931   |
|    value_loss           | 768         |
-----------------------------------------
Num timesteps: 2220000
Best mean reward: 2273.03 - Last mean reward per episode: 2285.19
Saving new best model to tmp/best_model
Num timesteps: 2232000
Best mean reward: 2285.19 - Last mean reward per episode: 2259.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2278.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 91          |
|    time_elapsed         | 26577       |
|    total_timesteps      | 2236416     |
| train/                  |             |
|    approx_kl            | 0.012515928 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.928       |
|    learning_rate        | 3e-06       |
|    loss                 | 643         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 846         |
-----------------------------------------
Num timesteps: 2244000
Best mean reward: 2285.19 - Last mean reward per episode: 2262.09
Num timesteps: 2256000
Best mean reward: 2285.19 - Last mean reward per episode: 2253.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2271.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 92          |
|    time_elapsed         | 26864       |
|    total_timesteps      | 2260992     |
| train/                  |             |
|    approx_kl            | 0.012332812 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 457         |
|    n_updates            | 910         |
|    policy_gradient_loss | 0.000925    |
|    value_loss           | 682         |
-----------------------------------------
Num timesteps: 2268000
Best mean reward: 2285.19 - Last mean reward per episode: 2307.08
Saving new best model to tmp/best_model
Num timesteps: 2280000
Best mean reward: 2307.08 - Last mean reward per episode: 2323.46
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2329.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 93          |
|    time_elapsed         | 27156       |
|    total_timesteps      | 2285568     |
| train/                  |             |
|    approx_kl            | 0.012507718 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-06       |
|    loss                 | 405         |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.000894   |
|    value_loss           | 828         |
-----------------------------------------
Num timesteps: 2292000
Best mean reward: 2323.46 - Last mean reward per episode: 2294.62
Num timesteps: 2304000
Best mean reward: 2323.46 - Last mean reward per episode: 2362.29
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.33e+03    |
|    ep_rew_mean          | 2328.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 94          |
|    time_elapsed         | 27445       |
|    total_timesteps      | 2310144     |
| train/                  |             |
|    approx_kl            | 0.012237929 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 515         |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 798         |
-----------------------------------------
Num timesteps: 2316000
Best mean reward: 2362.29 - Last mean reward per episode: 2319.61
Num timesteps: 2328000
Best mean reward: 2362.29 - Last mean reward per episode: 2364.93
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.36e+03    |
|    ep_rew_mean          | 2396.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 95          |
|    time_elapsed         | 27741       |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.012453693 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 363         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 755         |
-----------------------------------------
Num timesteps: 2340000
Best mean reward: 2364.93 - Last mean reward per episode: 2393.49
Saving new best model to tmp/best_model
Num timesteps: 2352000
Best mean reward: 2393.49 - Last mean reward per episode: 2451.08
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.38e+03     |
|    ep_rew_mean          | 2437.31      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 96           |
|    time_elapsed         | 28034        |
|    total_timesteps      | 2359296      |
| train/                  |              |
|    approx_kl            | 0.0138937645 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.922        |
|    learning_rate        | 3e-06        |
|    loss                 | 283          |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000967    |
|    value_loss           | 803          |
------------------------------------------
Num timesteps: 2364000
Best mean reward: 2451.08 - Last mean reward per episode: 2448.47
Num timesteps: 2376000
Best mean reward: 2451.08 - Last mean reward per episode: 2456.92
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 2443.98    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 97         |
|    time_elapsed         | 28332      |
|    total_timesteps      | 2383872    |
| train/                  |            |
|    approx_kl            | 0.01297789 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.927      |
|    learning_rate        | 3e-06      |
|    loss                 | 266        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00136   |
|    value_loss           | 795        |
----------------------------------------
Num timesteps: 2388000
Best mean reward: 2456.92 - Last mean reward per episode: 2443.98
Num timesteps: 2400000
Best mean reward: 2456.92 - Last mean reward per episode: 2448.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 2455.19    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 98         |
|    time_elapsed         | 28633      |
|    total_timesteps      | 2408448    |
| train/                  |            |
|    approx_kl            | 0.01110101 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.934      |
|    learning_rate        | 3e-06      |
|    loss                 | 593        |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0019    |
|    value_loss           | 773        |
----------------------------------------
Num timesteps: 2412000
Best mean reward: 2456.92 - Last mean reward per episode: 2435.68
Num timesteps: 2424000
Best mean reward: 2456.92 - Last mean reward per episode: 2494.74
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 2515.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 99          |
|    time_elapsed         | 28922       |
|    total_timesteps      | 2433024     |
| train/                  |             |
|    approx_kl            | 0.014103527 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 602         |
-----------------------------------------
Num timesteps: 2436000
Best mean reward: 2494.74 - Last mean reward per episode: 2513.80
Saving new best model to tmp/best_model
Num timesteps: 2448000
Best mean reward: 2513.80 - Last mean reward per episode: 2521.39
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.48e+03    |
|    ep_rew_mean          | 2507.15     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 100         |
|    time_elapsed         | 29217       |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.013957006 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 167         |
|    n_updates            | 990         |
|    policy_gradient_loss | -9.73e-05   |
|    value_loss           | 744         |
-----------------------------------------
Num timesteps: 2460000
Best mean reward: 2521.39 - Last mean reward per episode: 2476.23
Num timesteps: 2472000
Best mean reward: 2521.39 - Last mean reward per episode: 2477.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.42e+03    |
|    ep_rew_mean          | 2437.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 101         |
|    time_elapsed         | 29509       |
|    total_timesteps      | 2482176     |
| train/                  |             |
|    approx_kl            | 0.014229674 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 589         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 884         |
-----------------------------------------
Num timesteps: 2484000
Best mean reward: 2521.39 - Last mean reward per episode: 2447.85
Num timesteps: 2496000
Best mean reward: 2521.39 - Last mean reward per episode: 2401.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.41e+03    |
|    ep_rew_mean          | 2402.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 102         |
|    time_elapsed         | 29796       |
|    total_timesteps      | 2506752     |
| train/                  |             |
|    approx_kl            | 0.013196009 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 462         |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 726         |
-----------------------------------------
Num timesteps: 2508000
Best mean reward: 2521.39 - Last mean reward per episode: 2381.22
Num timesteps: 2520000
Best mean reward: 2521.39 - Last mean reward per episode: 2357.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 2339.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 103         |
|    time_elapsed         | 30090       |
|    total_timesteps      | 2531328     |
| train/                  |             |
|    approx_kl            | 0.012588333 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.93        |
|    learning_rate        | 3e-06       |
|    loss                 | 171         |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 883         |
-----------------------------------------
Num timesteps: 2532000
Best mean reward: 2521.39 - Last mean reward per episode: 2339.04
Num timesteps: 2544000
Best mean reward: 2521.39 - Last mean reward per episode: 2280.47
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.32e+03    |
|    ep_rew_mean          | 2327.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 104         |
|    time_elapsed         | 30382       |
|    total_timesteps      | 2555904     |
| train/                  |             |
|    approx_kl            | 0.011977837 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 498         |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 806         |
-----------------------------------------
Num timesteps: 2556000
Best mean reward: 2521.39 - Last mean reward per episode: 2327.93
Num timesteps: 2568000
Best mean reward: 2521.39 - Last mean reward per episode: 2339.39
Num timesteps: 2580000
Best mean reward: 2521.39 - Last mean reward per episode: 2309.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 2309.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 105         |
|    time_elapsed         | 30676       |
|    total_timesteps      | 2580480     |
| train/                  |             |
|    approx_kl            | 0.012505506 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.923       |
|    learning_rate        | 3e-06       |
|    loss                 | 423         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 855         |
-----------------------------------------
Num timesteps: 2592000
Best mean reward: 2521.39 - Last mean reward per episode: 2355.17
Num timesteps: 2604000
Best mean reward: 2521.39 - Last mean reward per episode: 2335.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2335.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 106         |
|    time_elapsed         | 30965       |
|    total_timesteps      | 2605056     |
| train/                  |             |
|    approx_kl            | 0.011232421 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 380         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 731         |
-----------------------------------------
Num timesteps: 2616000
Best mean reward: 2521.39 - Last mean reward per episode: 2342.77
Num timesteps: 2628000
Best mean reward: 2521.39 - Last mean reward per episode: 2355.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | 2355.43     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 107         |
|    time_elapsed         | 31254       |
|    total_timesteps      | 2629632     |
| train/                  |             |
|    approx_kl            | 0.013537613 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-06       |
|    loss                 | 360         |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 935         |
-----------------------------------------
Num timesteps: 2640000
Best mean reward: 2521.39 - Last mean reward per episode: 2346.75
Num timesteps: 2652000
Best mean reward: 2521.39 - Last mean reward per episode: 2412.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.34e+03    |
|    ep_rew_mean          | 2423.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 108         |
|    time_elapsed         | 31543       |
|    total_timesteps      | 2654208     |
| train/                  |             |
|    approx_kl            | 0.012660076 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 453         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.000999   |
|    value_loss           | 761         |
-----------------------------------------
Num timesteps: 2664000
Best mean reward: 2521.39 - Last mean reward per episode: 2458.67
Num timesteps: 2676000
Best mean reward: 2521.39 - Last mean reward per episode: 2475.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.39e+03   |
|    ep_rew_mean          | 2462.66    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 109        |
|    time_elapsed         | 31827      |
|    total_timesteps      | 2678784    |
| train/                  |            |
|    approx_kl            | 0.01272541 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.99      |
|    explained_variance   | 0.935      |
|    learning_rate        | 3e-06      |
|    loss                 | 522        |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.00229   |
|    value_loss           | 746        |
----------------------------------------
Num timesteps: 2688000
Best mean reward: 2521.39 - Last mean reward per episode: 2422.99
Num timesteps: 2700000
Best mean reward: 2521.39 - Last mean reward per episode: 2367.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.35e+03    |
|    ep_rew_mean          | 2360.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 110         |
|    time_elapsed         | 32113       |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.012332388 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 347         |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 765         |
-----------------------------------------
Num timesteps: 2712000
Best mean reward: 2521.39 - Last mean reward per episode: 2369.72
Num timesteps: 2724000
Best mean reward: 2521.39 - Last mean reward per episode: 2313.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.35e+03    |
|    ep_rew_mean          | 2311.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 111         |
|    time_elapsed         | 32400       |
|    total_timesteps      | 2727936     |
| train/                  |             |
|    approx_kl            | 0.012930104 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 684         |
-----------------------------------------
Num timesteps: 2736000
Best mean reward: 2521.39 - Last mean reward per episode: 2344.02
Num timesteps: 2748000
Best mean reward: 2521.39 - Last mean reward per episode: 2359.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.37e+03     |
|    ep_rew_mean          | 2344.87      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 112          |
|    time_elapsed         | 32690        |
|    total_timesteps      | 2752512      |
| train/                  |              |
|    approx_kl            | 0.0118833715 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.81        |
|    explained_variance   | 0.949        |
|    learning_rate        | 3e-06        |
|    loss                 | 217          |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.000303    |
|    value_loss           | 700          |
------------------------------------------
Num timesteps: 2760000
Best mean reward: 2521.39 - Last mean reward per episode: 2393.17
Num timesteps: 2772000
Best mean reward: 2521.39 - Last mean reward per episode: 2369.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.4e+03     |
|    ep_rew_mean          | 2381.42     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 113         |
|    time_elapsed         | 32977       |
|    total_timesteps      | 2777088     |
| train/                  |             |
|    approx_kl            | 0.013337155 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3e-06       |
|    loss                 | 699         |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 838         |
-----------------------------------------
Num timesteps: 2784000
Best mean reward: 2521.39 - Last mean reward per episode: 2373.62
Num timesteps: 2796000
Best mean reward: 2521.39 - Last mean reward per episode: 2364.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.43e+03    |
|    ep_rew_mean          | 2388.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 114         |
|    time_elapsed         | 33267       |
|    total_timesteps      | 2801664     |
| train/                  |             |
|    approx_kl            | 0.013219532 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 401         |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 793         |
-----------------------------------------
Num timesteps: 2808000
Best mean reward: 2521.39 - Last mean reward per episode: 2401.19
Num timesteps: 2820000
Best mean reward: 2521.39 - Last mean reward per episode: 2380.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 2401.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 115         |
|    time_elapsed         | 33561       |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.012826628 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 398         |
|    n_updates            | 1140        |
|    policy_gradient_loss | 0.000142    |
|    value_loss           | 631         |
-----------------------------------------
Num timesteps: 2832000
Best mean reward: 2521.39 - Last mean reward per episode: 2396.95
Num timesteps: 2844000
Best mean reward: 2521.39 - Last mean reward per episode: 2348.06
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.42e+03   |
|    ep_rew_mean          | 2356.69    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 116        |
|    time_elapsed         | 33856      |
|    total_timesteps      | 2850816    |
| train/                  |            |
|    approx_kl            | 0.01304021 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.88      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 176        |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.00183   |
|    value_loss           | 767        |
----------------------------------------
Num timesteps: 2856000
Best mean reward: 2521.39 - Last mean reward per episode: 2370.89
Num timesteps: 2868000
Best mean reward: 2521.39 - Last mean reward per episode: 2429.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 2468.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 117         |
|    time_elapsed         | 34141       |
|    total_timesteps      | 2875392     |
| train/                  |             |
|    approx_kl            | 0.013269247 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 773         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 837         |
-----------------------------------------
Num timesteps: 2880000
Best mean reward: 2521.39 - Last mean reward per episode: 2445.86
Num timesteps: 2892000
Best mean reward: 2521.39 - Last mean reward per episode: 2409.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | 2401.64    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 118        |
|    time_elapsed         | 34429      |
|    total_timesteps      | 2899968    |
| train/                  |            |
|    approx_kl            | 0.01272653 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.76      |
|    explained_variance   | 0.936      |
|    learning_rate        | 3e-06      |
|    loss                 | 534        |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.000589  |
|    value_loss           | 751        |
----------------------------------------
Num timesteps: 2904000
Best mean reward: 2521.39 - Last mean reward per episode: 2398.76
Num timesteps: 2916000
Best mean reward: 2521.39 - Last mean reward per episode: 2483.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.44e+03    |
|    ep_rew_mean          | 2495.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 119         |
|    time_elapsed         | 34720       |
|    total_timesteps      | 2924544     |
| train/                  |             |
|    approx_kl            | 0.014711112 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 315         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.000711   |
|    value_loss           | 806         |
-----------------------------------------
Num timesteps: 2928000
Best mean reward: 2521.39 - Last mean reward per episode: 2487.75
Num timesteps: 2940000
Best mean reward: 2521.39 - Last mean reward per episode: 2445.03
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 2440.8     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 120        |
|    time_elapsed         | 35011      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.01349657 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.71      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 362        |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.00115   |
|    value_loss           | 788        |
----------------------------------------
Num timesteps: 2952000
Best mean reward: 2521.39 - Last mean reward per episode: 2459.72
Num timesteps: 2964000
Best mean reward: 2521.39 - Last mean reward per episode: 2500.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.37e+03    |
|    ep_rew_mean          | 2491.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 121         |
|    time_elapsed         | 35302       |
|    total_timesteps      | 2973696     |
| train/                  |             |
|    approx_kl            | 0.013193662 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 543         |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 688         |
-----------------------------------------
Num timesteps: 2976000
Best mean reward: 2521.39 - Last mean reward per episode: 2491.23
Num timesteps: 2988000
Best mean reward: 2521.39 - Last mean reward per episode: 2575.01
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.47e+03    |
|    ep_rew_mean          | 2610.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 122         |
|    time_elapsed         | 35596       |
|    total_timesteps      | 2998272     |
| train/                  |             |
|    approx_kl            | 0.013159342 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 371         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 655         |
-----------------------------------------
Num timesteps: 3000000
Best mean reward: 2575.01 - Last mean reward per episode: 2606.81
Saving new best model to tmp/best_model
Num timesteps: 3012000
Best mean reward: 2606.81 - Last mean reward per episode: 2543.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.42e+03    |
|    ep_rew_mean          | 2509.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 123         |
|    time_elapsed         | 35883       |
|    total_timesteps      | 3022848     |
| train/                  |             |
|    approx_kl            | 0.014322226 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.66       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 676         |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 800         |
-----------------------------------------
Num timesteps: 3024000
Best mean reward: 2606.81 - Last mean reward per episode: 2550.54
Num timesteps: 3036000
Best mean reward: 2606.81 - Last mean reward per episode: 2583.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.48e+03    |
|    ep_rew_mean          | 2596.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 124         |
|    time_elapsed         | 36175       |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.014385112 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 477         |
|    n_updates            | 1230        |
|    policy_gradient_loss | 1.09e-05    |
|    value_loss           | 892         |
-----------------------------------------
Num timesteps: 3048000
Best mean reward: 2606.81 - Last mean reward per episode: 2604.35
Num timesteps: 3060000
Best mean reward: 2606.81 - Last mean reward per episode: 2612.06
Saving new best model to tmp/best_model
Num timesteps: 3072000
Best mean reward: 2612.06 - Last mean reward per episode: 2610.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.49e+03    |
|    ep_rew_mean          | 2610.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 125         |
|    time_elapsed         | 36468       |
|    total_timesteps      | 3072000     |
| train/                  |             |
|    approx_kl            | 0.013414939 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 616         |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.000834   |
|    value_loss           | 784         |
-----------------------------------------
Num timesteps: 3084000
Best mean reward: 2612.06 - Last mean reward per episode: 2660.71
Saving new best model to tmp/best_model
Num timesteps: 3096000
Best mean reward: 2660.71 - Last mean reward per episode: 2698.04
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 2698.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 126         |
|    time_elapsed         | 36761       |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.015505713 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.000918   |
|    value_loss           | 654         |
-----------------------------------------
Num timesteps: 3108000
Best mean reward: 2698.04 - Last mean reward per episode: 2657.18
Num timesteps: 3120000
Best mean reward: 2698.04 - Last mean reward per episode: 2694.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.56e+03    |
|    ep_rew_mean          | 2690.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 127         |
|    time_elapsed         | 37055       |
|    total_timesteps      | 3121152     |
| train/                  |             |
|    approx_kl            | 0.015645312 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.93        |
|    learning_rate        | 3e-06       |
|    loss                 | 571         |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.000617   |
|    value_loss           | 814         |
-----------------------------------------
Num timesteps: 3132000
Best mean reward: 2698.04 - Last mean reward per episode: 2751.22
Saving new best model to tmp/best_model
Num timesteps: 3144000
Best mean reward: 2751.22 - Last mean reward per episode: 2835.59
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.6e+03    |
|    ep_rew_mean          | 2823.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 128        |
|    time_elapsed         | 37346      |
|    total_timesteps      | 3145728    |
| train/                  |            |
|    approx_kl            | 0.01345595 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.68      |
|    explained_variance   | 0.936      |
|    learning_rate        | 3e-06      |
|    loss                 | 477        |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.00203   |
|    value_loss           | 779        |
----------------------------------------
Num timesteps: 3156000
Best mean reward: 2835.59 - Last mean reward per episode: 2781.80
Num timesteps: 3168000
Best mean reward: 2835.59 - Last mean reward per episode: 2836.80
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2843.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 129         |
|    time_elapsed         | 37637       |
|    total_timesteps      | 3170304     |
| train/                  |             |
|    approx_kl            | 0.014694002 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 1280        |
|    policy_gradient_loss | 0.000688    |
|    value_loss           | 803         |
-----------------------------------------
Num timesteps: 3180000
Best mean reward: 2836.80 - Last mean reward per episode: 2891.04
Saving new best model to tmp/best_model
Num timesteps: 3192000
Best mean reward: 2891.04 - Last mean reward per episode: 2870.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2870.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 130         |
|    time_elapsed         | 37932       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.012964033 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-06       |
|    loss                 | 242         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00081    |
|    value_loss           | 923         |
-----------------------------------------
Num timesteps: 3204000
Best mean reward: 2891.04 - Last mean reward per episode: 2884.04
Num timesteps: 3216000
Best mean reward: 2891.04 - Last mean reward per episode: 2919.78
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | 2986.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 131         |
|    time_elapsed         | 38225       |
|    total_timesteps      | 3219456     |
| train/                  |             |
|    approx_kl            | 0.015605616 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 361         |
|    n_updates            | 1300        |
|    policy_gradient_loss | 0.000512    |
|    value_loss           | 761         |
-----------------------------------------
Num timesteps: 3228000
Best mean reward: 2919.78 - Last mean reward per episode: 3020.66
Saving new best model to tmp/best_model
Num timesteps: 3240000
Best mean reward: 3020.66 - Last mean reward per episode: 2975.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 2929.76     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 132         |
|    time_elapsed         | 38519       |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.013380739 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 311         |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 646         |
-----------------------------------------
Num timesteps: 3252000
Best mean reward: 3020.66 - Last mean reward per episode: 2992.27
Num timesteps: 3264000
Best mean reward: 3020.66 - Last mean reward per episode: 2944.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | 2960.77     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 133         |
|    time_elapsed         | 38811       |
|    total_timesteps      | 3268608     |
| train/                  |             |
|    approx_kl            | 0.015131214 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 579         |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.000484   |
|    value_loss           | 780         |
-----------------------------------------
Num timesteps: 3276000
Best mean reward: 3020.66 - Last mean reward per episode: 2899.34
Num timesteps: 3288000
Best mean reward: 3020.66 - Last mean reward per episode: 2908.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 2924.6      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 134         |
|    time_elapsed         | 39102       |
|    total_timesteps      | 3293184     |
| train/                  |             |
|    approx_kl            | 0.014190495 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 156         |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.000298    |
|    value_loss           | 795         |
-----------------------------------------
Num timesteps: 3300000
Best mean reward: 3020.66 - Last mean reward per episode: 2940.60
Num timesteps: 3312000
Best mean reward: 3020.66 - Last mean reward per episode: 2939.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 2964.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 135         |
|    time_elapsed         | 39391       |
|    total_timesteps      | 3317760     |
| train/                  |             |
|    approx_kl            | 0.015220266 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 369         |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 3324000
Best mean reward: 3020.66 - Last mean reward per episode: 2984.18
Num timesteps: 3336000
Best mean reward: 3020.66 - Last mean reward per episode: 2999.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.65e+03   |
|    ep_rew_mean          | 3019.32    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 136        |
|    time_elapsed         | 39687      |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.01501105 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.52      |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-06      |
|    loss                 | 324        |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.000874  |
|    value_loss           | 788        |
----------------------------------------
Num timesteps: 3348000
Best mean reward: 3020.66 - Last mean reward per episode: 3036.67
Saving new best model to tmp/best_model
Num timesteps: 3360000
Best mean reward: 3036.67 - Last mean reward per episode: 3041.83
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 3034.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 137         |
|    time_elapsed         | 39982       |
|    total_timesteps      | 3366912     |
| train/                  |             |
|    approx_kl            | 0.014543829 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 318         |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.000918   |
|    value_loss           | 903         |
-----------------------------------------
Num timesteps: 3372000
Best mean reward: 3041.83 - Last mean reward per episode: 3020.75
Num timesteps: 3384000
Best mean reward: 3041.83 - Last mean reward per episode: 2905.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2968.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 138         |
|    time_elapsed         | 40272       |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.013495799 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 438         |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.000891   |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 3396000
Best mean reward: 3041.83 - Last mean reward per episode: 2959.75
Num timesteps: 3408000
Best mean reward: 3041.83 - Last mean reward per episode: 2966.82
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.64e+03   |
|    ep_rew_mean          | 3003.27    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 139        |
|    time_elapsed         | 40561      |
|    total_timesteps      | 3416064    |
| train/                  |            |
|    approx_kl            | 0.01504125 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.45      |
|    explained_variance   | 0.939      |
|    learning_rate        | 3e-06      |
|    loss                 | 268        |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.000518  |
|    value_loss           | 835        |
----------------------------------------
Num timesteps: 3420000
Best mean reward: 3041.83 - Last mean reward per episode: 2979.57
Num timesteps: 3432000
Best mean reward: 3041.83 - Last mean reward per episode: 2991.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | 3008.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 140         |
|    time_elapsed         | 40849       |
|    total_timesteps      | 3440640     |
| train/                  |             |
|    approx_kl            | 0.014641945 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.934       |
|    learning_rate        | 3e-06       |
|    loss                 | 583         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 840         |
-----------------------------------------
Num timesteps: 3444000
Best mean reward: 3041.83 - Last mean reward per episode: 3000.57
Num timesteps: 3456000
Best mean reward: 3041.83 - Last mean reward per episode: 2935.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 2913.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 141         |
|    time_elapsed         | 41139       |
|    total_timesteps      | 3465216     |
| train/                  |             |
|    approx_kl            | 0.014062027 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 386         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.000148   |
|    value_loss           | 817         |
-----------------------------------------
Num timesteps: 3468000
Best mean reward: 3041.83 - Last mean reward per episode: 2890.76
Num timesteps: 3480000
Best mean reward: 3041.83 - Last mean reward per episode: 2833.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.58e+03    |
|    ep_rew_mean          | 2868.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 142         |
|    time_elapsed         | 41427       |
|    total_timesteps      | 3489792     |
| train/                  |             |
|    approx_kl            | 0.015076528 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-06       |
|    loss                 | 724         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 978         |
-----------------------------------------
Num timesteps: 3492000
Best mean reward: 3041.83 - Last mean reward per episode: 2860.71
Num timesteps: 3504000
Best mean reward: 3041.83 - Last mean reward per episode: 2778.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.55e+03    |
|    ep_rew_mean          | 2774.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 143         |
|    time_elapsed         | 41715       |
|    total_timesteps      | 3514368     |
| train/                  |             |
|    approx_kl            | 0.016032754 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 790         |
-----------------------------------------
Num timesteps: 3516000
Best mean reward: 3041.83 - Last mean reward per episode: 2776.87
Num timesteps: 3528000
Best mean reward: 3041.83 - Last mean reward per episode: 2793.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.54e+03    |
|    ep_rew_mean          | 2766.19     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 144         |
|    time_elapsed         | 42003       |
|    total_timesteps      | 3538944     |
| train/                  |             |
|    approx_kl            | 0.015780615 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.941       |
|    learning_rate        | 3e-06       |
|    loss                 | 349         |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.000708    |
|    value_loss           | 766         |
-----------------------------------------
Num timesteps: 3540000
Best mean reward: 3041.83 - Last mean reward per episode: 2766.19
Num timesteps: 3552000
Best mean reward: 3041.83 - Last mean reward per episode: 2759.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.49e+03    |
|    ep_rew_mean          | 2701.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 145         |
|    time_elapsed         | 42297       |
|    total_timesteps      | 3563520     |
| train/                  |             |
|    approx_kl            | 0.013999782 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 417         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 906         |
-----------------------------------------
Num timesteps: 3564000
Best mean reward: 3041.83 - Last mean reward per episode: 2701.21
Num timesteps: 3576000
Best mean reward: 3041.83 - Last mean reward per episode: 2731.44
Num timesteps: 3588000
Best mean reward: 3041.83 - Last mean reward per episode: 2722.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.49e+03    |
|    ep_rew_mean          | 2722.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 146         |
|    time_elapsed         | 42582       |
|    total_timesteps      | 3588096     |
| train/                  |             |
|    approx_kl            | 0.014627159 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 200         |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.000812   |
|    value_loss           | 900         |
-----------------------------------------
Num timesteps: 3600000
Best mean reward: 3041.83 - Last mean reward per episode: 2748.66
Num timesteps: 3612000
Best mean reward: 3041.83 - Last mean reward per episode: 2755.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.52e+03    |
|    ep_rew_mean          | 2784.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 147         |
|    time_elapsed         | 42869       |
|    total_timesteps      | 3612672     |
| train/                  |             |
|    approx_kl            | 0.013867349 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 377         |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 815         |
-----------------------------------------
Num timesteps: 3624000
Best mean reward: 3041.83 - Last mean reward per episode: 2775.31
Num timesteps: 3636000
Best mean reward: 3041.83 - Last mean reward per episode: 2815.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.51e+03     |
|    ep_rew_mean          | 2815.73      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 148          |
|    time_elapsed         | 43162        |
|    total_timesteps      | 3637248      |
| train/                  |              |
|    approx_kl            | 0.0141060455 |
|    clip_fraction        | 0.172        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.936        |
|    learning_rate        | 3e-06        |
|    loss                 | 860          |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 882          |
------------------------------------------
Num timesteps: 3648000
Best mean reward: 3041.83 - Last mean reward per episode: 2814.87
Num timesteps: 3660000
Best mean reward: 3041.83 - Last mean reward per episode: 2853.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.53e+03     |
|    ep_rew_mean          | 2853.8       |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 149          |
|    time_elapsed         | 43450        |
|    total_timesteps      | 3661824      |
| train/                  |              |
|    approx_kl            | 0.0144949285 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.938        |
|    learning_rate        | 3e-06        |
|    loss                 | 264          |
|    n_updates            | 1480         |
|    policy_gradient_loss | 0.000682     |
|    value_loss           | 798          |
------------------------------------------
Num timesteps: 3672000
Best mean reward: 3041.83 - Last mean reward per episode: 2905.70
Num timesteps: 3684000
Best mean reward: 3041.83 - Last mean reward per episode: 2908.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.56e+03    |
|    ep_rew_mean          | 2904.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 150         |
|    time_elapsed         | 43734       |
|    total_timesteps      | 3686400     |
| train/                  |             |
|    approx_kl            | 0.016059527 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 395         |
|    n_updates            | 1490        |
|    policy_gradient_loss | 0.000598    |
|    value_loss           | 691         |
-----------------------------------------
Num timesteps: 3696000
Best mean reward: 3041.83 - Last mean reward per episode: 2964.09
Num timesteps: 3708000
Best mean reward: 3041.83 - Last mean reward per episode: 2981.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2956.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 151         |
|    time_elapsed         | 44028       |
|    total_timesteps      | 3710976     |
| train/                  |             |
|    approx_kl            | 0.016128702 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-06       |
|    loss                 | 162         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 733         |
-----------------------------------------
Num timesteps: 3720000
Best mean reward: 3041.83 - Last mean reward per episode: 2998.41
Num timesteps: 3732000
Best mean reward: 3041.83 - Last mean reward per episode: 2959.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | 2931.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 152         |
|    time_elapsed         | 44316       |
|    total_timesteps      | 3735552     |
| train/                  |             |
|    approx_kl            | 0.017992176 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 515         |
|    n_updates            | 1510        |
|    policy_gradient_loss | -6.66e-05   |
|    value_loss           | 815         |
-----------------------------------------
Num timesteps: 3744000
Best mean reward: 3041.83 - Last mean reward per episode: 2939.62
Num timesteps: 3756000
Best mean reward: 3041.83 - Last mean reward per episode: 2995.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2969.5      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 153         |
|    time_elapsed         | 44601       |
|    total_timesteps      | 3760128     |
| train/                  |             |
|    approx_kl            | 0.014874942 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 563         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000783   |
|    value_loss           | 836         |
-----------------------------------------
Num timesteps: 3768000
Best mean reward: 3041.83 - Last mean reward per episode: 2950.16
Num timesteps: 3780000
Best mean reward: 3041.83 - Last mean reward per episode: 2948.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2930.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 154         |
|    time_elapsed         | 44891       |
|    total_timesteps      | 3784704     |
| train/                  |             |
|    approx_kl            | 0.016286185 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-06       |
|    loss                 | 540         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 791         |
-----------------------------------------
Num timesteps: 3792000
Best mean reward: 3041.83 - Last mean reward per episode: 2931.46
Num timesteps: 3804000
Best mean reward: 3041.83 - Last mean reward per episode: 2899.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 2891.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 155         |
|    time_elapsed         | 45179       |
|    total_timesteps      | 3809280     |
| train/                  |             |
|    approx_kl            | 0.016315596 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.934       |
|    learning_rate        | 3e-06       |
|    loss                 | 492         |
|    n_updates            | 1540        |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 774         |
-----------------------------------------
Num timesteps: 3816000
Best mean reward: 3041.83 - Last mean reward per episode: 2880.03
Num timesteps: 3828000
Best mean reward: 3041.83 - Last mean reward per episode: 2884.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | 2893.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 156         |
|    time_elapsed         | 45467       |
|    total_timesteps      | 3833856     |
| train/                  |             |
|    approx_kl            | 0.015604153 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 1550        |
|    policy_gradient_loss | 0.000296    |
|    value_loss           | 650         |
-----------------------------------------
Num timesteps: 3840000
Best mean reward: 3041.83 - Last mean reward per episode: 2908.50
Num timesteps: 3852000
Best mean reward: 3041.83 - Last mean reward per episode: 2873.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 2840.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 157         |
|    time_elapsed         | 45747       |
|    total_timesteps      | 3858432     |
| train/                  |             |
|    approx_kl            | 0.015586416 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 174         |
|    n_updates            | 1560        |
|    policy_gradient_loss | 0.000261    |
|    value_loss           | 515         |
-----------------------------------------
Num timesteps: 3864000
Best mean reward: 3041.83 - Last mean reward per episode: 2813.92
Num timesteps: 3876000
Best mean reward: 3041.83 - Last mean reward per episode: 2811.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.55e+03    |
|    ep_rew_mean          | 2796.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 158         |
|    time_elapsed         | 46031       |
|    total_timesteps      | 3883008     |
| train/                  |             |
|    approx_kl            | 0.015966116 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 425         |
|    n_updates            | 1570        |
|    policy_gradient_loss | 0.000467    |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 3888000
Best mean reward: 3041.83 - Last mean reward per episode: 2833.66
Num timesteps: 3900000
Best mean reward: 3041.83 - Last mean reward per episode: 2867.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 2861.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 159         |
|    time_elapsed         | 46314       |
|    total_timesteps      | 3907584     |
| train/                  |             |
|    approx_kl            | 0.016004337 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 154         |
|    n_updates            | 1580        |
|    policy_gradient_loss | -7.43e-05   |
|    value_loss           | 670         |
-----------------------------------------
Num timesteps: 3912000
Best mean reward: 3041.83 - Last mean reward per episode: 2865.97
Num timesteps: 3924000
Best mean reward: 3041.83 - Last mean reward per episode: 2890.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 2875.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 160         |
|    time_elapsed         | 46602       |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.016894247 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 549         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 757         |
-----------------------------------------
Num timesteps: 3936000
Best mean reward: 3041.83 - Last mean reward per episode: 2893.41
Num timesteps: 3948000
Best mean reward: 3041.83 - Last mean reward per episode: 2942.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | 2968.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 161         |
|    time_elapsed         | 46885       |
|    total_timesteps      | 3956736     |
| train/                  |             |
|    approx_kl            | 0.015770683 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 198         |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.00026     |
|    value_loss           | 758         |
-----------------------------------------
Num timesteps: 3960000
Best mean reward: 3041.83 - Last mean reward per episode: 2947.49
Num timesteps: 3972000
Best mean reward: 3041.83 - Last mean reward per episode: 3003.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.67e+03   |
|    ep_rew_mean          | 3024.45    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 162        |
|    time_elapsed         | 47175      |
|    total_timesteps      | 3981312    |
| train/                  |            |
|    approx_kl            | 0.01430741 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.13      |
|    explained_variance   | 0.949      |
|    learning_rate        | 3e-06      |
|    loss                 | 531        |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0017    |
|    value_loss           | 643        |
----------------------------------------
Num timesteps: 3984000
Best mean reward: 3041.83 - Last mean reward per episode: 3024.45
Num timesteps: 3996000
Best mean reward: 3041.83 - Last mean reward per episode: 3043.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 3021.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 163         |
|    time_elapsed         | 47467       |
|    total_timesteps      | 4005888     |
| train/                  |             |
|    approx_kl            | 0.015978927 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 241         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 692         |
-----------------------------------------
Num timesteps: 4008000
Best mean reward: 3043.59 - Last mean reward per episode: 3021.26
Num timesteps: 4020000
Best mean reward: 3043.59 - Last mean reward per episode: 3051.85
Saving new best model to tmp/best_model
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.7e+03   |
|    ep_rew_mean          | 3073.19   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 164       |
|    time_elapsed         | 47756     |
|    total_timesteps      | 4030464   |
| train/                  |           |
|    approx_kl            | 0.0144925 |
|    clip_fraction        | 0.168     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.15     |
|    explained_variance   | 0.95      |
|    learning_rate        | 3e-06     |
|    loss                 | 318       |
|    n_updates            | 1630      |
|    policy_gradient_loss | -0.000691 |
|    value_loss           | 574       |
---------------------------------------
Num timesteps: 4032000
Best mean reward: 3051.85 - Last mean reward per episode: 3093.99
Saving new best model to tmp/best_model
Num timesteps: 4044000
Best mean reward: 3093.99 - Last mean reward per episode: 3166.78
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3228.73     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 165         |
|    time_elapsed         | 48053       |
|    total_timesteps      | 4055040     |
| train/                  |             |
|    approx_kl            | 0.018574199 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.06       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 479         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 757         |
-----------------------------------------
Num timesteps: 4056000
Best mean reward: 3166.78 - Last mean reward per episode: 3228.73
Saving new best model to tmp/best_model
Num timesteps: 4068000
Best mean reward: 3228.73 - Last mean reward per episode: 3208.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.82e+03    |
|    ep_rew_mean          | 3264.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 166         |
|    time_elapsed         | 48350       |
|    total_timesteps      | 4079616     |
| train/                  |             |
|    approx_kl            | 0.014356631 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.12       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 436         |
|    n_updates            | 1650        |
|    policy_gradient_loss | 0.000397    |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 4080000
Best mean reward: 3228.73 - Last mean reward per episode: 3264.84
Saving new best model to tmp/best_model
Num timesteps: 4092000
Best mean reward: 3264.84 - Last mean reward per episode: 3305.36
Saving new best model to tmp/best_model
Num timesteps: 4104000
Best mean reward: 3305.36 - Last mean reward per episode: 3365.07
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3365.07     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 167         |
|    time_elapsed         | 48640       |
|    total_timesteps      | 4104192     |
| train/                  |             |
|    approx_kl            | 0.015412747 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 157         |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.000299   |
|    value_loss           | 591         |
-----------------------------------------
Num timesteps: 4116000
Best mean reward: 3365.07 - Last mean reward per episode: 3405.57
Saving new best model to tmp/best_model
Num timesteps: 4128000
Best mean reward: 3405.57 - Last mean reward per episode: 3407.46
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3420.64     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 168         |
|    time_elapsed         | 48932       |
|    total_timesteps      | 4128768     |
| train/                  |             |
|    approx_kl            | 0.016776461 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.07       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 208         |
|    n_updates            | 1670        |
|    policy_gradient_loss | 0.000857    |
|    value_loss           | 641         |
-----------------------------------------
Num timesteps: 4140000
Best mean reward: 3407.46 - Last mean reward per episode: 3385.96
Num timesteps: 4152000
Best mean reward: 3407.46 - Last mean reward per episode: 3365.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3365.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 169         |
|    time_elapsed         | 49226       |
|    total_timesteps      | 4153344     |
| train/                  |             |
|    approx_kl            | 0.016707271 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.06       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 481         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.000536   |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 4164000
Best mean reward: 3407.46 - Last mean reward per episode: 3349.80
Num timesteps: 4176000
Best mean reward: 3407.46 - Last mean reward per episode: 3412.42
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3425.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 170         |
|    time_elapsed         | 49517       |
|    total_timesteps      | 4177920     |
| train/                  |             |
|    approx_kl            | 0.018226525 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.925       |
|    learning_rate        | 3e-06       |
|    loss                 | 532         |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 867         |
-----------------------------------------
Num timesteps: 4188000
Best mean reward: 3412.42 - Last mean reward per episode: 3462.72
Saving new best model to tmp/best_model
Num timesteps: 4200000
Best mean reward: 3462.72 - Last mean reward per episode: 3382.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.89e+03    |
|    ep_rew_mean          | 3373.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 171         |
|    time_elapsed         | 49813       |
|    total_timesteps      | 4202496     |
| train/                  |             |
|    approx_kl            | 0.014467967 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.95       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 259         |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 4212000
Best mean reward: 3462.72 - Last mean reward per episode: 3370.44
Num timesteps: 4224000
Best mean reward: 3462.72 - Last mean reward per episode: 3387.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3387.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 172         |
|    time_elapsed         | 50108       |
|    total_timesteps      | 4227072     |
| train/                  |             |
|    approx_kl            | 0.015969101 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.96       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 369         |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.000841   |
|    value_loss           | 785         |
-----------------------------------------
Num timesteps: 4236000
Best mean reward: 3462.72 - Last mean reward per episode: 3392.16
Num timesteps: 4248000
Best mean reward: 3462.72 - Last mean reward per episode: 3405.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3422.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 173         |
|    time_elapsed         | 50403       |
|    total_timesteps      | 4251648     |
| train/                  |             |
|    approx_kl            | 0.016785135 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 348         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 694         |
-----------------------------------------
Num timesteps: 4260000
Best mean reward: 3462.72 - Last mean reward per episode: 3424.69
Num timesteps: 4272000
Best mean reward: 3462.72 - Last mean reward per episode: 3353.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.85e+03    |
|    ep_rew_mean          | 3328.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 174         |
|    time_elapsed         | 50691       |
|    total_timesteps      | 4276224     |
| train/                  |             |
|    approx_kl            | 0.015238852 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3          |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 583         |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.000435    |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 4284000
Best mean reward: 3462.72 - Last mean reward per episode: 3280.32
Num timesteps: 4296000
Best mean reward: 3462.72 - Last mean reward per episode: 3272.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3222.71     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 175         |
|    time_elapsed         | 50984       |
|    total_timesteps      | 4300800     |
| train/                  |             |
|    approx_kl            | 0.017641693 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-06       |
|    loss                 | 155         |
|    n_updates            | 1740        |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 730         |
-----------------------------------------
Num timesteps: 4308000
Best mean reward: 3462.72 - Last mean reward per episode: 3265.93
Num timesteps: 4320000
Best mean reward: 3462.72 - Last mean reward per episode: 3317.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3315.69     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 176         |
|    time_elapsed         | 51274       |
|    total_timesteps      | 4325376     |
| train/                  |             |
|    approx_kl            | 0.017317701 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 316         |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.000255   |
|    value_loss           | 696         |
-----------------------------------------
Num timesteps: 4332000
Best mean reward: 3462.72 - Last mean reward per episode: 3285.94
Num timesteps: 4344000
Best mean reward: 3462.72 - Last mean reward per episode: 3307.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 3283.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 177         |
|    time_elapsed         | 51559       |
|    total_timesteps      | 4349952     |
| train/                  |             |
|    approx_kl            | 0.015922556 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.92       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 152         |
|    n_updates            | 1760        |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 727         |
-----------------------------------------
Num timesteps: 4356000
Best mean reward: 3462.72 - Last mean reward per episode: 3268.98
Num timesteps: 4368000
Best mean reward: 3462.72 - Last mean reward per episode: 3254.21
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.78e+03   |
|    ep_rew_mean          | 3237.62    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 178        |
|    time_elapsed         | 51848      |
|    total_timesteps      | 4374528    |
| train/                  |            |
|    approx_kl            | 0.01696252 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.81      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 152        |
|    n_updates            | 1770       |
|    policy_gradient_loss | 0.000342   |
|    value_loss           | 733        |
----------------------------------------
Num timesteps: 4380000
Best mean reward: 3462.72 - Last mean reward per episode: 3312.13
Num timesteps: 4392000
Best mean reward: 3462.72 - Last mean reward per episode: 3296.95
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.8e+03   |
|    ep_rew_mean          | 3275.97   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 179       |
|    time_elapsed         | 52142     |
|    total_timesteps      | 4399104   |
| train/                  |           |
|    approx_kl            | 0.0162282 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.9      |
|    explained_variance   | 0.938     |
|    learning_rate        | 3e-06     |
|    loss                 | 333       |
|    n_updates            | 1780      |
|    policy_gradient_loss | 0.000193  |
|    value_loss           | 738       |
---------------------------------------
Num timesteps: 4404000
Best mean reward: 3462.72 - Last mean reward per episode: 3286.23
Num timesteps: 4416000
Best mean reward: 3462.72 - Last mean reward per episode: 3248.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3277.2      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 180         |
|    time_elapsed         | 52431       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.017353082 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 248         |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 783         |
-----------------------------------------
Num timesteps: 4428000
Best mean reward: 3462.72 - Last mean reward per episode: 3252.52
Num timesteps: 4440000
Best mean reward: 3462.72 - Last mean reward per episode: 3290.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79e+03    |
|    ep_rew_mean          | 3294.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 181         |
|    time_elapsed         | 52715       |
|    total_timesteps      | 4448256     |
| train/                  |             |
|    approx_kl            | 0.015882721 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 229         |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.000659   |
|    value_loss           | 774         |
-----------------------------------------
Num timesteps: 4452000
Best mean reward: 3462.72 - Last mean reward per episode: 3303.42
Num timesteps: 4464000
Best mean reward: 3462.72 - Last mean reward per episode: 3274.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79e+03    |
|    ep_rew_mean          | 3321.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 182         |
|    time_elapsed         | 53007       |
|    total_timesteps      | 4472832     |
| train/                  |             |
|    approx_kl            | 0.016686583 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.73       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 383         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.000185   |
|    value_loss           | 750         |
-----------------------------------------
Num timesteps: 4476000
Best mean reward: 3462.72 - Last mean reward per episode: 3321.33
Num timesteps: 4488000
Best mean reward: 3462.72 - Last mean reward per episode: 3331.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.78e+03   |
|    ep_rew_mean          | 3325.39    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 183        |
|    time_elapsed         | 53296      |
|    total_timesteps      | 4497408    |
| train/                  |            |
|    approx_kl            | 0.01351476 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.82      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-06      |
|    loss                 | 614        |
|    n_updates            | 1820       |
|    policy_gradient_loss | 0.000365   |
|    value_loss           | 671        |
----------------------------------------
Num timesteps: 4500000
Best mean reward: 3462.72 - Last mean reward per episode: 3341.60
Num timesteps: 4512000
Best mean reward: 3462.72 - Last mean reward per episode: 3323.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.78e+03    |
|    ep_rew_mean          | 3305.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 184         |
|    time_elapsed         | 53581       |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.015340146 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.000432    |
|    value_loss           | 660         |
-----------------------------------------
Num timesteps: 4524000
Best mean reward: 3462.72 - Last mean reward per episode: 3318.49
Num timesteps: 4536000
Best mean reward: 3462.72 - Last mean reward per episode: 3330.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.81e+03    |
|    ep_rew_mean          | 3330.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 185         |
|    time_elapsed         | 53869       |
|    total_timesteps      | 4546560     |
| train/                  |             |
|    approx_kl            | 0.017965136 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 489         |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.000822    |
|    value_loss           | 775         |
-----------------------------------------
Num timesteps: 4548000
Best mean reward: 3462.72 - Last mean reward per episode: 3328.63
Num timesteps: 4560000
Best mean reward: 3462.72 - Last mean reward per episode: 3293.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.79e+03    |
|    ep_rew_mean          | 3311.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 186         |
|    time_elapsed         | 54153       |
|    total_timesteps      | 4571136     |
| train/                  |             |
|    approx_kl            | 0.017134087 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 1850        |
|    policy_gradient_loss | -5.82e-05   |
|    value_loss           | 792         |
-----------------------------------------
Num timesteps: 4572000
Best mean reward: 3462.72 - Last mean reward per episode: 3311.44
Num timesteps: 4584000
Best mean reward: 3462.72 - Last mean reward per episode: 3362.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3417.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 187         |
|    time_elapsed         | 54444       |
|    total_timesteps      | 4595712     |
| train/                  |             |
|    approx_kl            | 0.016535142 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 107         |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.000872    |
|    value_loss           | 663         |
-----------------------------------------
Num timesteps: 4596000
Best mean reward: 3462.72 - Last mean reward per episode: 3417.14
Num timesteps: 4608000
Best mean reward: 3462.72 - Last mean reward per episode: 3390.50
Num timesteps: 4620000
Best mean reward: 3462.72 - Last mean reward per episode: 3403.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.86e+03   |
|    ep_rew_mean          | 3403.89    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 188        |
|    time_elapsed         | 54736      |
|    total_timesteps      | 4620288    |
| train/                  |            |
|    approx_kl            | 0.01622552 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.79      |
|    explained_variance   | 0.946      |
|    learning_rate        | 3e-06      |
|    loss                 | 209        |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.000251  |
|    value_loss           | 706        |
----------------------------------------
Num timesteps: 4632000
Best mean reward: 3462.72 - Last mean reward per episode: 3451.42
Num timesteps: 4644000
Best mean reward: 3462.72 - Last mean reward per episode: 3453.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3453.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 189         |
|    time_elapsed         | 55023       |
|    total_timesteps      | 4644864     |
| train/                  |             |
|    approx_kl            | 0.016912669 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 136         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00021    |
|    value_loss           | 676         |
-----------------------------------------
Num timesteps: 4656000
Best mean reward: 3462.72 - Last mean reward per episode: 3427.93
Num timesteps: 4668000
Best mean reward: 3462.72 - Last mean reward per episode: 3381.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 3381.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 190         |
|    time_elapsed         | 55317       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.014539708 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 616         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.000373   |
|    value_loss           | 761         |
-----------------------------------------
Num timesteps: 4680000
Best mean reward: 3462.72 - Last mean reward per episode: 3433.41
Num timesteps: 4692000
Best mean reward: 3462.72 - Last mean reward per episode: 3484.56
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3484.56     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 191         |
|    time_elapsed         | 55605       |
|    total_timesteps      | 4694016     |
| train/                  |             |
|    approx_kl            | 0.014710087 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-06       |
|    loss                 | 224         |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.000622    |
|    value_loss           | 735         |
-----------------------------------------
Num timesteps: 4704000
Best mean reward: 3484.56 - Last mean reward per episode: 3519.98
Saving new best model to tmp/best_model
Num timesteps: 4716000
Best mean reward: 3519.98 - Last mean reward per episode: 3497.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3511.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 192         |
|    time_elapsed         | 55899       |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.018377298 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 508         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.000702   |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 4728000
Best mean reward: 3519.98 - Last mean reward per episode: 3539.20
Saving new best model to tmp/best_model
Num timesteps: 4740000
Best mean reward: 3539.20 - Last mean reward per episode: 3574.12
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.9e+03     |
|    ep_rew_mean          | 3604.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 193         |
|    time_elapsed         | 56190       |
|    total_timesteps      | 4743168     |
| train/                  |             |
|    approx_kl            | 0.015418839 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 496         |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 708         |
-----------------------------------------
Num timesteps: 4752000
Best mean reward: 3574.12 - Last mean reward per episode: 3606.54
Saving new best model to tmp/best_model
Num timesteps: 4764000
Best mean reward: 3606.54 - Last mean reward per episode: 3654.38
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3564.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 194         |
|    time_elapsed         | 56481       |
|    total_timesteps      | 4767744     |
| train/                  |             |
|    approx_kl            | 0.016191138 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 410         |
|    n_updates            | 1930        |
|    policy_gradient_loss | 0.00162     |
|    value_loss           | 566         |
-----------------------------------------
Num timesteps: 4776000
Best mean reward: 3654.38 - Last mean reward per episode: 3559.98
Num timesteps: 4788000
Best mean reward: 3654.38 - Last mean reward per episode: 3543.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3559.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 195         |
|    time_elapsed         | 56774       |
|    total_timesteps      | 4792320     |
| train/                  |             |
|    approx_kl            | 0.017339587 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 253         |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 680         |
-----------------------------------------
Num timesteps: 4800000
Best mean reward: 3654.38 - Last mean reward per episode: 3638.69
Num timesteps: 4812000
Best mean reward: 3654.38 - Last mean reward per episode: 3647.18
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3598.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 196         |
|    time_elapsed         | 57063       |
|    total_timesteps      | 4816896     |
| train/                  |             |
|    approx_kl            | 0.016968014 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 364         |
|    n_updates            | 1950        |
|    policy_gradient_loss | 0.000774    |
|    value_loss           | 656         |
-----------------------------------------
Num timesteps: 4824000
Best mean reward: 3654.38 - Last mean reward per episode: 3634.16
Num timesteps: 4836000
Best mean reward: 3654.38 - Last mean reward per episode: 3618.71
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 3639.14    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 197        |
|    time_elapsed         | 57353      |
|    total_timesteps      | 4841472    |
| train/                  |            |
|    approx_kl            | 0.01741175 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.55      |
|    explained_variance   | 0.945      |
|    learning_rate        | 3e-06      |
|    loss                 | 271        |
|    n_updates            | 1960       |
|    policy_gradient_loss | 0.000992   |
|    value_loss           | 705        |
----------------------------------------
Num timesteps: 4848000
Best mean reward: 3654.38 - Last mean reward per episode: 3679.15
Saving new best model to tmp/best_model
Num timesteps: 4860000
Best mean reward: 3679.15 - Last mean reward per episode: 3731.73
Saving new best model to tmp/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.03e+03   |
|    ep_rew_mean          | 3739.84    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 198        |
|    time_elapsed         | 57645      |
|    total_timesteps      | 4866048    |
| train/                  |            |
|    approx_kl            | 0.01564765 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.47      |
|    explained_variance   | 0.953      |
|    learning_rate        | 3e-06      |
|    loss                 | 96.2       |
|    n_updates            | 1970       |
|    policy_gradient_loss | -2.59e-05  |
|    value_loss           | 563        |
----------------------------------------
Num timesteps: 4872000
Best mean reward: 3731.73 - Last mean reward per episode: 3779.27
Saving new best model to tmp/best_model
Num timesteps: 4884000
Best mean reward: 3779.27 - Last mean reward per episode: 3697.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.04e+03    |
|    ep_rew_mean          | 3749.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 199         |
|    time_elapsed         | 57937       |
|    total_timesteps      | 4890624     |
| train/                  |             |
|    approx_kl            | 0.016198417 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 487         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 572         |
-----------------------------------------
Num timesteps: 4896000
Best mean reward: 3779.27 - Last mean reward per episode: 3738.88
Num timesteps: 4908000
Best mean reward: 3779.27 - Last mean reward per episode: 3700.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.03e+03    |
|    ep_rew_mean          | 3702.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 200         |
|    time_elapsed         | 58228       |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.015727252 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 615         |
-----------------------------------------
Num timesteps: 4920000
Best mean reward: 3779.27 - Last mean reward per episode: 3680.93
Num timesteps: 4932000
Best mean reward: 3779.27 - Last mean reward per episode: 3631.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.98e+03    |
|    ep_rew_mean          | 3625.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 201         |
|    time_elapsed         | 58517       |
|    total_timesteps      | 4939776     |
| train/                  |             |
|    approx_kl            | 0.016767478 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 189         |
|    n_updates            | 2000        |
|    policy_gradient_loss | 0.00127     |
|    value_loss           | 772         |
-----------------------------------------
Num timesteps: 4944000
Best mean reward: 3779.27 - Last mean reward per episode: 3649.64
Num timesteps: 4956000
Best mean reward: 3779.27 - Last mean reward per episode: 3625.80
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3607.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 202         |
|    time_elapsed         | 58812       |
|    total_timesteps      | 4964352     |
| train/                  |             |
|    approx_kl            | 0.015426074 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.000123   |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 4968000
Best mean reward: 3779.27 - Last mean reward per episode: 3673.64
Num timesteps: 4980000
Best mean reward: 3779.27 - Last mean reward per episode: 3666.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 3711.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 203         |
|    time_elapsed         | 59102       |
|    total_timesteps      | 4988928     |
| train/                  |             |
|    approx_kl            | 0.014566359 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 354         |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.000896   |
|    value_loss           | 571         |
-----------------------------------------
Num timesteps: 4992000
Best mean reward: 3779.27 - Last mean reward per episode: 3711.01
Num timesteps: 5004000
Best mean reward: 3779.27 - Last mean reward per episode: 3648.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.96e+03   |
|    ep_rew_mean          | 3632.66    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 204        |
|    time_elapsed         | 59391      |
|    total_timesteps      | 5013504    |
| train/                  |            |
|    approx_kl            | 0.01694342 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.5       |
|    explained_variance   | 0.949      |
|    learning_rate        | 3e-06      |
|    loss                 | 482        |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.00074   |
|    value_loss           | 648        |
----------------------------------------
Num timesteps: 5016000
Best mean reward: 3779.27 - Last mean reward per episode: 3659.68
Num timesteps: 5028000
Best mean reward: 3779.27 - Last mean reward per episode: 3635.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.96e+03    |
|    ep_rew_mean          | 3644.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 205         |
|    time_elapsed         | 59684       |
|    total_timesteps      | 5038080     |
| train/                  |             |
|    approx_kl            | 0.017274795 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 2040        |
|    policy_gradient_loss | 0.00039     |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 5040000
Best mean reward: 3779.27 - Last mean reward per episode: 3601.72
Num timesteps: 5052000
Best mean reward: 3779.27 - Last mean reward per episode: 3524.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3530.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 206         |
|    time_elapsed         | 59976       |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.017165827 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 356         |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.000199   |
|    value_loss           | 640         |
-----------------------------------------
Num timesteps: 5064000
Best mean reward: 3779.27 - Last mean reward per episode: 3506.89
Num timesteps: 5076000
Best mean reward: 3779.27 - Last mean reward per episode: 3576.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3543.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 207         |
|    time_elapsed         | 60268       |
|    total_timesteps      | 5087232     |
| train/                  |             |
|    approx_kl            | 0.015144569 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 259         |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.000536   |
|    value_loss           | 600         |
-----------------------------------------
Num timesteps: 5088000
Best mean reward: 3779.27 - Last mean reward per episode: 3543.08
Num timesteps: 5100000
Best mean reward: 3779.27 - Last mean reward per episode: 3518.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.98e+03    |
|    ep_rew_mean          | 3624.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 208         |
|    time_elapsed         | 60562       |
|    total_timesteps      | 5111808     |
| train/                  |             |
|    approx_kl            | 0.017252347 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 575         |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.00223     |
|    value_loss           | 662         |
-----------------------------------------
Num timesteps: 5112000
Best mean reward: 3779.27 - Last mean reward per episode: 3624.32
Num timesteps: 5124000
Best mean reward: 3779.27 - Last mean reward per episode: 3593.06
Num timesteps: 5136000
Best mean reward: 3779.27 - Last mean reward per episode: 3539.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3519.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 209         |
|    time_elapsed         | 60849       |
|    total_timesteps      | 5136384     |
| train/                  |             |
|    approx_kl            | 0.014714618 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.000753   |
|    value_loss           | 606         |
-----------------------------------------
Num timesteps: 5148000
Best mean reward: 3779.27 - Last mean reward per episode: 3542.59
Num timesteps: 5160000
Best mean reward: 3779.27 - Last mean reward per episode: 3545.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3532.44     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 210         |
|    time_elapsed         | 61140       |
|    total_timesteps      | 5160960     |
| train/                  |             |
|    approx_kl            | 0.016082378 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 546         |
|    n_updates            | 2090        |
|    policy_gradient_loss | 0.000773    |
|    value_loss           | 818         |
-----------------------------------------
Num timesteps: 5172000
Best mean reward: 3779.27 - Last mean reward per episode: 3502.53
Num timesteps: 5184000
Best mean reward: 3779.27 - Last mean reward per episode: 3535.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3478.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 211         |
|    time_elapsed         | 61429       |
|    total_timesteps      | 5185536     |
| train/                  |             |
|    approx_kl            | 0.014460501 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 179         |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 542         |
-----------------------------------------
Num timesteps: 5196000
Best mean reward: 3779.27 - Last mean reward per episode: 3523.80
Num timesteps: 5208000
Best mean reward: 3779.27 - Last mean reward per episode: 3496.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3496.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 212         |
|    time_elapsed         | 61717       |
|    total_timesteps      | 5210112     |
| train/                  |             |
|    approx_kl            | 0.018668545 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.937       |
|    learning_rate        | 3e-06       |
|    loss                 | 454         |
|    n_updates            | 2110        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 782         |
-----------------------------------------
Num timesteps: 5220000
Best mean reward: 3779.27 - Last mean reward per episode: 3503.79
Num timesteps: 5232000
Best mean reward: 3779.27 - Last mean reward per episode: 3503.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 3505.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 213         |
|    time_elapsed         | 62006       |
|    total_timesteps      | 5234688     |
| train/                  |             |
|    approx_kl            | 0.017423699 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 302         |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 777         |
-----------------------------------------
Num timesteps: 5244000
Best mean reward: 3779.27 - Last mean reward per episode: 3541.43
Num timesteps: 5256000
Best mean reward: 3779.27 - Last mean reward per episode: 3582.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3543.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 214         |
|    time_elapsed         | 62289       |
|    total_timesteps      | 5259264     |
| train/                  |             |
|    approx_kl            | 0.016555464 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 440         |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.000874   |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 5268000
Best mean reward: 3779.27 - Last mean reward per episode: 3484.85
Num timesteps: 5280000
Best mean reward: 3779.27 - Last mean reward per episode: 3566.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3578.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 215         |
|    time_elapsed         | 62576       |
|    total_timesteps      | 5283840     |
| train/                  |             |
|    approx_kl            | 0.017517606 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 217         |
|    n_updates            | 2140        |
|    policy_gradient_loss | -5.59e-05   |
|    value_loss           | 615         |
-----------------------------------------
Num timesteps: 5292000
Best mean reward: 3779.27 - Last mean reward per episode: 3569.94
Num timesteps: 5304000
Best mean reward: 3779.27 - Last mean reward per episode: 3502.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 3476.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 216         |
|    time_elapsed         | 62864       |
|    total_timesteps      | 5308416     |
| train/                  |             |
|    approx_kl            | 0.018087786 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.000754   |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 5316000
Best mean reward: 3779.27 - Last mean reward per episode: 3551.09
Num timesteps: 5328000
Best mean reward: 3779.27 - Last mean reward per episode: 3586.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 3585.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 217         |
|    time_elapsed         | 63153       |
|    total_timesteps      | 5332992     |
| train/                  |             |
|    approx_kl            | 0.019146912 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.94        |
|    learning_rate        | 3e-06       |
|    loss                 | 437         |
|    n_updates            | 2160        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 716         |
-----------------------------------------
Num timesteps: 5340000
Best mean reward: 3779.27 - Last mean reward per episode: 3619.67
Num timesteps: 5352000
Best mean reward: 3779.27 - Last mean reward per episode: 3600.85
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 3600.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 218        |
|    time_elapsed         | 63438      |
|    total_timesteps      | 5357568    |
| train/                  |            |
|    approx_kl            | 0.01950214 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.2       |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-06      |
|    loss                 | 280        |
|    n_updates            | 2170       |
|    policy_gradient_loss | 0.000529   |
|    value_loss           | 605        |
----------------------------------------
Num timesteps: 5364000
Best mean reward: 3779.27 - Last mean reward per episode: 3611.74
Num timesteps: 5376000
Best mean reward: 3779.27 - Last mean reward per episode: 3601.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3619.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 219         |
|    time_elapsed         | 63725       |
|    total_timesteps      | 5382144     |
| train/                  |             |
|    approx_kl            | 0.020262333 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 292         |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.000318    |
|    value_loss           | 742         |
-----------------------------------------
Num timesteps: 5388000
Best mean reward: 3779.27 - Last mean reward per episode: 3586.85
Num timesteps: 5400000
Best mean reward: 3779.27 - Last mean reward per episode: 3610.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.93e+03    |
|    ep_rew_mean          | 3601.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 220         |
|    time_elapsed         | 64014       |
|    total_timesteps      | 5406720     |
| train/                  |             |
|    approx_kl            | 0.017967856 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 254         |
|    n_updates            | 2190        |
|    policy_gradient_loss | 0.000872    |
|    value_loss           | 629         |
-----------------------------------------
Num timesteps: 5412000
Best mean reward: 3779.27 - Last mean reward per episode: 3592.21
Num timesteps: 5424000
Best mean reward: 3779.27 - Last mean reward per episode: 3590.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3577.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 221         |
|    time_elapsed         | 64303       |
|    total_timesteps      | 5431296     |
| train/                  |             |
|    approx_kl            | 0.019447403 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 275         |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.00107     |
|    value_loss           | 666         |
-----------------------------------------
Num timesteps: 5436000
Best mean reward: 3779.27 - Last mean reward per episode: 3581.53
Num timesteps: 5448000
Best mean reward: 3779.27 - Last mean reward per episode: 3573.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3600.1      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 222         |
|    time_elapsed         | 64592       |
|    total_timesteps      | 5455872     |
| train/                  |             |
|    approx_kl            | 0.018407077 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-06       |
|    loss                 | 302         |
|    n_updates            | 2210        |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 759         |
-----------------------------------------
Num timesteps: 5460000
Best mean reward: 3779.27 - Last mean reward per episode: 3625.36
Num timesteps: 5472000
Best mean reward: 3779.27 - Last mean reward per episode: 3567.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.92e+03    |
|    ep_rew_mean          | 3566.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 223         |
|    time_elapsed         | 64878       |
|    total_timesteps      | 5480448     |
| train/                  |             |
|    approx_kl            | 0.015997674 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 210         |
|    n_updates            | 2220        |
|    policy_gradient_loss | -6.93e-05   |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 5484000
Best mean reward: 3779.27 - Last mean reward per episode: 3581.50
Num timesteps: 5496000
Best mean reward: 3779.27 - Last mean reward per episode: 3554.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3591.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 224         |
|    time_elapsed         | 65164       |
|    total_timesteps      | 5505024     |
| train/                  |             |
|    approx_kl            | 0.016519511 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 343         |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 646         |
-----------------------------------------
Num timesteps: 5508000
Best mean reward: 3779.27 - Last mean reward per episode: 3553.30
Num timesteps: 5520000
Best mean reward: 3779.27 - Last mean reward per episode: 3570.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3511.24     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 225         |
|    time_elapsed         | 65452       |
|    total_timesteps      | 5529600     |
| train/                  |             |
|    approx_kl            | 0.016194478 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 378         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.000398   |
|    value_loss           | 685         |
-----------------------------------------
Num timesteps: 5532000
Best mean reward: 3779.27 - Last mean reward per episode: 3492.69
Num timesteps: 5544000
Best mean reward: 3779.27 - Last mean reward per episode: 3517.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3480.02     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 226         |
|    time_elapsed         | 65738       |
|    total_timesteps      | 5554176     |
| train/                  |             |
|    approx_kl            | 0.016967686 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 278         |
|    n_updates            | 2250        |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 731         |
-----------------------------------------
Num timesteps: 5556000
Best mean reward: 3779.27 - Last mean reward per episode: 3504.05
Num timesteps: 5568000
Best mean reward: 3779.27 - Last mean reward per episode: 3490.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3515.25     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 227         |
|    time_elapsed         | 66024       |
|    total_timesteps      | 5578752     |
| train/                  |             |
|    approx_kl            | 0.019021597 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 423         |
|    n_updates            | 2260        |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 727         |
-----------------------------------------
Num timesteps: 5580000
Best mean reward: 3779.27 - Last mean reward per episode: 3515.25
Num timesteps: 5592000
Best mean reward: 3779.27 - Last mean reward per episode: 3470.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 3431.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 228         |
|    time_elapsed         | 66315       |
|    total_timesteps      | 5603328     |
| train/                  |             |
|    approx_kl            | 0.019252945 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 613         |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 5604000
Best mean reward: 3779.27 - Last mean reward per episode: 3431.61
Num timesteps: 5616000
Best mean reward: 3779.27 - Last mean reward per episode: 3443.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3475.06     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 229         |
|    time_elapsed         | 66600       |
|    total_timesteps      | 5627904     |
| train/                  |             |
|    approx_kl            | 0.017678538 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 2280        |
|    policy_gradient_loss | 0.0006      |
|    value_loss           | 799         |
-----------------------------------------
Num timesteps: 5628000
Best mean reward: 3779.27 - Last mean reward per episode: 3475.06
Num timesteps: 5640000
Best mean reward: 3779.27 - Last mean reward per episode: 3540.91
Num timesteps: 5652000
Best mean reward: 3779.27 - Last mean reward per episode: 3525.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.89e+03    |
|    ep_rew_mean          | 3525.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 230         |
|    time_elapsed         | 66884       |
|    total_timesteps      | 5652480     |
| train/                  |             |
|    approx_kl            | 0.016453987 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 285         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.000111   |
|    value_loss           | 571         |
-----------------------------------------
Num timesteps: 5664000
Best mean reward: 3779.27 - Last mean reward per episode: 3595.04
Num timesteps: 5676000
Best mean reward: 3779.27 - Last mean reward per episode: 3605.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.94e+03    |
|    ep_rew_mean          | 3605.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 231         |
|    time_elapsed         | 67171       |
|    total_timesteps      | 5677056     |
| train/                  |             |
|    approx_kl            | 0.017445462 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 344         |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.000996    |
|    value_loss           | 465         |
-----------------------------------------
Num timesteps: 5688000
Best mean reward: 3779.27 - Last mean reward per episode: 3634.70
Num timesteps: 5700000
Best mean reward: 3779.27 - Last mean reward per episode: 3622.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3622.3      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 232         |
|    time_elapsed         | 67456       |
|    total_timesteps      | 5701632     |
| train/                  |             |
|    approx_kl            | 0.018791676 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 2310        |
|    policy_gradient_loss | -5.89e-05   |
|    value_loss           | 609         |
-----------------------------------------
Num timesteps: 5712000
Best mean reward: 3779.27 - Last mean reward per episode: 3670.51
Num timesteps: 5724000
Best mean reward: 3779.27 - Last mean reward per episode: 3679.47
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 3719.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 233         |
|    time_elapsed         | 67740       |
|    total_timesteps      | 5726208     |
| train/                  |             |
|    approx_kl            | 0.016734779 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 602         |
|    n_updates            | 2320        |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 659         |
-----------------------------------------
Num timesteps: 5736000
Best mean reward: 3779.27 - Last mean reward per episode: 3754.51
Num timesteps: 5748000
Best mean reward: 3779.27 - Last mean reward per episode: 3741.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.02e+03   |
|    ep_rew_mean          | 3741.68    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 234        |
|    time_elapsed         | 68028      |
|    total_timesteps      | 5750784    |
| train/                  |            |
|    approx_kl            | 0.01842521 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.13      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-06      |
|    loss                 | 301        |
|    n_updates            | 2330       |
|    policy_gradient_loss | 0.00226    |
|    value_loss           | 726        |
----------------------------------------
Num timesteps: 5760000
Best mean reward: 3779.27 - Last mean reward per episode: 3778.87
Num timesteps: 5772000
Best mean reward: 3779.27 - Last mean reward per episode: 3753.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.02e+03    |
|    ep_rew_mean          | 3739.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 235         |
|    time_elapsed         | 68311       |
|    total_timesteps      | 5775360     |
| train/                  |             |
|    approx_kl            | 0.017576674 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 563         |
-----------------------------------------
Num timesteps: 5784000
Best mean reward: 3779.27 - Last mean reward per episode: 3767.12
Num timesteps: 5796000
Best mean reward: 3779.27 - Last mean reward per episode: 3801.85
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 3808.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 236         |
|    time_elapsed         | 68606       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.016971024 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 330         |
|    n_updates            | 2350        |
|    policy_gradient_loss | 0.000806    |
|    value_loss           | 607         |
-----------------------------------------
Num timesteps: 5808000
Best mean reward: 3801.85 - Last mean reward per episode: 3881.02
Saving new best model to tmp/best_model
Num timesteps: 5820000
Best mean reward: 3881.02 - Last mean reward per episode: 3919.15
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3901.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 237         |
|    time_elapsed         | 68892       |
|    total_timesteps      | 5824512     |
| train/                  |             |
|    approx_kl            | 0.017119043 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 219         |
|    n_updates            | 2360        |
|    policy_gradient_loss | 0.000595    |
|    value_loss           | 592         |
-----------------------------------------
Num timesteps: 5832000
Best mean reward: 3919.15 - Last mean reward per episode: 3915.22
Num timesteps: 5844000
Best mean reward: 3919.15 - Last mean reward per episode: 3815.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 3815.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 238         |
|    time_elapsed         | 69182       |
|    total_timesteps      | 5849088     |
| train/                  |             |
|    approx_kl            | 0.018147739 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-06       |
|    loss                 | 260         |
|    n_updates            | 2370        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 681         |
-----------------------------------------
Num timesteps: 5856000
Best mean reward: 3919.15 - Last mean reward per episode: 3767.29
Num timesteps: 5868000
Best mean reward: 3919.15 - Last mean reward per episode: 3724.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.99e+03    |
|    ep_rew_mean          | 3678.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 239         |
|    time_elapsed         | 69468       |
|    total_timesteps      | 5873664     |
| train/                  |             |
|    approx_kl            | 0.018999299 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 398         |
|    n_updates            | 2380        |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 662         |
-----------------------------------------
Num timesteps: 5880000
Best mean reward: 3919.15 - Last mean reward per episode: 3611.11
Num timesteps: 5892000
Best mean reward: 3919.15 - Last mean reward per episode: 3591.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3648.59     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 240         |
|    time_elapsed         | 69755       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.018024618 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 611         |
|    n_updates            | 2390        |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 721         |
-----------------------------------------
Num timesteps: 5904000
Best mean reward: 3919.15 - Last mean reward per episode: 3603.66
Num timesteps: 5916000
Best mean reward: 3919.15 - Last mean reward per episode: 3656.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.96e+03    |
|    ep_rew_mean          | 3639.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 241         |
|    time_elapsed         | 70044       |
|    total_timesteps      | 5922816     |
| train/                  |             |
|    approx_kl            | 0.019709446 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 342         |
|    n_updates            | 2400        |
|    policy_gradient_loss | 0.00146     |
|    value_loss           | 632         |
-----------------------------------------
Num timesteps: 5928000
Best mean reward: 3919.15 - Last mean reward per episode: 3606.05
Num timesteps: 5940000
Best mean reward: 3919.15 - Last mean reward per episode: 3567.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3515.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 242         |
|    time_elapsed         | 70332       |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.019253766 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 360         |
|    n_updates            | 2410        |
|    policy_gradient_loss | 0.000469    |
|    value_loss           | 703         |
-----------------------------------------
Num timesteps: 5952000
Best mean reward: 3919.15 - Last mean reward per episode: 3568.61
Num timesteps: 5964000
Best mean reward: 3919.15 - Last mean reward per episode: 3588.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.91e+03   |
|    ep_rew_mean          | 3572.14    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 243        |
|    time_elapsed         | 70622      |
|    total_timesteps      | 5971968    |
| train/                  |            |
|    approx_kl            | 0.01803161 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.21      |
|    explained_variance   | 0.94       |
|    learning_rate        | 3e-06      |
|    loss                 | 141        |
|    n_updates            | 2420       |
|    policy_gradient_loss | 0.00163    |
|    value_loss           | 735        |
----------------------------------------
Num timesteps: 5976000
Best mean reward: 3919.15 - Last mean reward per episode: 3537.44
Num timesteps: 5988000
Best mean reward: 3919.15 - Last mean reward per episode: 3524.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3522.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 244         |
|    time_elapsed         | 70914       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.017028103 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 457         |
|    n_updates            | 2430        |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 618         |
-----------------------------------------
Num timesteps: 6000000
Best mean reward: 3919.15 - Last mean reward per episode: 3496.32
Num timesteps: 6012000
Best mean reward: 3919.15 - Last mean reward per episode: 3444.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.85e+03   |
|    ep_rew_mean          | 3455.63    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 245        |
|    time_elapsed         | 71207      |
|    total_timesteps      | 6021120    |
| train/                  |            |
|    approx_kl            | 0.01685966 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.25      |
|    explained_variance   | 0.947      |
|    learning_rate        | 3e-06      |
|    loss                 | 307        |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.000215  |
|    value_loss           | 762        |
----------------------------------------
Num timesteps: 6024000
Best mean reward: 3919.15 - Last mean reward per episode: 3447.75
Num timesteps: 6036000
Best mean reward: 3919.15 - Last mean reward per episode: 3454.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.82e+03    |
|    ep_rew_mean          | 3413.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 246         |
|    time_elapsed         | 71496       |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.017630203 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 158         |
|    n_updates            | 2450        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 580         |
-----------------------------------------
Num timesteps: 6048000
Best mean reward: 3919.15 - Last mean reward per episode: 3417.85
Num timesteps: 6060000
Best mean reward: 3919.15 - Last mean reward per episode: 3406.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 3529.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 247         |
|    time_elapsed         | 71787       |
|    total_timesteps      | 6070272     |
| train/                  |             |
|    approx_kl            | 0.017468745 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 447         |
|    n_updates            | 2460        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 736         |
-----------------------------------------
Num timesteps: 6072000
Best mean reward: 3919.15 - Last mean reward per episode: 3529.99
Num timesteps: 6084000
Best mean reward: 3919.15 - Last mean reward per episode: 3511.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3445.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 248         |
|    time_elapsed         | 72076       |
|    total_timesteps      | 6094848     |
| train/                  |             |
|    approx_kl            | 0.014237042 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 419         |
|    n_updates            | 2470        |
|    policy_gradient_loss | 0.000188    |
|    value_loss           | 629         |
-----------------------------------------
Num timesteps: 6096000
Best mean reward: 3919.15 - Last mean reward per episode: 3451.86
Num timesteps: 6108000
Best mean reward: 3919.15 - Last mean reward per episode: 3434.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.83e+03    |
|    ep_rew_mean          | 3416.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 249         |
|    time_elapsed         | 72368       |
|    total_timesteps      | 6119424     |
| train/                  |             |
|    approx_kl            | 0.017243816 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.941       |
|    learning_rate        | 3e-06       |
|    loss                 | 118         |
|    n_updates            | 2480        |
|    policy_gradient_loss | 0.000359    |
|    value_loss           | 756         |
-----------------------------------------
Num timesteps: 6120000
Best mean reward: 3919.15 - Last mean reward per episode: 3416.62
Num timesteps: 6132000
Best mean reward: 3919.15 - Last mean reward per episode: 3456.66
Num timesteps: 6144000
Best mean reward: 3919.15 - Last mean reward per episode: 3348.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.81e+03    |
|    ep_rew_mean          | 3348.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 250         |
|    time_elapsed         | 72659       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.017931895 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 160         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.000253   |
|    value_loss           | 730         |
-----------------------------------------
Num timesteps: 6156000
Best mean reward: 3919.15 - Last mean reward per episode: 3345.51
Num timesteps: 6168000
Best mean reward: 3919.15 - Last mean reward per episode: 3323.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3323.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 251         |
|    time_elapsed         | 72947       |
|    total_timesteps      | 6168576     |
| train/                  |             |
|    approx_kl            | 0.014457566 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 199         |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.00015    |
|    value_loss           | 620         |
-----------------------------------------
Num timesteps: 6180000
Best mean reward: 3919.15 - Last mean reward per episode: 3295.09
Num timesteps: 6192000
Best mean reward: 3919.15 - Last mean reward per episode: 3304.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.84e+03    |
|    ep_rew_mean          | 3304.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 252         |
|    time_elapsed         | 73233       |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.015899532 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 253         |
|    n_updates            | 2510        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 648         |
-----------------------------------------
Num timesteps: 6204000
Best mean reward: 3919.15 - Last mean reward per episode: 3360.36
Num timesteps: 6216000
Best mean reward: 3919.15 - Last mean reward per episode: 3373.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.86e+03    |
|    ep_rew_mean          | 3375.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 253         |
|    time_elapsed         | 73524       |
|    total_timesteps      | 6217728     |
| train/                  |             |
|    approx_kl            | 0.018288903 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 506         |
|    n_updates            | 2520        |
|    policy_gradient_loss | 0.00358     |
|    value_loss           | 675         |
-----------------------------------------
Num timesteps: 6228000
Best mean reward: 3919.15 - Last mean reward per episode: 3431.76
Num timesteps: 6240000
Best mean reward: 3919.15 - Last mean reward per episode: 3538.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3538.96     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 254         |
|    time_elapsed         | 73814       |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.017133737 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 2530        |
|    policy_gradient_loss | 0.00142     |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 6252000
Best mean reward: 3919.15 - Last mean reward per episode: 3607.96
Num timesteps: 6264000
Best mean reward: 3919.15 - Last mean reward per episode: 3593.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.01e+03   |
|    ep_rew_mean          | 3593.14    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 255        |
|    time_elapsed         | 74098      |
|    total_timesteps      | 6266880    |
| train/                  |            |
|    approx_kl            | 0.01476219 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.08      |
|    explained_variance   | 0.957      |
|    learning_rate        | 3e-06      |
|    loss                 | 238        |
|    n_updates            | 2540       |
|    policy_gradient_loss | 0.00235    |
|    value_loss           | 492        |
----------------------------------------
Num timesteps: 6276000
Best mean reward: 3919.15 - Last mean reward per episode: 3611.81
Num timesteps: 6288000
Best mean reward: 3919.15 - Last mean reward per episode: 3662.35
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.06e+03   |
|    ep_rew_mean          | 3658.87    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 256        |
|    time_elapsed         | 74389      |
|    total_timesteps      | 6291456    |
| train/                  |            |
|    approx_kl            | 0.01786853 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.16      |
|    explained_variance   | 0.945      |
|    learning_rate        | 3e-06      |
|    loss                 | 344        |
|    n_updates            | 2550       |
|    policy_gradient_loss | 0.000317   |
|    value_loss           | 597        |
----------------------------------------
Num timesteps: 6300000
Best mean reward: 3919.15 - Last mean reward per episode: 3726.47
Num timesteps: 6312000
Best mean reward: 3919.15 - Last mean reward per episode: 3748.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.1e+03     |
|    ep_rew_mean          | 3748.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 257         |
|    time_elapsed         | 74675       |
|    total_timesteps      | 6316032     |
| train/                  |             |
|    approx_kl            | 0.016294615 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 227         |
|    n_updates            | 2560        |
|    policy_gradient_loss | 0.000429    |
|    value_loss           | 508         |
-----------------------------------------
Num timesteps: 6324000
Best mean reward: 3919.15 - Last mean reward per episode: 3762.82
Num timesteps: 6336000
Best mean reward: 3919.15 - Last mean reward per episode: 3816.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 3905.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 258         |
|    time_elapsed         | 74966       |
|    total_timesteps      | 6340608     |
| train/                  |             |
|    approx_kl            | 0.016512819 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 215         |
|    n_updates            | 2570        |
|    policy_gradient_loss | 0.00031     |
|    value_loss           | 585         |
-----------------------------------------
Num timesteps: 6348000
Best mean reward: 3919.15 - Last mean reward per episode: 3942.35
Saving new best model to tmp/best_model
Num timesteps: 6360000
Best mean reward: 3942.35 - Last mean reward per episode: 3956.27
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 4010.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 259         |
|    time_elapsed         | 75257       |
|    total_timesteps      | 6365184     |
| train/                  |             |
|    approx_kl            | 0.013151406 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 197         |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 6372000
Best mean reward: 3956.27 - Last mean reward per episode: 4027.52
Saving new best model to tmp/best_model
Num timesteps: 6384000
Best mean reward: 4027.52 - Last mean reward per episode: 4084.24
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4080.52     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 260         |
|    time_elapsed         | 75556       |
|    total_timesteps      | 6389760     |
| train/                  |             |
|    approx_kl            | 0.016379038 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 130         |
|    n_updates            | 2590        |
|    policy_gradient_loss | 0.00266     |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 6396000
Best mean reward: 4084.24 - Last mean reward per episode: 4061.73
Num timesteps: 6408000
Best mean reward: 4084.24 - Last mean reward per episode: 4056.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4120.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 261         |
|    time_elapsed         | 75846       |
|    total_timesteps      | 6414336     |
| train/                  |             |
|    approx_kl            | 0.017820157 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-06       |
|    loss                 | 346         |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 608         |
-----------------------------------------
Num timesteps: 6420000
Best mean reward: 4084.24 - Last mean reward per episode: 4151.12
Saving new best model to tmp/best_model
Num timesteps: 6432000
Best mean reward: 4151.12 - Last mean reward per episode: 4114.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 4064.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 262         |
|    time_elapsed         | 76138       |
|    total_timesteps      | 6438912     |
| train/                  |             |
|    approx_kl            | 0.016239377 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 432         |
|    n_updates            | 2610        |
|    policy_gradient_loss | 0.00253     |
|    value_loss           | 736         |
-----------------------------------------
Num timesteps: 6444000
Best mean reward: 4151.12 - Last mean reward per episode: 4060.36
Num timesteps: 6456000
Best mean reward: 4151.12 - Last mean reward per episode: 3949.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.09e+03    |
|    ep_rew_mean          | 3937.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 263         |
|    time_elapsed         | 76429       |
|    total_timesteps      | 6463488     |
| train/                  |             |
|    approx_kl            | 0.015181187 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 614         |
|    n_updates            | 2620        |
|    policy_gradient_loss | 0.00165     |
|    value_loss           | 788         |
-----------------------------------------
Num timesteps: 6468000
Best mean reward: 4151.12 - Last mean reward per episode: 3912.62
Num timesteps: 6480000
Best mean reward: 4151.12 - Last mean reward per episode: 3907.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 3832.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 264         |
|    time_elapsed         | 76720       |
|    total_timesteps      | 6488064     |
| train/                  |             |
|    approx_kl            | 0.014928181 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 137         |
|    n_updates            | 2630        |
|    policy_gradient_loss | 2.95e-05    |
|    value_loss           | 785         |
-----------------------------------------
Num timesteps: 6492000
Best mean reward: 4151.12 - Last mean reward per episode: 3774.61
Num timesteps: 6504000
Best mean reward: 4151.12 - Last mean reward per episode: 3761.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3713.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 265         |
|    time_elapsed         | 77007       |
|    total_timesteps      | 6512640     |
| train/                  |             |
|    approx_kl            | 0.016521906 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-06       |
|    loss                 | 171         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.000882   |
|    value_loss           | 845         |
-----------------------------------------
Num timesteps: 6516000
Best mean reward: 4151.12 - Last mean reward per episode: 3696.72
Num timesteps: 6528000
Best mean reward: 4151.12 - Last mean reward per episode: 3701.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3682.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 266         |
|    time_elapsed         | 77293       |
|    total_timesteps      | 6537216     |
| train/                  |             |
|    approx_kl            | 0.017519178 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 387         |
|    n_updates            | 2650        |
|    policy_gradient_loss | 0.000971    |
|    value_loss           | 682         |
-----------------------------------------
Num timesteps: 6540000
Best mean reward: 4151.12 - Last mean reward per episode: 3709.60
Num timesteps: 6552000
Best mean reward: 4151.12 - Last mean reward per episode: 3706.92
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.92e+03   |
|    ep_rew_mean          | 3694.6     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 267        |
|    time_elapsed         | 77578      |
|    total_timesteps      | 6561792    |
| train/                  |            |
|    approx_kl            | 0.01581587 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.958      |
|    learning_rate        | 3e-06      |
|    loss                 | 481        |
|    n_updates            | 2660       |
|    policy_gradient_loss | -5.78e-05  |
|    value_loss           | 570        |
----------------------------------------
Num timesteps: 6564000
Best mean reward: 4151.12 - Last mean reward per episode: 3718.62
Num timesteps: 6576000
Best mean reward: 4151.12 - Last mean reward per episode: 3676.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.91e+03    |
|    ep_rew_mean          | 3700.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 268         |
|    time_elapsed         | 77859       |
|    total_timesteps      | 6586368     |
| train/                  |             |
|    approx_kl            | 0.016641645 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 376         |
|    n_updates            | 2670        |
|    policy_gradient_loss | 0.000522    |
|    value_loss           | 718         |
-----------------------------------------
Num timesteps: 6588000
Best mean reward: 4151.12 - Last mean reward per episode: 3700.61
Num timesteps: 6600000
Best mean reward: 4151.12 - Last mean reward per episode: 3774.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.97e+03    |
|    ep_rew_mean          | 3777.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 269         |
|    time_elapsed         | 78147       |
|    total_timesteps      | 6610944     |
| train/                  |             |
|    approx_kl            | 0.015157097 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 451         |
|    n_updates            | 2680        |
|    policy_gradient_loss | 0.000154    |
|    value_loss           | 679         |
-----------------------------------------
Num timesteps: 6612000
Best mean reward: 4151.12 - Last mean reward per episode: 3784.66
Num timesteps: 6624000
Best mean reward: 4151.12 - Last mean reward per episode: 3784.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.98e+03    |
|    ep_rew_mean          | 3797.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 270         |
|    time_elapsed         | 78432       |
|    total_timesteps      | 6635520     |
| train/                  |             |
|    approx_kl            | 0.017190909 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 328         |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00037    |
|    value_loss           | 600         |
-----------------------------------------
Num timesteps: 6636000
Best mean reward: 4151.12 - Last mean reward per episode: 3797.91
Num timesteps: 6648000
Best mean reward: 4151.12 - Last mean reward per episode: 3793.71
Num timesteps: 6660000
Best mean reward: 4151.12 - Last mean reward per episode: 3896.60
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.03e+03   |
|    ep_rew_mean          | 3896.6     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 271        |
|    time_elapsed         | 78723      |
|    total_timesteps      | 6660096    |
| train/                  |            |
|    approx_kl            | 0.01464206 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-06      |
|    loss                 | 234        |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.000212  |
|    value_loss           | 671        |
----------------------------------------
Num timesteps: 6672000
Best mean reward: 4151.12 - Last mean reward per episode: 3969.38
Num timesteps: 6684000
Best mean reward: 4151.12 - Last mean reward per episode: 3955.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.06e+03    |
|    ep_rew_mean          | 3959.13     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 272         |
|    time_elapsed         | 79007       |
|    total_timesteps      | 6684672     |
| train/                  |             |
|    approx_kl            | 0.012097545 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 292         |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.000606   |
|    value_loss           | 544         |
-----------------------------------------
Num timesteps: 6696000
Best mean reward: 4151.12 - Last mean reward per episode: 4020.76
Num timesteps: 6708000
Best mean reward: 4151.12 - Last mean reward per episode: 4126.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16e+03    |
|    ep_rew_mean          | 4126.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 273         |
|    time_elapsed         | 79300       |
|    total_timesteps      | 6709248     |
| train/                  |             |
|    approx_kl            | 0.016959159 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 438         |
|    n_updates            | 2720        |
|    policy_gradient_loss | 6.48e-05    |
|    value_loss           | 688         |
-----------------------------------------
Num timesteps: 6720000
Best mean reward: 4151.12 - Last mean reward per episode: 4158.14
Saving new best model to tmp/best_model
Num timesteps: 6732000
Best mean reward: 4158.14 - Last mean reward per episode: 4227.41
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 4205.51     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 274         |
|    time_elapsed         | 79587       |
|    total_timesteps      | 6733824     |
| train/                  |             |
|    approx_kl            | 0.015613199 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00031    |
|    value_loss           | 631         |
-----------------------------------------
Num timesteps: 6744000
Best mean reward: 4227.41 - Last mean reward per episode: 4209.26
Num timesteps: 6756000
Best mean reward: 4227.41 - Last mean reward per episode: 4191.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 4191.54     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 275         |
|    time_elapsed         | 79887       |
|    total_timesteps      | 6758400     |
| train/                  |             |
|    approx_kl            | 0.018181214 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 888         |
|    n_updates            | 2740        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 667         |
-----------------------------------------
Num timesteps: 6768000
Best mean reward: 4227.41 - Last mean reward per episode: 4185.78
Num timesteps: 6780000
Best mean reward: 4227.41 - Last mean reward per episode: 4195.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 4178.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 276         |
|    time_elapsed         | 80181       |
|    total_timesteps      | 6782976     |
| train/                  |             |
|    approx_kl            | 0.017357908 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 725         |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.000698   |
|    value_loss           | 640         |
-----------------------------------------
Num timesteps: 6792000
Best mean reward: 4227.41 - Last mean reward per episode: 4190.86
Num timesteps: 6804000
Best mean reward: 4227.41 - Last mean reward per episode: 4181.72
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.19e+03   |
|    ep_rew_mean          | 4181.72    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 277        |
|    time_elapsed         | 80478      |
|    total_timesteps      | 6807552    |
| train/                  |            |
|    approx_kl            | 0.01595109 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.05      |
|    explained_variance   | 0.952      |
|    learning_rate        | 3e-06      |
|    loss                 | 509        |
|    n_updates            | 2760       |
|    policy_gradient_loss | 0.000864   |
|    value_loss           | 639        |
----------------------------------------
Num timesteps: 6816000
Best mean reward: 4227.41 - Last mean reward per episode: 4273.19
Saving new best model to tmp/best_model
Num timesteps: 6828000
Best mean reward: 4273.19 - Last mean reward per episode: 4279.59
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 4259.23     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 278         |
|    time_elapsed         | 80774       |
|    total_timesteps      | 6832128     |
| train/                  |             |
|    approx_kl            | 0.017008437 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 284         |
|    n_updates            | 2770        |
|    policy_gradient_loss | 0.00149     |
|    value_loss           | 582         |
-----------------------------------------
Num timesteps: 6840000
Best mean reward: 4279.59 - Last mean reward per episode: 4215.50
Num timesteps: 6852000
Best mean reward: 4279.59 - Last mean reward per episode: 4223.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4215.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 279         |
|    time_elapsed         | 81062       |
|    total_timesteps      | 6856704     |
| train/                  |             |
|    approx_kl            | 0.014638805 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 446         |
|    n_updates            | 2780        |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 658         |
-----------------------------------------
Num timesteps: 6864000
Best mean reward: 4279.59 - Last mean reward per episode: 4199.69
Num timesteps: 6876000
Best mean reward: 4279.59 - Last mean reward per episode: 4132.09
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.17e+03   |
|    ep_rew_mean          | 4101.44    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 280        |
|    time_elapsed         | 81355      |
|    total_timesteps      | 6881280    |
| train/                  |            |
|    approx_kl            | 0.01931112 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-06      |
|    loss                 | 297        |
|    n_updates            | 2790       |
|    policy_gradient_loss | 0.000932   |
|    value_loss           | 828        |
----------------------------------------
Num timesteps: 6888000
Best mean reward: 4279.59 - Last mean reward per episode: 4037.23
Num timesteps: 6900000
Best mean reward: 4279.59 - Last mean reward per episode: 4078.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.16e+03   |
|    ep_rew_mean          | 4078.43    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 281        |
|    time_elapsed         | 81643      |
|    total_timesteps      | 6905856    |
| train/                  |            |
|    approx_kl            | 0.01837156 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-06      |
|    loss                 | 158        |
|    n_updates            | 2800       |
|    policy_gradient_loss | 0.00181    |
|    value_loss           | 675        |
----------------------------------------
Num timesteps: 6912000
Best mean reward: 4279.59 - Last mean reward per episode: 4078.38
Num timesteps: 6924000
Best mean reward: 4279.59 - Last mean reward per episode: 4043.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.12e+03    |
|    ep_rew_mean          | 3998.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 282         |
|    time_elapsed         | 81931       |
|    total_timesteps      | 6930432     |
| train/                  |             |
|    approx_kl            | 0.016751915 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 669         |
|    n_updates            | 2810        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 520         |
-----------------------------------------
Num timesteps: 6936000
Best mean reward: 4279.59 - Last mean reward per episode: 3980.97
Num timesteps: 6948000
Best mean reward: 4279.59 - Last mean reward per episode: 3951.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.14e+03    |
|    ep_rew_mean          | 3907.75     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 283         |
|    time_elapsed         | 82219       |
|    total_timesteps      | 6955008     |
| train/                  |             |
|    approx_kl            | 0.017386034 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-06       |
|    loss                 | 625         |
|    n_updates            | 2820        |
|    policy_gradient_loss | 0.00065     |
|    value_loss           | 679         |
-----------------------------------------
Num timesteps: 6960000
Best mean reward: 4279.59 - Last mean reward per episode: 3920.05
Num timesteps: 6972000
Best mean reward: 4279.59 - Last mean reward per episode: 3893.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3891.64     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 284         |
|    time_elapsed         | 82510       |
|    total_timesteps      | 6979584     |
| train/                  |             |
|    approx_kl            | 0.015911696 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 152         |
|    n_updates            | 2830        |
|    policy_gradient_loss | 0.000719    |
|    value_loss           | 585         |
-----------------------------------------
Num timesteps: 6984000
Best mean reward: 4279.59 - Last mean reward per episode: 3896.48
Num timesteps: 6996000
Best mean reward: 4279.59 - Last mean reward per episode: 3930.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.16e+03    |
|    ep_rew_mean          | 3981.86     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 285         |
|    time_elapsed         | 82801       |
|    total_timesteps      | 7004160     |
| train/                  |             |
|    approx_kl            | 0.017226363 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 248         |
|    n_updates            | 2840        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 604         |
-----------------------------------------
Num timesteps: 7008000
Best mean reward: 4279.59 - Last mean reward per episode: 4006.92
Num timesteps: 7020000
Best mean reward: 4279.59 - Last mean reward per episode: 3983.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 3886.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 286         |
|    time_elapsed         | 83096       |
|    total_timesteps      | 7028736     |
| train/                  |             |
|    approx_kl            | 0.016687933 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 169         |
|    n_updates            | 2850        |
|    policy_gradient_loss | 0.0026      |
|    value_loss           | 525         |
-----------------------------------------
Num timesteps: 7032000
Best mean reward: 4279.59 - Last mean reward per episode: 3907.78
Num timesteps: 7044000
Best mean reward: 4279.59 - Last mean reward per episode: 3911.48
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.12e+03     |
|    ep_rew_mean          | 3928.29      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 287          |
|    time_elapsed         | 83400        |
|    total_timesteps      | 7053312      |
| train/                  |              |
|    approx_kl            | 0.0155957425 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.957        |
|    learning_rate        | 3e-06        |
|    loss                 | 110          |
|    n_updates            | 2860         |
|    policy_gradient_loss | 0.00031      |
|    value_loss           | 566          |
------------------------------------------
Num timesteps: 7056000
Best mean reward: 4279.59 - Last mean reward per episode: 3956.98
Num timesteps: 7068000
Best mean reward: 4279.59 - Last mean reward per episode: 4000.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 3991.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 288         |
|    time_elapsed         | 83694       |
|    total_timesteps      | 7077888     |
| train/                  |             |
|    approx_kl            | 0.013727456 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.000968   |
|    value_loss           | 527         |
-----------------------------------------
Num timesteps: 7080000
Best mean reward: 4279.59 - Last mean reward per episode: 3991.80
Num timesteps: 7092000
Best mean reward: 4279.59 - Last mean reward per episode: 4026.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4078.16     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 289         |
|    time_elapsed         | 83993       |
|    total_timesteps      | 7102464     |
| train/                  |             |
|    approx_kl            | 0.018628003 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 263         |
|    n_updates            | 2880        |
|    policy_gradient_loss | 0.00199     |
|    value_loss           | 806         |
-----------------------------------------
Num timesteps: 7104000
Best mean reward: 4279.59 - Last mean reward per episode: 4065.45
Num timesteps: 7116000
Best mean reward: 4279.59 - Last mean reward per episode: 4072.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4108.8      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 290         |
|    time_elapsed         | 84282       |
|    total_timesteps      | 7127040     |
| train/                  |             |
|    approx_kl            | 0.017661292 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.943       |
|    learning_rate        | 3e-06       |
|    loss                 | 253         |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 727         |
-----------------------------------------
Num timesteps: 7128000
Best mean reward: 4279.59 - Last mean reward per episode: 4093.90
Num timesteps: 7140000
Best mean reward: 4279.59 - Last mean reward per episode: 4107.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4141.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 291         |
|    time_elapsed         | 84575       |
|    total_timesteps      | 7151616     |
| train/                  |             |
|    approx_kl            | 0.017762953 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 342         |
|    n_updates            | 2900        |
|    policy_gradient_loss | 0.000169    |
|    value_loss           | 645         |
-----------------------------------------
Num timesteps: 7152000
Best mean reward: 4279.59 - Last mean reward per episode: 4141.45
Num timesteps: 7164000
Best mean reward: 4279.59 - Last mean reward per episode: 4239.45
Num timesteps: 7176000
Best mean reward: 4279.59 - Last mean reward per episode: 4225.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.29e+03   |
|    ep_rew_mean          | 4225.34    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 292        |
|    time_elapsed         | 84867      |
|    total_timesteps      | 7176192    |
| train/                  |            |
|    approx_kl            | 0.01431597 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.964      |
|    learning_rate        | 3e-06      |
|    loss                 | 175        |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.00176   |
|    value_loss           | 511        |
----------------------------------------
Num timesteps: 7188000
Best mean reward: 4279.59 - Last mean reward per episode: 4242.30
Num timesteps: 7200000
Best mean reward: 4279.59 - Last mean reward per episode: 4242.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4242.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 293         |
|    time_elapsed         | 85154       |
|    total_timesteps      | 7200768     |
| train/                  |             |
|    approx_kl            | 0.015828034 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 147         |
|    n_updates            | 2920        |
|    policy_gradient_loss | 0.000881    |
|    value_loss           | 673         |
-----------------------------------------
Num timesteps: 7212000
Best mean reward: 4279.59 - Last mean reward per episode: 4157.32
Num timesteps: 7224000
Best mean reward: 4279.59 - Last mean reward per episode: 4153.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 4159.21     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 294         |
|    time_elapsed         | 85444       |
|    total_timesteps      | 7225344     |
| train/                  |             |
|    approx_kl            | 0.018185195 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-06       |
|    loss                 | 454         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.000188   |
|    value_loss           | 837         |
-----------------------------------------
Num timesteps: 7236000
Best mean reward: 4279.59 - Last mean reward per episode: 4174.47
Num timesteps: 7248000
Best mean reward: 4279.59 - Last mean reward per episode: 4176.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4215.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 295         |
|    time_elapsed         | 85731       |
|    total_timesteps      | 7249920     |
| train/                  |             |
|    approx_kl            | 0.017168047 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 202         |
|    n_updates            | 2940        |
|    policy_gradient_loss | 0.001       |
|    value_loss           | 595         |
-----------------------------------------
Num timesteps: 7260000
Best mean reward: 4279.59 - Last mean reward per episode: 4237.55
Num timesteps: 7272000
Best mean reward: 4279.59 - Last mean reward per episode: 4198.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4198.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 296         |
|    time_elapsed         | 86019       |
|    total_timesteps      | 7274496     |
| train/                  |             |
|    approx_kl            | 0.015197553 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-06       |
|    loss                 | 251         |
|    n_updates            | 2950        |
|    policy_gradient_loss | 0.000466    |
|    value_loss           | 583         |
-----------------------------------------
Num timesteps: 7284000
Best mean reward: 4279.59 - Last mean reward per episode: 4143.67
Num timesteps: 7296000
Best mean reward: 4279.59 - Last mean reward per episode: 4164.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4201.63     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 297         |
|    time_elapsed         | 86309       |
|    total_timesteps      | 7299072     |
| train/                  |             |
|    approx_kl            | 0.015453234 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 340         |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.000307   |
|    value_loss           | 579         |
-----------------------------------------
Num timesteps: 7308000
Best mean reward: 4279.59 - Last mean reward per episode: 4254.96
Num timesteps: 7320000
Best mean reward: 4279.59 - Last mean reward per episode: 4310.77
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4309.4      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 298         |
|    time_elapsed         | 86599       |
|    total_timesteps      | 7323648     |
| train/                  |             |
|    approx_kl            | 0.016761266 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 288         |
|    n_updates            | 2970        |
|    policy_gradient_loss | 0.000594    |
|    value_loss           | 728         |
-----------------------------------------
Num timesteps: 7332000
Best mean reward: 4310.77 - Last mean reward per episode: 4298.53
Num timesteps: 7344000
Best mean reward: 4310.77 - Last mean reward per episode: 4337.56
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 4351.37     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 299         |
|    time_elapsed         | 86886       |
|    total_timesteps      | 7348224     |
| train/                  |             |
|    approx_kl            | 0.015590479 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 229         |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.000607   |
|    value_loss           | 568         |
-----------------------------------------
Num timesteps: 7356000
Best mean reward: 4337.56 - Last mean reward per episode: 4348.55
Saving new best model to tmp/best_model
Num timesteps: 7368000
Best mean reward: 4348.55 - Last mean reward per episode: 4386.55
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4386.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 300         |
|    time_elapsed         | 87171       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.019293094 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 344         |
|    n_updates            | 2990        |
|    policy_gradient_loss | 0.000756    |
|    value_loss           | 674         |
-----------------------------------------
Num timesteps: 7380000
Best mean reward: 4386.55 - Last mean reward per episode: 4373.98
Num timesteps: 7392000
Best mean reward: 4386.55 - Last mean reward per episode: 4354.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4392.26     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 301         |
|    time_elapsed         | 87457       |
|    total_timesteps      | 7397376     |
| train/                  |             |
|    approx_kl            | 0.015814483 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 167         |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.000762   |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 7404000
Best mean reward: 4386.55 - Last mean reward per episode: 4419.10
Saving new best model to tmp/best_model
Num timesteps: 7416000
Best mean reward: 4419.10 - Last mean reward per episode: 4471.95
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4436.68     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 302         |
|    time_elapsed         | 87745       |
|    total_timesteps      | 7421952     |
| train/                  |             |
|    approx_kl            | 0.015627496 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 223         |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.000155   |
|    value_loss           | 439         |
-----------------------------------------
Num timesteps: 7428000
Best mean reward: 4471.95 - Last mean reward per episode: 4434.17
Num timesteps: 7440000
Best mean reward: 4471.95 - Last mean reward per episode: 4436.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4441.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 303         |
|    time_elapsed         | 88032       |
|    total_timesteps      | 7446528     |
| train/                  |             |
|    approx_kl            | 0.016305527 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 3020        |
|    policy_gradient_loss | 0.00135     |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 7452000
Best mean reward: 4471.95 - Last mean reward per episode: 4407.45
Num timesteps: 7464000
Best mean reward: 4471.95 - Last mean reward per episode: 4313.80
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.32e+03   |
|    ep_rew_mean          | 4313.8     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 304        |
|    time_elapsed         | 88317      |
|    total_timesteps      | 7471104    |
| train/                  |            |
|    approx_kl            | 0.01816682 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.958      |
|    learning_rate        | 3e-06      |
|    loss                 | 276        |
|    n_updates            | 3030       |
|    policy_gradient_loss | 1.31e-05   |
|    value_loss           | 623        |
----------------------------------------
Num timesteps: 7476000
Best mean reward: 4471.95 - Last mean reward per episode: 4313.80
Num timesteps: 7488000
Best mean reward: 4471.95 - Last mean reward per episode: 4268.02
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.31e+03  |
|    ep_rew_mean          | 4268.84   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 305       |
|    time_elapsed         | 88605     |
|    total_timesteps      | 7495680   |
| train/                  |           |
|    approx_kl            | 0.0195875 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.54     |
|    explained_variance   | 0.96      |
|    learning_rate        | 3e-06     |
|    loss                 | 121       |
|    n_updates            | 3040      |
|    policy_gradient_loss | 0.00323   |
|    value_loss           | 545       |
---------------------------------------
Num timesteps: 7500000
Best mean reward: 4471.95 - Last mean reward per episode: 4268.84
Num timesteps: 7512000
Best mean reward: 4471.95 - Last mean reward per episode: 4229.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4212.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 306         |
|    time_elapsed         | 88889       |
|    total_timesteps      | 7520256     |
| train/                  |             |
|    approx_kl            | 0.014813937 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 181         |
|    n_updates            | 3050        |
|    policy_gradient_loss | 0.000279    |
|    value_loss           | 631         |
-----------------------------------------
Num timesteps: 7524000
Best mean reward: 4471.95 - Last mean reward per episode: 4200.86
Num timesteps: 7536000
Best mean reward: 4471.95 - Last mean reward per episode: 4131.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4057.38     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 307         |
|    time_elapsed         | 89175       |
|    total_timesteps      | 7544832     |
| train/                  |             |
|    approx_kl            | 0.015307202 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 416         |
|    n_updates            | 3060        |
|    policy_gradient_loss | 0.000127    |
|    value_loss           | 666         |
-----------------------------------------
Num timesteps: 7548000
Best mean reward: 4471.95 - Last mean reward per episode: 4020.69
Num timesteps: 7560000
Best mean reward: 4471.95 - Last mean reward per episode: 3978.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 3980.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 308         |
|    time_elapsed         | 89459       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.015788194 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 125         |
|    n_updates            | 3070        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 508         |
-----------------------------------------
Num timesteps: 7572000
Best mean reward: 4471.95 - Last mean reward per episode: 3951.81
Num timesteps: 7584000
Best mean reward: 4471.95 - Last mean reward per episode: 3940.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.14e+03    |
|    ep_rew_mean          | 3853.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 309         |
|    time_elapsed         | 89745       |
|    total_timesteps      | 7593984     |
| train/                  |             |
|    approx_kl            | 0.021126384 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 177         |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 707         |
-----------------------------------------
Num timesteps: 7596000
Best mean reward: 4471.95 - Last mean reward per episode: 3874.66
Num timesteps: 7608000
Best mean reward: 4471.95 - Last mean reward per episode: 3826.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 3807.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 310         |
|    time_elapsed         | 90035       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.017743872 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 407         |
|    n_updates            | 3090        |
|    policy_gradient_loss | 0.00282     |
|    value_loss           | 735         |
-----------------------------------------
Num timesteps: 7620000
Best mean reward: 4471.95 - Last mean reward per episode: 3826.27
Num timesteps: 7632000
Best mean reward: 4471.95 - Last mean reward per episode: 3809.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.11e+03   |
|    ep_rew_mean          | 3801.19    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 311        |
|    time_elapsed         | 90324      |
|    total_timesteps      | 7643136    |
| train/                  |            |
|    approx_kl            | 0.01885411 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-06      |
|    loss                 | 188        |
|    n_updates            | 3100       |
|    policy_gradient_loss | 0.000413   |
|    value_loss           | 664        |
----------------------------------------
Num timesteps: 7644000
Best mean reward: 4471.95 - Last mean reward per episode: 3791.96
Num timesteps: 7656000
Best mean reward: 4471.95 - Last mean reward per episode: 3856.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3864.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 312         |
|    time_elapsed         | 90618       |
|    total_timesteps      | 7667712     |
| train/                  |             |
|    approx_kl            | 0.013781144 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 3110        |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 520         |
-----------------------------------------
Num timesteps: 7668000
Best mean reward: 4471.95 - Last mean reward per episode: 3877.56
Num timesteps: 7680000
Best mean reward: 4471.95 - Last mean reward per episode: 3895.82
Num timesteps: 7692000
Best mean reward: 4471.95 - Last mean reward per episode: 3947.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 3947.15     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 313         |
|    time_elapsed         | 90906       |
|    total_timesteps      | 7692288     |
| train/                  |             |
|    approx_kl            | 0.017858038 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 271         |
|    n_updates            | 3120        |
|    policy_gradient_loss | 0.00398     |
|    value_loss           | 667         |
-----------------------------------------
Num timesteps: 7704000
Best mean reward: 4471.95 - Last mean reward per episode: 3893.78
Num timesteps: 7716000
Best mean reward: 4471.95 - Last mean reward per episode: 3892.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.11e+03    |
|    ep_rew_mean          | 3892.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 314         |
|    time_elapsed         | 91196       |
|    total_timesteps      | 7716864     |
| train/                  |             |
|    approx_kl            | 0.017752724 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 502         |
|    n_updates            | 3130        |
|    policy_gradient_loss | 0.000646    |
|    value_loss           | 595         |
-----------------------------------------
Num timesteps: 7728000
Best mean reward: 4471.95 - Last mean reward per episode: 3947.16
Num timesteps: 7740000
Best mean reward: 4471.95 - Last mean reward per episode: 4019.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 4019.04     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 315         |
|    time_elapsed         | 91487       |
|    total_timesteps      | 7741440     |
| train/                  |             |
|    approx_kl            | 0.017806573 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 220         |
|    n_updates            | 3140        |
|    policy_gradient_loss | -2.74e-05   |
|    value_loss           | 596         |
-----------------------------------------
Num timesteps: 7752000
Best mean reward: 4471.95 - Last mean reward per episode: 4011.42
Num timesteps: 7764000
Best mean reward: 4471.95 - Last mean reward per episode: 4109.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4114.97     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 316         |
|    time_elapsed         | 91777       |
|    total_timesteps      | 7766016     |
| train/                  |             |
|    approx_kl            | 0.016432486 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 255         |
|    n_updates            | 3150        |
|    policy_gradient_loss | 0.00169     |
|    value_loss           | 562         |
-----------------------------------------
Num timesteps: 7776000
Best mean reward: 4471.95 - Last mean reward per episode: 4136.86
Num timesteps: 7788000
Best mean reward: 4471.95 - Last mean reward per episode: 4111.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4065.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 317         |
|    time_elapsed         | 92066       |
|    total_timesteps      | 7790592     |
| train/                  |             |
|    approx_kl            | 0.016766021 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 184         |
|    n_updates            | 3160        |
|    policy_gradient_loss | 0.000213    |
|    value_loss           | 566         |
-----------------------------------------
Num timesteps: 7800000
Best mean reward: 4471.95 - Last mean reward per episode: 4040.81
Num timesteps: 7812000
Best mean reward: 4471.95 - Last mean reward per episode: 4028.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | 4028.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 318         |
|    time_elapsed         | 92357       |
|    total_timesteps      | 7815168     |
| train/                  |             |
|    approx_kl            | 0.018249178 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-06       |
|    loss                 | 525         |
|    n_updates            | 3170        |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 808         |
-----------------------------------------
Num timesteps: 7824000
Best mean reward: 4471.95 - Last mean reward per episode: 4065.55
Num timesteps: 7836000
Best mean reward: 4471.95 - Last mean reward per episode: 4079.32
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4079.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 319         |
|    time_elapsed         | 92645       |
|    total_timesteps      | 7839744     |
| train/                  |             |
|    approx_kl            | 0.019673683 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-06       |
|    loss                 | 377         |
|    n_updates            | 3180        |
|    policy_gradient_loss | 0.00368     |
|    value_loss           | 737         |
-----------------------------------------
Num timesteps: 7848000
Best mean reward: 4471.95 - Last mean reward per episode: 4143.83
Num timesteps: 7860000
Best mean reward: 4471.95 - Last mean reward per episode: 4215.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4201.03     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 320         |
|    time_elapsed         | 92931       |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.014654775 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 226         |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 568         |
-----------------------------------------
Num timesteps: 7872000
Best mean reward: 4471.95 - Last mean reward per episode: 4224.52
Num timesteps: 7884000
Best mean reward: 4471.95 - Last mean reward per episode: 4194.41
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.27e+03  |
|    ep_rew_mean          | 4195.79   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 321       |
|    time_elapsed         | 93220     |
|    total_timesteps      | 7888896   |
| train/                  |           |
|    approx_kl            | 0.0161421 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.5      |
|    explained_variance   | 0.947     |
|    learning_rate        | 3e-06     |
|    loss                 | 296       |
|    n_updates            | 3200      |
|    policy_gradient_loss | -9.79e-05 |
|    value_loss           | 661       |
---------------------------------------
Num timesteps: 7896000
Best mean reward: 4471.95 - Last mean reward per episode: 4184.59
Num timesteps: 7908000
Best mean reward: 4471.95 - Last mean reward per episode: 4184.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 4188.35     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 322         |
|    time_elapsed         | 93508       |
|    total_timesteps      | 7913472     |
| train/                  |             |
|    approx_kl            | 0.016504316 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 3210        |
|    policy_gradient_loss | 0.00039     |
|    value_loss           | 576         |
-----------------------------------------
Num timesteps: 7920000
Best mean reward: 4471.95 - Last mean reward per episode: 4255.26
Num timesteps: 7932000
Best mean reward: 4471.95 - Last mean reward per episode: 4316.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4336.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 323         |
|    time_elapsed         | 93800       |
|    total_timesteps      | 7938048     |
| train/                  |             |
|    approx_kl            | 0.015827885 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 496         |
|    n_updates            | 3220        |
|    policy_gradient_loss | 0.000105    |
|    value_loss           | 555         |
-----------------------------------------
Num timesteps: 7944000
Best mean reward: 4471.95 - Last mean reward per episode: 4336.26
Num timesteps: 7956000
Best mean reward: 4471.95 - Last mean reward per episode: 4338.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.24e+03    |
|    ep_rew_mean          | 4313.88     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 324         |
|    time_elapsed         | 94088       |
|    total_timesteps      | 7962624     |
| train/                  |             |
|    approx_kl            | 0.015654888 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 205         |
|    n_updates            | 3230        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 593         |
-----------------------------------------
Num timesteps: 7968000
Best mean reward: 4471.95 - Last mean reward per episode: 4355.26
Num timesteps: 7980000
Best mean reward: 4471.95 - Last mean reward per episode: 4252.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 4203.93     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 325         |
|    time_elapsed         | 94379       |
|    total_timesteps      | 7987200     |
| train/                  |             |
|    approx_kl            | 0.013797618 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 89          |
|    n_updates            | 3240        |
|    policy_gradient_loss | -4.1e-05    |
|    value_loss           | 519         |
-----------------------------------------
Num timesteps: 7992000
Best mean reward: 4471.95 - Last mean reward per episode: 4203.93
Num timesteps: 8004000
Best mean reward: 4471.95 - Last mean reward per episode: 4313.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 4349.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 326         |
|    time_elapsed         | 94680       |
|    total_timesteps      | 8011776     |
| train/                  |             |
|    approx_kl            | 0.017806439 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.953       |
|    learning_rate        | 3e-06       |
|    loss                 | 227         |
|    n_updates            | 3250        |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 663         |
-----------------------------------------
Num timesteps: 8016000
Best mean reward: 4471.95 - Last mean reward per episode: 4346.50
Num timesteps: 8028000
Best mean reward: 4471.95 - Last mean reward per episode: 4377.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.29e+03   |
|    ep_rew_mean          | 4463.83    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 327        |
|    time_elapsed         | 94968      |
|    total_timesteps      | 8036352    |
| train/                  |            |
|    approx_kl            | 0.01803949 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.41      |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-06      |
|    loss                 | 151        |
|    n_updates            | 3260       |
|    policy_gradient_loss | 0.00266    |
|    value_loss           | 605        |
----------------------------------------
Num timesteps: 8040000
Best mean reward: 4471.95 - Last mean reward per episode: 4478.60
Saving new best model to tmp/best_model
Num timesteps: 8052000
Best mean reward: 4478.60 - Last mean reward per episode: 4471.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.31e+03   |
|    ep_rew_mean          | 4490.4     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 328        |
|    time_elapsed         | 95261      |
|    total_timesteps      | 8060928    |
| train/                  |            |
|    approx_kl            | 0.01838169 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-06      |
|    loss                 | 299        |
|    n_updates            | 3270       |
|    policy_gradient_loss | 0.000746   |
|    value_loss           | 644        |
----------------------------------------
Num timesteps: 8064000
Best mean reward: 4478.60 - Last mean reward per episode: 4505.07
Saving new best model to tmp/best_model
Num timesteps: 8076000
Best mean reward: 4505.07 - Last mean reward per episode: 4450.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4437.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 329         |
|    time_elapsed         | 95556       |
|    total_timesteps      | 8085504     |
| train/                  |             |
|    approx_kl            | 0.016672859 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 203         |
|    n_updates            | 3280        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 535         |
-----------------------------------------
Num timesteps: 8088000
Best mean reward: 4505.07 - Last mean reward per episode: 4401.57
Num timesteps: 8100000
Best mean reward: 4505.07 - Last mean reward per episode: 4423.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4457.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 330         |
|    time_elapsed         | 95843       |
|    total_timesteps      | 8110080     |
| train/                  |             |
|    approx_kl            | 0.015233251 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 258         |
|    n_updates            | 3290        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 511         |
-----------------------------------------
Num timesteps: 8112000
Best mean reward: 4505.07 - Last mean reward per episode: 4459.16
Num timesteps: 8124000
Best mean reward: 4505.07 - Last mean reward per episode: 4442.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 4442.78     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 331         |
|    time_elapsed         | 96131       |
|    total_timesteps      | 8134656     |
| train/                  |             |
|    approx_kl            | 0.014584926 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 282         |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.000566   |
|    value_loss           | 579         |
-----------------------------------------
Num timesteps: 8136000
Best mean reward: 4505.07 - Last mean reward per episode: 4437.43
Num timesteps: 8148000
Best mean reward: 4505.07 - Last mean reward per episode: 4374.99
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.27e+03  |
|    ep_rew_mean          | 4411.24   |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 332       |
|    time_elapsed         | 96422     |
|    total_timesteps      | 8159232   |
| train/                  |           |
|    approx_kl            | 0.0164338 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36     |
|    explained_variance   | 0.957     |
|    learning_rate        | 3e-06     |
|    loss                 | 382       |
|    n_updates            | 3310      |
|    policy_gradient_loss | 0.000278  |
|    value_loss           | 647       |
---------------------------------------
Num timesteps: 8160000
Best mean reward: 4505.07 - Last mean reward per episode: 4417.63
Num timesteps: 8172000
Best mean reward: 4505.07 - Last mean reward per episode: 4428.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 4384.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 333         |
|    time_elapsed         | 96714       |
|    total_timesteps      | 8183808     |
| train/                  |             |
|    approx_kl            | 0.013357334 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 150         |
|    n_updates            | 3320        |
|    policy_gradient_loss | 0.000681    |
|    value_loss           | 496         |
-----------------------------------------
Num timesteps: 8184000
Best mean reward: 4505.07 - Last mean reward per episode: 4377.19
Num timesteps: 8196000
Best mean reward: 4505.07 - Last mean reward per episode: 4377.99
Num timesteps: 8208000
Best mean reward: 4505.07 - Last mean reward per episode: 4409.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 4409.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 334         |
|    time_elapsed         | 97001       |
|    total_timesteps      | 8208384     |
| train/                  |             |
|    approx_kl            | 0.019752866 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-06       |
|    loss                 | 715         |
|    n_updates            | 3330        |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 722         |
-----------------------------------------
Num timesteps: 8220000
Best mean reward: 4505.07 - Last mean reward per episode: 4522.28
Saving new best model to tmp/best_model
Num timesteps: 8232000
Best mean reward: 4522.28 - Last mean reward per episode: 4446.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4446.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 335         |
|    time_elapsed         | 97292       |
|    total_timesteps      | 8232960     |
| train/                  |             |
|    approx_kl            | 0.016899372 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 295         |
|    n_updates            | 3340        |
|    policy_gradient_loss | 0.00233     |
|    value_loss           | 559         |
-----------------------------------------
Num timesteps: 8244000
Best mean reward: 4522.28 - Last mean reward per episode: 4466.72
Num timesteps: 8256000
Best mean reward: 4522.28 - Last mean reward per episode: 4463.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4468.98     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 336         |
|    time_elapsed         | 97583       |
|    total_timesteps      | 8257536     |
| train/                  |             |
|    approx_kl            | 0.019003583 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 215         |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.000242   |
|    value_loss           | 621         |
-----------------------------------------
Num timesteps: 8268000
Best mean reward: 4522.28 - Last mean reward per episode: 4539.59
Saving new best model to tmp/best_model
Num timesteps: 8280000
Best mean reward: 4539.59 - Last mean reward per episode: 4538.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4538.95     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 337         |
|    time_elapsed         | 97878       |
|    total_timesteps      | 8282112     |
| train/                  |             |
|    approx_kl            | 0.017258309 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 93.4        |
|    n_updates            | 3360        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 539         |
-----------------------------------------
Num timesteps: 8292000
Best mean reward: 4539.59 - Last mean reward per episode: 4555.62
Saving new best model to tmp/best_model
Num timesteps: 8304000
Best mean reward: 4555.62 - Last mean reward per episode: 4560.11
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4560.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 338         |
|    time_elapsed         | 98169       |
|    total_timesteps      | 8306688     |
| train/                  |             |
|    approx_kl            | 0.016968567 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 102         |
|    n_updates            | 3370        |
|    policy_gradient_loss | 0.000574    |
|    value_loss           | 538         |
-----------------------------------------
Num timesteps: 8316000
Best mean reward: 4560.11 - Last mean reward per episode: 4562.98
Saving new best model to tmp/best_model
Num timesteps: 8328000
Best mean reward: 4562.98 - Last mean reward per episode: 4596.85
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4612.87     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 339         |
|    time_elapsed         | 98458       |
|    total_timesteps      | 8331264     |
| train/                  |             |
|    approx_kl            | 0.014576383 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 551         |
-----------------------------------------
Num timesteps: 8340000
Best mean reward: 4596.85 - Last mean reward per episode: 4637.80
Saving new best model to tmp/best_model
Num timesteps: 8352000
Best mean reward: 4637.80 - Last mean reward per episode: 4678.43
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4737.57     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 340         |
|    time_elapsed         | 98765       |
|    total_timesteps      | 8355840     |
| train/                  |             |
|    approx_kl            | 0.014423638 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.955       |
|    learning_rate        | 3e-06       |
|    loss                 | 746         |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.000309   |
|    value_loss           | 719         |
-----------------------------------------
Num timesteps: 8364000
Best mean reward: 4678.43 - Last mean reward per episode: 4712.32
Saving new best model to tmp/best_model
Num timesteps: 8376000
Best mean reward: 4712.32 - Last mean reward per episode: 4745.64
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4745.64     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 341         |
|    time_elapsed         | 99062       |
|    total_timesteps      | 8380416     |
| train/                  |             |
|    approx_kl            | 0.013825472 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.000732   |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 8388000
Best mean reward: 4745.64 - Last mean reward per episode: 4739.70
Num timesteps: 8400000
Best mean reward: 4745.64 - Last mean reward per episode: 4730.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4730.94     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 342         |
|    time_elapsed         | 99358       |
|    total_timesteps      | 8404992     |
| train/                  |             |
|    approx_kl            | 0.019858258 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 512         |
|    n_updates            | 3410        |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 686         |
-----------------------------------------
Num timesteps: 8412000
Best mean reward: 4745.64 - Last mean reward per episode: 4762.45
Saving new best model to tmp/best_model
Num timesteps: 8424000
Best mean reward: 4762.45 - Last mean reward per episode: 4787.50
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4783.63     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 343         |
|    time_elapsed         | 99652       |
|    total_timesteps      | 8429568     |
| train/                  |             |
|    approx_kl            | 0.018615028 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 436         |
|    n_updates            | 3420        |
|    policy_gradient_loss | 0.00318     |
|    value_loss           | 585         |
-----------------------------------------
Num timesteps: 8436000
Best mean reward: 4787.50 - Last mean reward per episode: 4757.11
Num timesteps: 8448000
Best mean reward: 4787.50 - Last mean reward per episode: 4729.30
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.37e+03   |
|    ep_rew_mean          | 4692.94    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 344        |
|    time_elapsed         | 99951      |
|    total_timesteps      | 8454144    |
| train/                  |            |
|    approx_kl            | 0.01500545 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.96       |
|    learning_rate        | 3e-06      |
|    loss                 | 181        |
|    n_updates            | 3430       |
|    policy_gradient_loss | -0.000732  |
|    value_loss           | 565        |
----------------------------------------
Num timesteps: 8460000
Best mean reward: 4787.50 - Last mean reward per episode: 4675.41
Num timesteps: 8472000
Best mean reward: 4787.50 - Last mean reward per episode: 4708.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4744.11     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 345         |
|    time_elapsed         | 100247      |
|    total_timesteps      | 8478720     |
| train/                  |             |
|    approx_kl            | 0.020056937 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-06       |
|    loss                 | 618         |
|    n_updates            | 3440        |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 777         |
-----------------------------------------
Num timesteps: 8484000
Best mean reward: 4787.50 - Last mean reward per episode: 4730.41
Num timesteps: 8496000
Best mean reward: 4787.50 - Last mean reward per episode: 4759.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4725.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 346         |
|    time_elapsed         | 100543      |
|    total_timesteps      | 8503296     |
| train/                  |             |
|    approx_kl            | 0.017019408 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 3450        |
|    policy_gradient_loss | 0.000466    |
|    value_loss           | 528         |
-----------------------------------------
Num timesteps: 8508000
Best mean reward: 4787.50 - Last mean reward per episode: 4677.21
Num timesteps: 8520000
Best mean reward: 4787.50 - Last mean reward per episode: 4640.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4635.08     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 347         |
|    time_elapsed         | 100836      |
|    total_timesteps      | 8527872     |
| train/                  |             |
|    approx_kl            | 0.015482637 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 379         |
|    n_updates            | 3460        |
|    policy_gradient_loss | 0.00234     |
|    value_loss           | 581         |
-----------------------------------------
Num timesteps: 8532000
Best mean reward: 4787.50 - Last mean reward per episode: 4595.86
Num timesteps: 8544000
Best mean reward: 4787.50 - Last mean reward per episode: 4604.71
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.34e+03   |
|    ep_rew_mean          | 4638.25    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 348        |
|    time_elapsed         | 101132     |
|    total_timesteps      | 8552448    |
| train/                  |            |
|    approx_kl            | 0.01672769 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.962      |
|    learning_rate        | 3e-06      |
|    loss                 | 154        |
|    n_updates            | 3470       |
|    policy_gradient_loss | 0.000894   |
|    value_loss           | 653        |
----------------------------------------
Num timesteps: 8556000
Best mean reward: 4787.50 - Last mean reward per episode: 4638.25
Num timesteps: 8568000
Best mean reward: 4787.50 - Last mean reward per episode: 4620.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4577.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 349         |
|    time_elapsed         | 101430      |
|    total_timesteps      | 8577024     |
| train/                  |             |
|    approx_kl            | 0.017380046 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 201         |
|    n_updates            | 3480        |
|    policy_gradient_loss | 0.000816    |
|    value_loss           | 509         |
-----------------------------------------
Num timesteps: 8580000
Best mean reward: 4787.50 - Last mean reward per episode: 4573.78
Num timesteps: 8592000
Best mean reward: 4787.50 - Last mean reward per episode: 4573.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4639.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 350         |
|    time_elapsed         | 101731      |
|    total_timesteps      | 8601600     |
| train/                  |             |
|    approx_kl            | 0.025694484 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 313         |
|    n_updates            | 3490        |
|    policy_gradient_loss | 0.000796    |
|    value_loss           | 455         |
-----------------------------------------
Num timesteps: 8604000
Best mean reward: 4787.50 - Last mean reward per episode: 4639.12
Num timesteps: 8616000
Best mean reward: 4787.50 - Last mean reward per episode: 4597.54
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4599.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 351         |
|    time_elapsed         | 102029      |
|    total_timesteps      | 8626176     |
| train/                  |             |
|    approx_kl            | 0.018210018 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 170         |
|    n_updates            | 3500        |
|    policy_gradient_loss | 0.00245     |
|    value_loss           | 556         |
-----------------------------------------
Num timesteps: 8628000
Best mean reward: 4787.50 - Last mean reward per episode: 4599.82
Num timesteps: 8640000
Best mean reward: 4787.50 - Last mean reward per episode: 4527.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4551.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 352         |
|    time_elapsed         | 102320      |
|    total_timesteps      | 8650752     |
| train/                  |             |
|    approx_kl            | 0.015416746 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 175         |
|    n_updates            | 3510        |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 412         |
-----------------------------------------
Num timesteps: 8652000
Best mean reward: 4787.50 - Last mean reward per episode: 4551.05
Num timesteps: 8664000
Best mean reward: 4787.50 - Last mean reward per episode: 4531.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.32e+03    |
|    ep_rew_mean          | 4592.05     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 353         |
|    time_elapsed         | 102618      |
|    total_timesteps      | 8675328     |
| train/                  |             |
|    approx_kl            | 0.018252105 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 213         |
|    n_updates            | 3520        |
|    policy_gradient_loss | 0.000683    |
|    value_loss           | 550         |
-----------------------------------------
Num timesteps: 8676000
Best mean reward: 4787.50 - Last mean reward per episode: 4627.80
Num timesteps: 8688000
Best mean reward: 4787.50 - Last mean reward per episode: 4659.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4688.61     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 354         |
|    time_elapsed         | 102903      |
|    total_timesteps      | 8699904     |
| train/                  |             |
|    approx_kl            | 0.024291666 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 84.9        |
|    n_updates            | 3530        |
|    policy_gradient_loss | 0.00409     |
|    value_loss           | 469         |
-----------------------------------------
Num timesteps: 8700000
Best mean reward: 4787.50 - Last mean reward per episode: 4688.61
Num timesteps: 8712000
Best mean reward: 4787.50 - Last mean reward per episode: 4735.81
Num timesteps: 8724000
Best mean reward: 4787.50 - Last mean reward per episode: 4704.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4704.45     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 355         |
|    time_elapsed         | 103190      |
|    total_timesteps      | 8724480     |
| train/                  |             |
|    approx_kl            | 0.014285233 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 235         |
|    n_updates            | 3540        |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 427         |
-----------------------------------------
Num timesteps: 8736000
Best mean reward: 4787.50 - Last mean reward per episode: 4632.88
Num timesteps: 8748000
Best mean reward: 4787.50 - Last mean reward per episode: 4673.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4673.89     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 356         |
|    time_elapsed         | 103479      |
|    total_timesteps      | 8749056     |
| train/                  |             |
|    approx_kl            | 0.017399339 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 367         |
|    n_updates            | 3550        |
|    policy_gradient_loss | 0.00137     |
|    value_loss           | 488         |
-----------------------------------------
Num timesteps: 8760000
Best mean reward: 4787.50 - Last mean reward per episode: 4707.21
Num timesteps: 8772000
Best mean reward: 4787.50 - Last mean reward per episode: 4757.33
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.41e+03     |
|    ep_rew_mean          | 4757.33      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 357          |
|    time_elapsed         | 103769       |
|    total_timesteps      | 8773632      |
| train/                  |              |
|    approx_kl            | 0.0146535635 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.974        |
|    learning_rate        | 3e-06        |
|    loss                 | 193          |
|    n_updates            | 3560         |
|    policy_gradient_loss | 0.00232      |
|    value_loss           | 460          |
------------------------------------------
Num timesteps: 8784000
Best mean reward: 4787.50 - Last mean reward per episode: 4791.20
Saving new best model to tmp/best_model
Num timesteps: 8796000
Best mean reward: 4791.20 - Last mean reward per episode: 4792.21
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4803.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 358         |
|    time_elapsed         | 104061      |
|    total_timesteps      | 8798208     |
| train/                  |             |
|    approx_kl            | 0.016088659 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 166         |
|    n_updates            | 3570        |
|    policy_gradient_loss | 0.000562    |
|    value_loss           | 443         |
-----------------------------------------
Num timesteps: 8808000
Best mean reward: 4792.21 - Last mean reward per episode: 4795.65
Saving new best model to tmp/best_model
Num timesteps: 8820000
Best mean reward: 4795.65 - Last mean reward per episode: 4814.60
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4828.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 359         |
|    time_elapsed         | 104350      |
|    total_timesteps      | 8822784     |
| train/                  |             |
|    approx_kl            | 0.014836547 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 86.5        |
|    n_updates            | 3580        |
|    policy_gradient_loss | 0.000787    |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 8832000
Best mean reward: 4814.60 - Last mean reward per episode: 4812.90
Num timesteps: 8844000
Best mean reward: 4814.60 - Last mean reward per episode: 4850.63
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 4800.01     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 360         |
|    time_elapsed         | 104651      |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.018412217 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 261         |
|    n_updates            | 3590        |
|    policy_gradient_loss | 0.000939    |
|    value_loss           | 591         |
-----------------------------------------
Num timesteps: 8856000
Best mean reward: 4850.63 - Last mean reward per episode: 4833.43
Num timesteps: 8868000
Best mean reward: 4850.63 - Last mean reward per episode: 4739.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 4740.32     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 361         |
|    time_elapsed         | 104949      |
|    total_timesteps      | 8871936     |
| train/                  |             |
|    approx_kl            | 0.019764438 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 385         |
|    n_updates            | 3600        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 582         |
-----------------------------------------
Num timesteps: 8880000
Best mean reward: 4850.63 - Last mean reward per episode: 4732.41
Num timesteps: 8892000
Best mean reward: 4850.63 - Last mean reward per episode: 4751.57
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | 4742.07     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 362         |
|    time_elapsed         | 105242      |
|    total_timesteps      | 8896512     |
| train/                  |             |
|    approx_kl            | 0.021084862 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-06       |
|    loss                 | 139         |
|    n_updates            | 3610        |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 654         |
-----------------------------------------
Num timesteps: 8904000
Best mean reward: 4850.63 - Last mean reward per episode: 4705.97
Num timesteps: 8916000
Best mean reward: 4850.63 - Last mean reward per episode: 4749.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 4718.72     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 363         |
|    time_elapsed         | 105534      |
|    total_timesteps      | 8921088     |
| train/                  |             |
|    approx_kl            | 0.016004222 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 3620        |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 526         |
-----------------------------------------
Num timesteps: 8928000
Best mean reward: 4850.63 - Last mean reward per episode: 4715.61
Num timesteps: 8940000
Best mean reward: 4850.63 - Last mean reward per episode: 4644.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4646.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 364         |
|    time_elapsed         | 105826      |
|    total_timesteps      | 8945664     |
| train/                  |             |
|    approx_kl            | 0.016519478 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 318         |
|    n_updates            | 3630        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 8952000
Best mean reward: 4850.63 - Last mean reward per episode: 4636.51
Num timesteps: 8964000
Best mean reward: 4850.63 - Last mean reward per episode: 4721.85
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.46e+03    |
|    ep_rew_mean          | 4753.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 365         |
|    time_elapsed         | 106114      |
|    total_timesteps      | 8970240     |
| train/                  |             |
|    approx_kl            | 0.022039449 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 322         |
|    n_updates            | 3640        |
|    policy_gradient_loss | 0.00415     |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 8976000
Best mean reward: 4850.63 - Last mean reward per episode: 4753.66
Num timesteps: 8988000
Best mean reward: 4850.63 - Last mean reward per episode: 4729.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4680.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 366         |
|    time_elapsed         | 106401      |
|    total_timesteps      | 8994816     |
| train/                  |             |
|    approx_kl            | 0.017653199 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.96        |
|    learning_rate        | 3e-06       |
|    loss                 | 450         |
|    n_updates            | 3650        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 637         |
-----------------------------------------
Num timesteps: 9000000
Best mean reward: 4850.63 - Last mean reward per episode: 4659.72
Num timesteps: 9012000
Best mean reward: 4850.63 - Last mean reward per episode: 4604.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 4604.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 367         |
|    time_elapsed         | 106683      |
|    total_timesteps      | 9019392     |
| train/                  |             |
|    approx_kl            | 0.016219793 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 357         |
|    n_updates            | 3660        |
|    policy_gradient_loss | 0.00431     |
|    value_loss           | 651         |
-----------------------------------------
Num timesteps: 9024000
Best mean reward: 4850.63 - Last mean reward per episode: 4594.71
Num timesteps: 9036000
Best mean reward: 4850.63 - Last mean reward per episode: 4553.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 4487.48     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 368         |
|    time_elapsed         | 106966      |
|    total_timesteps      | 9043968     |
| train/                  |             |
|    approx_kl            | 0.014665221 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 164         |
|    n_updates            | 3670        |
|    policy_gradient_loss | 0.00201     |
|    value_loss           | 523         |
-----------------------------------------
Num timesteps: 9048000
Best mean reward: 4850.63 - Last mean reward per episode: 4487.48
Num timesteps: 9060000
Best mean reward: 4850.63 - Last mean reward per episode: 4507.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.29e+03   |
|    ep_rew_mean          | 4490.77    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 369        |
|    time_elapsed         | 107250     |
|    total_timesteps      | 9068544    |
| train/                  |            |
|    approx_kl            | 0.01615119 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.969      |
|    learning_rate        | 3e-06      |
|    loss                 | 297        |
|    n_updates            | 3680       |
|    policy_gradient_loss | 0.00177    |
|    value_loss           | 487        |
----------------------------------------
Num timesteps: 9072000
Best mean reward: 4850.63 - Last mean reward per episode: 4528.88
Num timesteps: 9084000
Best mean reward: 4850.63 - Last mean reward per episode: 4515.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.3e+03     |
|    ep_rew_mean          | 4505.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 370         |
|    time_elapsed         | 107538      |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.016052293 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.966       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.000208   |
|    value_loss           | 536         |
-----------------------------------------
Num timesteps: 9096000
Best mean reward: 4850.63 - Last mean reward per episode: 4540.31
Num timesteps: 9108000
Best mean reward: 4850.63 - Last mean reward per episode: 4624.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4617.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 371         |
|    time_elapsed         | 107826      |
|    total_timesteps      | 9117696     |
| train/                  |             |
|    approx_kl            | 0.015908325 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 239         |
|    n_updates            | 3700        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 578         |
-----------------------------------------
Num timesteps: 9120000
Best mean reward: 4850.63 - Last mean reward per episode: 4617.18
Num timesteps: 9132000
Best mean reward: 4850.63 - Last mean reward per episode: 4594.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4636.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 372         |
|    time_elapsed         | 108120      |
|    total_timesteps      | 9142272     |
| train/                  |             |
|    approx_kl            | 0.017252699 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 96.1        |
|    n_updates            | 3710        |
|    policy_gradient_loss | 0.00182     |
|    value_loss           | 599         |
-----------------------------------------
Num timesteps: 9144000
Best mean reward: 4850.63 - Last mean reward per episode: 4635.33
Num timesteps: 9156000
Best mean reward: 4850.63 - Last mean reward per episode: 4626.90
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.39e+03   |
|    ep_rew_mean          | 4676.33    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 373        |
|    time_elapsed         | 108405     |
|    total_timesteps      | 9166848    |
| train/                  |            |
|    approx_kl            | 0.01494218 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 93.3       |
|    n_updates            | 3720       |
|    policy_gradient_loss | 0.000706   |
|    value_loss           | 469        |
----------------------------------------
Num timesteps: 9168000
Best mean reward: 4850.63 - Last mean reward per episode: 4676.33
Num timesteps: 9180000
Best mean reward: 4850.63 - Last mean reward per episode: 4712.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4692.27     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 374         |
|    time_elapsed         | 108692      |
|    total_timesteps      | 9191424     |
| train/                  |             |
|    approx_kl            | 0.012486294 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.967       |
|    learning_rate        | 3e-06       |
|    loss                 | 128         |
|    n_updates            | 3730        |
|    policy_gradient_loss | 0.000315    |
|    value_loss           | 451         |
-----------------------------------------
Num timesteps: 9192000
Best mean reward: 4850.63 - Last mean reward per episode: 4700.64
Num timesteps: 9204000
Best mean reward: 4850.63 - Last mean reward per episode: 4651.53
Num timesteps: 9216000
Best mean reward: 4850.63 - Last mean reward per episode: 4613.83
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4613.83     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 375         |
|    time_elapsed         | 108976      |
|    total_timesteps      | 9216000     |
| train/                  |             |
|    approx_kl            | 0.017212784 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3e-06       |
|    loss                 | 352         |
|    n_updates            | 3740        |
|    policy_gradient_loss | 0.00244     |
|    value_loss           | 669         |
-----------------------------------------
Num timesteps: 9228000
Best mean reward: 4850.63 - Last mean reward per episode: 4630.14
Num timesteps: 9240000
Best mean reward: 4850.63 - Last mean reward per episode: 4657.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | 4657.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 376         |
|    time_elapsed         | 109260      |
|    total_timesteps      | 9240576     |
| train/                  |             |
|    approx_kl            | 0.017443703 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 471         |
|    n_updates            | 3750        |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 562         |
-----------------------------------------
Num timesteps: 9252000
Best mean reward: 4850.63 - Last mean reward per episode: 4632.65
Num timesteps: 9264000
Best mean reward: 4850.63 - Last mean reward per episode: 4642.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4642.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 377         |
|    time_elapsed         | 109549      |
|    total_timesteps      | 9265152     |
| train/                  |             |
|    approx_kl            | 0.016528236 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 316         |
|    n_updates            | 3760        |
|    policy_gradient_loss | 0.000592    |
|    value_loss           | 548         |
-----------------------------------------
Num timesteps: 9276000
Best mean reward: 4850.63 - Last mean reward per episode: 4690.30
Num timesteps: 9288000
Best mean reward: 4850.63 - Last mean reward per episode: 4702.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4672.67     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 378         |
|    time_elapsed         | 109832      |
|    total_timesteps      | 9289728     |
| train/                  |             |
|    approx_kl            | 0.017643178 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-06       |
|    loss                 | 265         |
|    n_updates            | 3770        |
|    policy_gradient_loss | 0.00205     |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 9300000
Best mean reward: 4850.63 - Last mean reward per episode: 4655.72
Num timesteps: 9312000
Best mean reward: 4850.63 - Last mean reward per episode: 4630.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4623.29     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 379         |
|    time_elapsed         | 110118      |
|    total_timesteps      | 9314304     |
| train/                  |             |
|    approx_kl            | 0.012455498 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 234         |
|    n_updates            | 3780        |
|    policy_gradient_loss | -2.59e-05   |
|    value_loss           | 694         |
-----------------------------------------
Num timesteps: 9324000
Best mean reward: 4850.63 - Last mean reward per episode: 4647.12
Num timesteps: 9336000
Best mean reward: 4850.63 - Last mean reward per episode: 4635.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4641.58     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 380         |
|    time_elapsed         | 110401      |
|    total_timesteps      | 9338880     |
| train/                  |             |
|    approx_kl            | 0.017070675 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 200         |
|    n_updates            | 3790        |
|    policy_gradient_loss | 0.00243     |
|    value_loss           | 540         |
-----------------------------------------
Num timesteps: 9348000
Best mean reward: 4850.63 - Last mean reward per episode: 4693.85
Num timesteps: 9360000
Best mean reward: 4850.63 - Last mean reward per episode: 4732.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4732.39     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 381         |
|    time_elapsed         | 110685      |
|    total_timesteps      | 9363456     |
| train/                  |             |
|    approx_kl            | 0.016686564 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 167         |
|    n_updates            | 3800        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 517         |
-----------------------------------------
Num timesteps: 9372000
Best mean reward: 4850.63 - Last mean reward per episode: 4721.76
Num timesteps: 9384000
Best mean reward: 4850.63 - Last mean reward per episode: 4734.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | 4739.12     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 382         |
|    time_elapsed         | 110987      |
|    total_timesteps      | 9388032     |
| train/                  |             |
|    approx_kl            | 0.014761817 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 148         |
|    n_updates            | 3810        |
|    policy_gradient_loss | 9.19e-05    |
|    value_loss           | 476         |
-----------------------------------------
Num timesteps: 9396000
Best mean reward: 4850.63 - Last mean reward per episode: 4721.05
Num timesteps: 9408000
Best mean reward: 4850.63 - Last mean reward per episode: 4729.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.35e+03   |
|    ep_rew_mean          | 4700.36    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 383        |
|    time_elapsed         | 111285     |
|    total_timesteps      | 9412608    |
| train/                  |            |
|    approx_kl            | 0.01428451 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.975      |
|    learning_rate        | 3e-06      |
|    loss                 | 215        |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.00163   |
|    value_loss           | 386        |
----------------------------------------
Num timesteps: 9420000
Best mean reward: 4850.63 - Last mean reward per episode: 4744.90
Num timesteps: 9432000
Best mean reward: 4850.63 - Last mean reward per episode: 4793.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4795.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 384         |
|    time_elapsed         | 111580      |
|    total_timesteps      | 9437184     |
| train/                  |             |
|    approx_kl            | 0.016208904 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 178         |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.000729   |
|    value_loss           | 407         |
-----------------------------------------
Num timesteps: 9444000
Best mean reward: 4850.63 - Last mean reward per episode: 4817.63
Num timesteps: 9456000
Best mean reward: 4850.63 - Last mean reward per episode: 4875.08
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | 4846.47     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 385         |
|    time_elapsed         | 111880      |
|    total_timesteps      | 9461760     |
| train/                  |             |
|    approx_kl            | 0.016885906 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 404         |
|    n_updates            | 3840        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 454         |
-----------------------------------------
Num timesteps: 9468000
Best mean reward: 4875.08 - Last mean reward per episode: 4855.43
Num timesteps: 9480000
Best mean reward: 4875.08 - Last mean reward per episode: 4819.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 4856.18     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 386         |
|    time_elapsed         | 112179      |
|    total_timesteps      | 9486336     |
| train/                  |             |
|    approx_kl            | 0.016466284 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-06       |
|    loss                 | 123         |
|    n_updates            | 3850        |
|    policy_gradient_loss | 0.00331     |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 9492000
Best mean reward: 4875.08 - Last mean reward per episode: 4853.75
Num timesteps: 9504000
Best mean reward: 4875.08 - Last mean reward per episode: 4885.75
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.51e+03    |
|    ep_rew_mean          | 4887.77     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 387         |
|    time_elapsed         | 112479      |
|    total_timesteps      | 9510912     |
| train/                  |             |
|    approx_kl            | 0.015036948 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 181         |
|    n_updates            | 3860        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 645         |
-----------------------------------------
Num timesteps: 9516000
Best mean reward: 4885.75 - Last mean reward per episode: 4882.12
Num timesteps: 9528000
Best mean reward: 4885.75 - Last mean reward per episode: 4833.87
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.5e+03    |
|    ep_rew_mean          | 4836.17    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 388        |
|    time_elapsed         | 112770     |
|    total_timesteps      | 9535488    |
| train/                  |            |
|    approx_kl            | 0.01312726 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.922     |
|    explained_variance   | 0.964      |
|    learning_rate        | 3e-06      |
|    loss                 | 221        |
|    n_updates            | 3870       |
|    policy_gradient_loss | 0.000498   |
|    value_loss           | 497        |
----------------------------------------
Num timesteps: 9540000
Best mean reward: 4885.75 - Last mean reward per episode: 4883.44
Num timesteps: 9552000
Best mean reward: 4885.75 - Last mean reward per episode: 4859.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.54e+03    |
|    ep_rew_mean          | 4911.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 389         |
|    time_elapsed         | 113059      |
|    total_timesteps      | 9560064     |
| train/                  |             |
|    approx_kl            | 0.015871106 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 422         |
|    n_updates            | 3880        |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 717         |
-----------------------------------------
Num timesteps: 9564000
Best mean reward: 4885.75 - Last mean reward per episode: 4911.81
Saving new best model to tmp/best_model
Num timesteps: 9576000
Best mean reward: 4911.81 - Last mean reward per episode: 4924.44
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4896.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 390         |
|    time_elapsed         | 113353      |
|    total_timesteps      | 9584640     |
| train/                  |             |
|    approx_kl            | 0.014117355 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 361         |
|    n_updates            | 3890        |
|    policy_gradient_loss | 0.00111     |
|    value_loss           | 512         |
-----------------------------------------
Num timesteps: 9588000
Best mean reward: 4924.44 - Last mean reward per episode: 4896.66
Num timesteps: 9600000
Best mean reward: 4924.44 - Last mean reward per episode: 4825.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.49e+03    |
|    ep_rew_mean          | 4849.43     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 391         |
|    time_elapsed         | 113643      |
|    total_timesteps      | 9609216     |
| train/                  |             |
|    approx_kl            | 0.016866526 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.958       |
|    learning_rate        | 3e-06       |
|    loss                 | 275         |
|    n_updates            | 3900        |
|    policy_gradient_loss | 8.77e-05    |
|    value_loss           | 649         |
-----------------------------------------
Num timesteps: 9612000
Best mean reward: 4924.44 - Last mean reward per episode: 4844.48
Num timesteps: 9624000
Best mean reward: 4924.44 - Last mean reward per episode: 4899.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 4878.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 392         |
|    time_elapsed         | 113937      |
|    total_timesteps      | 9633792     |
| train/                  |             |
|    approx_kl            | 0.013361365 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 343         |
|    n_updates            | 3910        |
|    policy_gradient_loss | 0.000467    |
|    value_loss           | 434         |
-----------------------------------------
Num timesteps: 9636000
Best mean reward: 4924.44 - Last mean reward per episode: 4880.57
Num timesteps: 9648000
Best mean reward: 4924.44 - Last mean reward per episode: 4837.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 4825.79     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 393         |
|    time_elapsed         | 114227      |
|    total_timesteps      | 9658368     |
| train/                  |             |
|    approx_kl            | 0.013870659 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 199         |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.000197   |
|    value_loss           | 422         |
-----------------------------------------
Num timesteps: 9660000
Best mean reward: 4924.44 - Last mean reward per episode: 4792.25
Num timesteps: 9672000
Best mean reward: 4924.44 - Last mean reward per episode: 4777.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.5e+03    |
|    ep_rew_mean          | 4772.34    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 394        |
|    time_elapsed         | 114520     |
|    total_timesteps      | 9682944    |
| train/                  |            |
|    approx_kl            | 0.02056151 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.957      |
|    learning_rate        | 3e-06      |
|    loss                 | 396        |
|    n_updates            | 3930       |
|    policy_gradient_loss | 0.00325    |
|    value_loss           | 667        |
----------------------------------------
Num timesteps: 9684000
Best mean reward: 4924.44 - Last mean reward per episode: 4772.34
Num timesteps: 9696000
Best mean reward: 4924.44 - Last mean reward per episode: 4770.18
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.49e+03   |
|    ep_rew_mean          | 4753.78    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 395        |
|    time_elapsed         | 114810     |
|    total_timesteps      | 9707520    |
| train/                  |            |
|    approx_kl            | 0.02391847 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.964      |
|    learning_rate        | 3e-06      |
|    loss                 | 225        |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.00258   |
|    value_loss           | 476        |
----------------------------------------
Num timesteps: 9708000
Best mean reward: 4924.44 - Last mean reward per episode: 4753.78
Num timesteps: 9720000
Best mean reward: 4924.44 - Last mean reward per episode: 4783.93
Num timesteps: 9732000
Best mean reward: 4924.44 - Last mean reward per episode: 4825.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.5e+03     |
|    ep_rew_mean          | 4825.84     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 396         |
|    time_elapsed         | 115098      |
|    total_timesteps      | 9732096     |
| train/                  |             |
|    approx_kl            | 0.023928516 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 199         |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.000345   |
|    value_loss           | 333         |
-----------------------------------------
Num timesteps: 9744000
Best mean reward: 4924.44 - Last mean reward per episode: 4877.35
Num timesteps: 9756000
Best mean reward: 4924.44 - Last mean reward per episode: 4901.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | 4901.99     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 397         |
|    time_elapsed         | 115386      |
|    total_timesteps      | 9756672     |
| train/                  |             |
|    approx_kl            | 0.013381432 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.971       |
|    learning_rate        | 3e-06       |
|    loss                 | 225         |
|    n_updates            | 3960        |
|    policy_gradient_loss | 0.000779    |
|    value_loss           | 432         |
-----------------------------------------
Num timesteps: 9768000
Best mean reward: 4924.44 - Last mean reward per episode: 4947.77
Saving new best model to tmp/best_model
Num timesteps: 9780000
Best mean reward: 4947.77 - Last mean reward per episode: 4950.51
Saving new best model to tmp/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.59e+03    |
|    ep_rew_mean          | 4960.09     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 398         |
|    time_elapsed         | 115679      |
|    total_timesteps      | 9781248     |
| train/                  |             |
|    approx_kl            | 0.009507871 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 262         |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 348         |
-----------------------------------------
Num timesteps: 9792000
Best mean reward: 4950.51 - Last mean reward per episode: 5028.14
Saving new best model to tmp/best_model
Num timesteps: 9804000
Best mean reward: 5028.14 - Last mean reward per episode: 5027.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61e+03    |
|    ep_rew_mean          | 5018.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 399         |
|    time_elapsed         | 115975      |
|    total_timesteps      | 9805824     |
| train/                  |             |
|    approx_kl            | 0.011519375 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.977       |
|    learning_rate        | 3e-06       |
|    loss                 | 85.2        |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.000739   |
|    value_loss           | 364         |
-----------------------------------------
Num timesteps: 9816000
Best mean reward: 5028.14 - Last mean reward per episode: 5033.59
Saving new best model to tmp/best_model
Num timesteps: 9828000
Best mean reward: 5033.59 - Last mean reward per episode: 5017.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.61e+03    |
|    ep_rew_mean          | 5017.53     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 400         |
|    time_elapsed         | 116262      |
|    total_timesteps      | 9830400     |
| train/                  |             |
|    approx_kl            | 0.016391821 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-06       |
|    loss                 | 123         |
|    n_updates            | 3990        |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 539         |
-----------------------------------------
Num timesteps: 9840000
Best mean reward: 5033.59 - Last mean reward per episode: 5005.75
Num timesteps: 9852000
Best mean reward: 5033.59 - Last mean reward per episode: 5058.16
Saving new best model to tmp/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64e+03     |
|    ep_rew_mean          | 5058.16      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 401          |
|    time_elapsed         | 116550       |
|    total_timesteps      | 9854976      |
| train/                  |              |
|    approx_kl            | 0.0131799085 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.907       |
|    explained_variance   | 0.97         |
|    learning_rate        | 3e-06        |
|    loss                 | 183          |
|    n_updates            | 4000         |
|    policy_gradient_loss | 0.000916     |
|    value_loss           | 474          |
------------------------------------------
Num timesteps: 9864000
Best mean reward: 5058.16 - Last mean reward per episode: 4996.81
Num timesteps: 9876000
Best mean reward: 5058.16 - Last mean reward per episode: 4926.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.58e+03   |
|    ep_rew_mean          | 4921.3     |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 402        |
|    time_elapsed         | 116837     |
|    total_timesteps      | 9879552    |
| train/                  |            |
|    approx_kl            | 0.01647196 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | 0.963      |
|    learning_rate        | 3e-06      |
|    loss                 | 237        |
|    n_updates            | 4010       |
|    policy_gradient_loss | -0.000866  |
|    value_loss           | 579        |
----------------------------------------
Num timesteps: 9888000
Best mean reward: 5058.16 - Last mean reward per episode: 4870.12
Num timesteps: 9900000
Best mean reward: 5058.16 - Last mean reward per episode: 4860.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.55e+03     |
|    ep_rew_mean          | 4852.03      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 403          |
|    time_elapsed         | 117128       |
|    total_timesteps      | 9904128      |
| train/                  |              |
|    approx_kl            | 0.0151804015 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.967       |
|    explained_variance   | 0.967        |
|    learning_rate        | 3e-06        |
|    loss                 | 448          |
|    n_updates            | 4020         |
|    policy_gradient_loss | 1.72e-05     |
|    value_loss           | 531          |
------------------------------------------
Num timesteps: 9912000
Best mean reward: 5058.16 - Last mean reward per episode: 4861.59
Num timesteps: 9924000
Best mean reward: 5058.16 - Last mean reward per episode: 4891.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.57e+03   |
|    ep_rew_mean          | 4894.65    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 404        |
|    time_elapsed         | 117416     |
|    total_timesteps      | 9928704    |
| train/                  |            |
|    approx_kl            | 0.01644642 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.901     |
|    explained_variance   | 0.967      |
|    learning_rate        | 3e-06      |
|    loss                 | 429        |
|    n_updates            | 4030       |
|    policy_gradient_loss | 0.00232    |
|    value_loss           | 579        |
----------------------------------------
Num timesteps: 9936000
Best mean reward: 5058.16 - Last mean reward per episode: 4854.68
Num timesteps: 9948000
Best mean reward: 5058.16 - Last mean reward per episode: 4845.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.55e+03     |
|    ep_rew_mean          | 4850.43      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 405          |
|    time_elapsed         | 117702       |
|    total_timesteps      | 9953280      |
| train/                  |              |
|    approx_kl            | 0.0142869055 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.909       |
|    explained_variance   | 0.964        |
|    learning_rate        | 3e-06        |
|    loss                 | 138          |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.000517    |
|    value_loss           | 606          |
------------------------------------------
Num timesteps: 9960000
Best mean reward: 5058.16 - Last mean reward per episode: 4821.20
Num timesteps: 9972000
Best mean reward: 5058.16 - Last mean reward per episode: 4803.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.47e+03    |
|    ep_rew_mean          | 4771.34     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 406         |
|    time_elapsed         | 117992      |
|    total_timesteps      | 9977856     |
| train/                  |             |
|    approx_kl            | 0.016683226 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 256         |
|    n_updates            | 4050        |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 414         |
-----------------------------------------
Num timesteps: 9984000
Best mean reward: 5058.16 - Last mean reward per episode: 4780.35
Num timesteps: 9996000
Best mean reward: 5058.16 - Last mean reward per episode: 4686.17
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.44e+03   |
|    ep_rew_mean          | 4687.46    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 407        |
|    time_elapsed         | 118278     |
|    total_timesteps      | 10002432   |
| train/                  |            |
|    approx_kl            | 0.01780815 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.913     |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 140        |
|    n_updates            | 4060       |
|    policy_gradient_loss | 0.00199    |
|    value_loss           | 469        |
----------------------------------------
Num timesteps: 10008000
Best mean reward: 5058.16 - Last mean reward per episode: 4674.65
Num timesteps: 10020000
Best mean reward: 5058.16 - Last mean reward per episode: 4677.03
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.38e+03   |
|    ep_rew_mean          | 4667.36    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 408        |
|    time_elapsed         | 118563     |
|    total_timesteps      | 10027008   |
| train/                  |            |
|    approx_kl            | 0.02055479 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.909     |
|    explained_variance   | 0.968      |
|    learning_rate        | 3e-06      |
|    loss                 | 235        |
|    n_updates            | 4070       |
|    policy_gradient_loss | -0.00162   |
|    value_loss           | 490        |
----------------------------------------
Num timesteps: 10032000
Best mean reward: 5058.16 - Last mean reward per episode: 4591.82
Num timesteps: 10044000
Best mean reward: 5058.16 - Last mean reward per episode: 4629.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 4647.46     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 409         |
|    time_elapsed         | 118847      |
|    total_timesteps      | 10051584    |
| train/                  |             |
|    approx_kl            | 0.019193126 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.954      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 895         |
|    n_updates            | 4080        |
|    policy_gradient_loss | 0.00326     |
|    value_loss           | 515         |
-----------------------------------------
Num timesteps: 10056000
Best mean reward: 5058.16 - Last mean reward per episode: 4661.36
Num timesteps: 10068000
Best mean reward: 5058.16 - Last mean reward per episode: 4663.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4681.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 410         |
|    time_elapsed         | 119133      |
|    total_timesteps      | 10076160    |
| train/                  |             |
|    approx_kl            | 0.014961027 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.973       |
|    learning_rate        | 3e-06       |
|    loss                 | 183         |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 470         |
-----------------------------------------
Num timesteps: 10080000
Best mean reward: 5058.16 - Last mean reward per episode: 4688.50
Num timesteps: 10092000
Best mean reward: 5058.16 - Last mean reward per episode: 4616.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4705.65     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 411         |
|    time_elapsed         | 119423      |
|    total_timesteps      | 10100736    |
| train/                  |             |
|    approx_kl            | 0.011526086 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.972       |
|    learning_rate        | 3e-06       |
|    loss                 | 230         |
|    n_updates            | 4100        |
|    policy_gradient_loss | 0.000654    |
|    value_loss           | 490         |
-----------------------------------------
Num timesteps: 10104000
Best mean reward: 5058.16 - Last mean reward per episode: 4696.05
Num timesteps: 10116000
Best mean reward: 5058.16 - Last mean reward per episode: 4673.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | 4698.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 412         |
|    time_elapsed         | 119718      |
|    total_timesteps      | 10125312    |
| train/                  |             |
|    approx_kl            | 0.015730558 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 517         |
|    n_updates            | 4110        |
|    policy_gradient_loss | 0.00254     |
|    value_loss           | 629         |
-----------------------------------------
Num timesteps: 10128000
Best mean reward: 5058.16 - Last mean reward per episode: 4711.58
Num timesteps: 10140000
Best mean reward: 5058.16 - Last mean reward per episode: 4719.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4738.81     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 413         |
|    time_elapsed         | 120005      |
|    total_timesteps      | 10149888    |
| train/                  |             |
|    approx_kl            | 0.014673295 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-06       |
|    loss                 | 932         |
|    n_updates            | 4120        |
|    policy_gradient_loss | 0.000552    |
|    value_loss           | 720         |
-----------------------------------------
Num timesteps: 10152000
Best mean reward: 5058.16 - Last mean reward per episode: 4735.28
Num timesteps: 10164000
Best mean reward: 5058.16 - Last mean reward per episode: 4756.57
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.37e+03   |
|    ep_rew_mean          | 4781.95    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 414        |
|    time_elapsed         | 120293     |
|    total_timesteps      | 10174464   |
| train/                  |            |
|    approx_kl            | 0.01721459 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.902     |
|    explained_variance   | 0.972      |
|    learning_rate        | 3e-06      |
|    loss                 | 136        |
|    n_updates            | 4130       |
|    policy_gradient_loss | 0.00155    |
|    value_loss           | 514        |
----------------------------------------
Num timesteps: 10176000
Best mean reward: 5058.16 - Last mean reward per episode: 4781.95
Num timesteps: 10188000
Best mean reward: 5058.16 - Last mean reward per episode: 4738.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 4719.14     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 415         |
|    time_elapsed         | 120583      |
|    total_timesteps      | 10199040    |
| train/                  |             |
|    approx_kl            | 0.015430274 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.969       |
|    learning_rate        | 3e-06       |
|    loss                 | 144         |
|    n_updates            | 4140        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 507         |
-----------------------------------------
Num timesteps: 10200000
Best mean reward: 5058.16 - Last mean reward per episode: 4719.14
Num timesteps: 10212000
Best mean reward: 5058.16 - Last mean reward per episode: 4764.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.35e+03    |
|    ep_rew_mean          | 4757.66     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 416         |
|    time_elapsed         | 120870      |
|    total_timesteps      | 10223616    |
| train/                  |             |
|    approx_kl            | 0.016870992 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.962       |
|    learning_rate        | 3e-06       |
|    loss                 | 255         |
|    n_updates            | 4150        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 600         |
-----------------------------------------
Num timesteps: 10224000
Best mean reward: 5058.16 - Last mean reward per episode: 4757.66
Num timesteps: 10236000
Best mean reward: 5058.16 - Last mean reward per episode: 4821.15
Num timesteps: 10248000
Best mean reward: 5058.16 - Last mean reward per episode: 4816.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4816.62     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 417         |
|    time_elapsed         | 121158      |
|    total_timesteps      | 10248192    |
| train/                  |             |
|    approx_kl            | 0.015586823 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.968       |
|    learning_rate        | 3e-06       |
|    loss                 | 131         |
|    n_updates            | 4160        |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 489         |
-----------------------------------------
Num timesteps: 10260000
Best mean reward: 5058.16 - Last mean reward per episode: 4818.96
Num timesteps: 10272000
Best mean reward: 5058.16 - Last mean reward per episode: 4844.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4844.91     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 418         |
|    time_elapsed         | 121444      |
|    total_timesteps      | 10272768    |
| train/                  |             |
|    approx_kl            | 0.011793081 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.876      |
|    explained_variance   | 0.974       |
|    learning_rate        | 3e-06       |
|    loss                 | 163         |
|    n_updates            | 4170        |
|    policy_gradient_loss | 0.000275    |
|    value_loss           | 409         |
-----------------------------------------
Num timesteps: 10284000
Best mean reward: 5058.16 - Last mean reward per episode: 4805.66
Num timesteps: 10296000
Best mean reward: 5058.16 - Last mean reward per episode: 4779.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.38e+03    |
|    ep_rew_mean          | 4779.36     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 419         |
|    time_elapsed         | 121733      |
|    total_timesteps      | 10297344    |
| train/                  |             |
|    approx_kl            | 0.012879561 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 251         |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.000151   |
|    value_loss           | 484         |
-----------------------------------------
Num timesteps: 10308000
Best mean reward: 5058.16 - Last mean reward per episode: 4714.12
Num timesteps: 10320000
Best mean reward: 5058.16 - Last mean reward per episode: 4726.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.35e+03     |
|    ep_rew_mean          | 4726.09      |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 420          |
|    time_elapsed         | 122025       |
|    total_timesteps      | 10321920     |
| train/                  |              |
|    approx_kl            | 0.0143312095 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.934       |
|    explained_variance   | 0.964        |
|    learning_rate        | 3e-06        |
|    loss                 | 313          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -6.42e-05    |
|    value_loss           | 594          |
------------------------------------------
Num timesteps: 10332000
Best mean reward: 5058.16 - Last mean reward per episode: 4774.22
Num timesteps: 10344000
Best mean reward: 5058.16 - Last mean reward per episode: 4813.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.39e+03    |
|    ep_rew_mean          | 4812.0      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 421         |
|    time_elapsed         | 122317      |
|    total_timesteps      | 10346496    |
| train/                  |             |
|    approx_kl            | 0.015520665 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 246         |
|    n_updates            | 4200        |
|    policy_gradient_loss | 0.000775    |
|    value_loss           | 581         |
-----------------------------------------
Num timesteps: 10356000
Best mean reward: 5058.16 - Last mean reward per episode: 4826.35
Num timesteps: 10368000
Best mean reward: 5058.16 - Last mean reward per episode: 4871.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4870.17     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 422         |
|    time_elapsed         | 122607      |
|    total_timesteps      | 10371072    |
| train/                  |             |
|    approx_kl            | 0.013750821 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | 0.97        |
|    learning_rate        | 3e-06       |
|    loss                 | 132         |
|    n_updates            | 4210        |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 472         |
-----------------------------------------
Num timesteps: 10380000
Best mean reward: 5058.16 - Last mean reward per episode: 4859.17
Num timesteps: 10392000
Best mean reward: 5058.16 - Last mean reward per episode: 4867.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 4869.82     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 423         |
|    time_elapsed         | 122890      |
|    total_timesteps      | 10395648    |
| train/                  |             |
|    approx_kl            | 0.016512923 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.917      |
|    explained_variance   | 0.963       |
|    learning_rate        | 3e-06       |
|    loss                 | 344         |
|    n_updates            | 4220        |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 540         |
-----------------------------------------
Num timesteps: 10404000
Best mean reward: 5058.16 - Last mean reward per episode: 4872.84
Num timesteps: 10416000
Best mean reward: 5058.16 - Last mean reward per episode: 4877.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.43e+03    |
|    ep_rew_mean          | 4877.41     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 424         |
|    time_elapsed         | 123182      |
|    total_timesteps      | 10420224    |
| train/                  |             |
|    approx_kl            | 0.024077944 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.965       |
|    learning_rate        | 3e-06       |
|    loss                 | 111         |
|    n_updates            | 4230        |
|    policy_gradient_loss | 0.00595     |
|    value_loss           | 456         |
-----------------------------------------
Num timesteps: 10428000
Best mean reward: 5058.16 - Last mean reward per episode: 4838.60
Num timesteps: 10440000
Best mean reward: 5058.16 - Last mean reward per episode: 4805.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.55e+03    |
|    ep_rew_mean          | 4809.22     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 425         |
|    time_elapsed         | 123474      |
|    total_timesteps      | 10444800    |
| train/                  |             |
|    approx_kl            | 0.015355646 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.986       |
|    learning_rate        | 3e-06       |
|    loss                 | 59          |
|    n_updates            | 4240        |
|    policy_gradient_loss | 0.000218    |
|    value_loss           | 212         |
-----------------------------------------
Num timesteps: 10452000
Best mean reward: 5058.16 - Last mean reward per episode: 4790.46
Num timesteps: 10464000
Best mean reward: 5058.16 - Last mean reward per episode: 4748.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.62e+03    |
|    ep_rew_mean          | 4748.55     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 426         |
|    time_elapsed         | 123766      |
|    total_timesteps      | 10469376    |
| train/                  |             |
|    approx_kl            | 0.019183235 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.98        |
|    learning_rate        | 3e-06       |
|    loss                 | 75.8        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.000334   |
|    value_loss           | 285         |
-----------------------------------------
Num timesteps: 10476000
Best mean reward: 5058.16 - Last mean reward per episode: 4784.29
Num timesteps: 10488000
Best mean reward: 5058.16 - Last mean reward per episode: 4792.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.68e+03    |
|    ep_rew_mean          | 4759.9      |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 427         |
|    time_elapsed         | 124057      |
|    total_timesteps      | 10493952    |
| train/                  |             |
|    approx_kl            | 0.018968737 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.983       |
|    learning_rate        | 3e-06       |
|    loss                 | 50          |
|    n_updates            | 4260        |
|    policy_gradient_loss | 0.0018      |
|    value_loss           | 267         |
-----------------------------------------
Num timesteps: 10500000
Best mean reward: 5058.16 - Last mean reward per episode: 4759.90
Num timesteps: 10512000
Best mean reward: 5058.16 - Last mean reward per episode: 4760.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.68e+03   |
|    ep_rew_mean          | 4763.09    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 428        |
|    time_elapsed         | 124345     |
|    total_timesteps      | 10518528   |
| train/                  |            |
|    approx_kl            | 0.01800365 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.977      |
|    learning_rate        | 3e-06      |
|    loss                 | 102        |
|    n_updates            | 4270       |
|    policy_gradient_loss | 0.00104    |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 10524000
Best mean reward: 5058.16 - Last mean reward per episode: 4753.93
Num timesteps: 10536000
Best mean reward: 5058.16 - Last mean reward per episode: 4720.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8e+03     |
|    ep_rew_mean          | 4720.74     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 429         |
|    time_elapsed         | 124633      |
|    total_timesteps      | 10543104    |
| train/                  |             |
|    approx_kl            | 0.014839974 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.982       |
|    learning_rate        | 3e-06       |
|    loss                 | 95.4        |
|    n_updates            | 4280        |
|    policy_gradient_loss | 3.89e-05    |
|    value_loss           | 221         |
-----------------------------------------
Num timesteps: 10548000
Best mean reward: 5058.16 - Last mean reward per episode: 4725.35
Num timesteps: 10560000
Best mean reward: 5058.16 - Last mean reward per episode: 4661.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.96e+03    |
|    ep_rew_mean          | 4659.33     |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 430         |
|    time_elapsed         | 124924      |
|    total_timesteps      | 10567680    |
| train/                  |             |
|    approx_kl            | 0.012819548 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.985       |
|    learning_rate        | 3e-06       |
|    loss                 | 195         |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 243         |
-----------------------------------------
Num timesteps: 10572000
Best mean reward: 5058.16 - Last mean reward per episode: 4671.49
Num timesteps: 10584000
Best mean reward: 5058.16 - Last mean reward per episode: 4667.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.98e+03   |
|    ep_rew_mean          | 4732.85    |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 431        |
|    time_elapsed         | 125212     |
|    total_timesteps      | 10592256   |
| train/                  |            |
|    approx_kl            | 0.02414453 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.999     |
|    explained_variance   | 0.97       |
|    learning_rate        | 3e-06      |
|    loss                 | 264        |
|    n_updates            | 4300       |
|    policy_gradient_loss | 0.00262    |
|    value_loss           | 478        |
----------------------------------------
